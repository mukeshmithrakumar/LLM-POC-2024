{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA From Scratch\n",
    "\n",
    "**References**\n",
    "- *Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm: [Youtube Video](https://youtu.be/oM4VmoabDAI?si=JtlNl00nZeIOkWxx), [Code](https://github.com/hkproj/pytorch-llama)*\n",
    "- *LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU: [Youtube Video](https://youtu.be/Mn_9W1nCFLo?si=4xJy4OzpPX5YxGqx)*\n",
    "- *RoFormer: Enhanced Transformer with Rotary Position Embedding: [Paper](https://arxiv.org/abs/2104.09864)*\n",
    "- *Root Mean Square Layer Normalization: [Paper](https://arxiv.org/abs/1910.07467)*\n",
    "- *GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints: [Paper](https://arxiv.org/abs/2305.13245)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Any\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA 1**\n",
    "![LLaMA 1 Parameters](images/llama-1-parameters.png)\n",
    "\n",
    "**LLaMA 2**\n",
    "![LLaMA 2 Parameters](images/llama-2-parameters.png)\n",
    "\n",
    "**LLaMA 3**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    # * Unlike the og transformer, we don't need to have the same q, k, v values in LLaMA\n",
    "    n_heads: int = 32  # number of heads for the queries\n",
    "    n_kv_heads: Optional[int] = None  # Number of heads for the keys and values\n",
    "    vocab_size: int = -1  # will be set when we load the tokenizer\n",
    "    # * since grouped query attention heads are reduced,\n",
    "    # * the number of params in the FFN is increased to keep the total number of parameters the same\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5  # epsilon for layer norm\n",
    "\n",
    "    # needed for KV cache\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "    device: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotary Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute Theta Posistional Frequencies\n",
    "\n",
    "Below are the steps involved in precomputing theta positional frequencies:\n",
    "\n",
    "![Precompute Theta Posistional Frequencies Steps](images/theta-pos-freq-steps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_pos_frequencies(\n",
    "    head_dim: int, seq_len: int, device: str, theta: float = 10000.0\n",
    "):\n",
    "    # theta 10000.0 is the default value in the paper\n",
    "    # As written in the paragraph 3.2.2 of the paper\n",
    "    # >> In order to generalize our results in 2D to any xi ∈ Rd where **d is even**, [...]\n",
    "    assert (\n",
    "        head_dim % 2 == 0\n",
    "    ), \"Dimension must be even since rotary embedding can't be applied to odd.\"\n",
    "\n",
    "    # Build the theta parameter\n",
    "    # According to the formula theta_i = 10000^(-2(i-1)/dim) for i = [1, 2, ..., dim/2]\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()  # (head_dim / 2)\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)  # (dim / 2)\n",
    "    # construct the positions (the \"m\" parameter)\n",
    "    m = torch.arange(seq_len, device=device)  # (seq_len)\n",
    "    # Multiply each theta by each position using the outer product.\n",
    "    # (seq_len), outer_product*(head_dim/2) -> (seq_len,head_dim/2)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    # we can compute complex numbers in the polar form c = R*exp(m*theta), where R=1 as follow:\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Embeddings\n",
    "\n",
    "The Steps in calculating the Rotary Embedding:\n",
    "![The Steps in calculating the Rotary Embedding](images/rotary-embedding-steps.png)\n",
    "\n",
    "Figure 1: Implementation of Rotary Position Embedding(RoPE):\n",
    "![Implementation of Rotary Position Embedding](images/implementation-of-rope.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    # * OP 1 & 2 >>\n",
    "    # seperate the last dimension pairs of 2 values, representing the real & imaginary parts of the complex number\n",
    "    # two consecutive values will become a single complex number\n",
    "    # (B,seq_len,H,head_dim) -> (B,seq_len,H,head_dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    # reshape the freqs_complex tensor to match the shape of the x_complex tensor.\n",
    "    # So we need to add the batch dimension and the head dimension.\n",
    "    # (seq_len,head_dim/2) -> (1,seq_len,1,head_dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    # * OP 3 >>\n",
    "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
    "    # which results in the rotation of the complex number as shown in the Figure 1 of the paper.\n",
    "    # (B,seq_len,H,head_dim/2)*(1,seq_len,1,head_dim/2) -> (B,seq_len,H,head_dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    # * OP 4 >> convert the complex number back to the real number\n",
    "    # (B,seq_len,H,head_dim/2) -> (B,seq_len,H,head_dim/2,2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    # * OP 5 >> Flattening to the shape of the original tensor\n",
    "    # (B,seq_len,H,head_dim/2,2) -> (B,seq_len,H,head_dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Normalization\n",
    "\n",
    "LayerNorm works because of its re-centering and re-scaling invariance property. Re-centering enables the model to be insensitive to shift noises on both inputs and weights, and re-scaling keeps the output representations intact when both inputs and weights are randomly scaled. \n",
    "RMS Normalizaiton paper hypothesize that the re-scaling invariance is the reason for success of LayerNorm, rather than re-centering invariance and they propose RMSNorm which only focuses on re-scaling invariance and regularizes the summed inputs simply according to the root mean square (RMS) statistic:\n",
    "\n",
    "$$\n",
    "\\bar{a}_i= \\frac{a_i}{RMS(a)} g_i \\\\\n",
    "\\text{where} \\ RMS(a) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n a_{i}^2}\n",
    "$$\n",
    "\n",
    "Intuitively, RMSNorm simplifies LayerNorm by totally removing the mean statistic at the cost of sacrificing the invariance that mean normalization affords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))  # The gamma parameter\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # rsqrt: 1 / sqrt(x)\n",
    "        # (B,seq_len,dim)*(B,seq_len,1) -> (B,seq_len,dim)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (dim)*(B,seq_len,dim) -> (B,seq_len,dim)\n",
    "        return self.weight * self._norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "### SwiGLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = args.multiple_of * (\n",
    "            (hidden_dim + args.multiple_of - 1) // args.multiple_of\n",
    "        )\n",
    "\n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        swish = F.silu(self.w1(x))  # (B, seq_len, dim) -> (B, seq_len, hidden_dim)\n",
    "        x_v = self.w3(x)  # (B, seq_len, dim) -> (B, seq_len, hidden_dim)\n",
    "        # (B, seq_len, hidden_dim) * (B, seq_len, hidden_dim) -> (B, seq_len, hidden_dim)\n",
    "        x = swish * x_v\n",
    "        x = self.w2(x)  # (B, seq_len, hidden_dim) -> (B, seq_len, dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat KV Cache\n",
    "\n",
    "Self-Attention during Next Token Prediction Task at Inference T=1:\n",
    "\n",
    "![Self-Attention during Next Token Prediction Task at T1](images/Self-Attention-during-NTP-Task-T1.png)\n",
    "\n",
    "Self-Attention during Next Token Prediction Task at Inference T=4:\n",
    "\n",
    "![Self-Attention during Next Token Prediction Task at T4](images/Self-Attention-during-NTP-Task-T4.png)\n",
    "\n",
    "Where KV Cache is useful:\n",
    "\n",
    "![Where KV Cache is useful](images/where-kv-cache-come-in.png)\n",
    "\n",
    "Self-Attention with KV-Cache at Inference T=1:\n",
    "\n",
    "![Self-Attention with KV-Cache at T1](images/Self-Attention-with-KV-Cache-T1.png)\n",
    "\n",
    "Self-Attention with KV-Cache at Inference T=4:\n",
    "\n",
    "![Self-Attention with KV-Cache at T4](images/Self-Attention-with-KV-Cache-T4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, None, :]  # (B, seq_len, n_kv_heads, 1, head_dim)\n",
    "        .expand(\n",
    "            batch_size, seq_len, n_kv_heads, n_rep, head_dim\n",
    "        )  # (B, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(\n",
    "            batch_size, seq_len, n_kv_heads * n_rep, head_dim\n",
    "        )  # (B, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped Query Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing different attention algorithms\n",
    "\n",
    "**Vanilla batched Multi-Head Attention**\n",
    "\n",
    "\n",
    "**Batched Multi-Head Attention with KV cache**\n",
    "\n",
    "\n",
    "**<span style=\"color:red\">Multi-Query</span> Attention with KV cache**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped Multi-Query Attention\n",
    "\n",
    "Grouped Multi-Query Attention is a compromise between Multi-Head Attention and Multi-Query Attention:\n",
    "\n",
    "![Grouped Multi-Query Attention compared](images/Grouped-Multi-Query-Attention-compared.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # indicates the number of heads for the keys and values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # indicates the number of heads for the queries\n",
    "        self.n_heads_q = args.n_heads\n",
    "        # indicates how many times the keys and values should be repeated\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        # indicates the dimension of each head, i.e the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        batch_size, seq_len, _ = x.shape  # (B, 1, dim)\n",
    "        xq = self.wq(x)  # (B, 1, dim) -> (B, 1, H_Q * head_dim)\n",
    "        xk = self.wk(x)  # (B, 1, dim) -> (B, 1, H_KV * head_dim)\n",
    "        xv = self.wv(x)  # (B, 1, dim) -> (B, 1, H_KV * head_dim)\n",
    "\n",
    "        # (B, 1, H_Q * Head_Dim) -> (B, 1, H_Q, Head_Dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "        # Size is the same for xk & xv: (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # Size doesn't change for xq & zk: (B, 1, H_Q, head_dim) -> (B, 1, H_Q, head_dim)\n",
    "        xq = apply_rotary_embeddings(xq, freqs_complex, x.device)\n",
    "        xk = apply_rotary_embeddings(xk, freqs_complex, x.device)\n",
    "\n",
    "        # replace the entry in the cache for this token\n",
    "        self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
    "\n",
    "        # retrieve all the cached keys and values so far\n",
    "        # Size is the same for keys & values: (B, seq_len_kv, H_KV, head_dim)\n",
    "        keys = self.cache_k[:batch_size, : start_pos + seq_len]\n",
    "        values = self.cache_v[:batch_size, : start_pos + seq_len]\n",
    "\n",
    "        # since every group of Q shares the same K & V heads,\n",
    "        # just repeat the K & V heads for every Q in the same group.\n",
    "        # Doesn't look like grouped query attention is being done here since only 70B LLaMA has this feature.\n",
    "        # So this is just multi-head attention.\n",
    "        # Size is the same for keys & values:\n",
    "        # (B, seq_len_kv, H_KV, head_dim) -> (B, seq_len_kv, H_Q, head_dim)\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (B, 1, H_Q, head_dim) -> (B, H_Q, 1, head_dim)\n",
    "        # Size is the same for keys & values:\n",
    "        # (B, seq_len_kv, H_Q, head_dim) -> (B, H_Q, seq_len_kv, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (B, H_Q, 1, head_dim) @ (B, H_Q, head_dim, seq_len_kv) -> (B, H_Q, 1, seq_len_kv)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # (B, H_Q, 1, seq_len_kv) -> (B, H_Q, 1, seq_len_kv)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "\n",
    "        # (B, H_Q, 1, seq_len) @ (B, H_Q, seq_len_kv, head_dim) -> (B, H_Q, 1, head_dim)\n",
    "        output = torch.matmul(scores, values)\n",
    "        # (B, H_Q, 1, head_dim) -> (B, 1, H_Q, head_dim) -> (B, 1, dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.wo(output)  # (B, 1, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = self.dim // self.n_head\n",
    "\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "\n",
    "        # Normalization before the attention block\n",
    "        self.attention_norm = RMSNorm(self.dim, eps=args.norm_eps)\n",
    "        # Normalization before the feed-forward block\n",
    "        self.ffn_norm = RMSNorm(self.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # (B,seq_len,dim) + (B,seq_len,dim) -> (B,seq_len,dim)\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_complex)\n",
    "        # (B,seq_len,dim) + (B,seq_len,dim) -> (B,seq_len,dim)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer vs LLaMA](images/Transformer-vs-LLaMA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        assert args.vocab_size != -1, \"vocab_size must be set\"\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers  # represents Nx in the figure above: 32 layers\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(args.n_layers):\n",
    "            self.layers.append(EncoderBlock(args))\n",
    "\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(\n",
    "            self.args.dim // self.args.n_heads,\n",
    "            self.args.max_seq_len * 2,\n",
    "            device=self.args.device,\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        batch_size, seq_len = tokens.shape  # (B, seq_len)\n",
    "        assert seq_len == 1, \"Only one token at a time can be processed.\"\n",
    "\n",
    "        h = self.tok_embeddings(tokens)  # (B, seq_len) -> (B, seq_len, dim)\n",
    "        # retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
    "        freqs_complex = self.freqs_complex[start_pos : start_pos + seq_len]\n",
    "\n",
    "        # consequently apply all the encoder layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_complex)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Transformer,\n",
    "        tokenizer: SentencePieceProcessor,\n",
    "        model_args: ModelArgs,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = model_args\n",
    "\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        checkpoints_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        load_model: bool,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        device: str,\n",
    "    ):\n",
    "        prev_time = time.time()\n",
    "        if load_model:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
    "            assert len(checkpoints) > 0, f\"No checkpoints found in {checkpoints_dir}\"\n",
    "            ckpt_path = checkpoints[0]\n",
    "            print(f\"Loading model from checkpoint: {ckpt_path}\")\n",
    "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            print(f\"Loaded checkpoint in {time.time() - prev_time:.2f} seconds\")\n",
    "            prev_time\n",
    "        with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            device=device,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
    "\n",
    "        model = Transformer(model_args).to(device)\n",
    "\n",
    "        if load_model:\n",
    "            # The only unmatched key in the checkpoint is rope.freqs. Remove it\n",
    "            del checkpoint[\"rope.freqs\"]\n",
    "            model.load_state_dict(checkpoint, strict=True)\n",
    "            print(f\"Loaded state dict in {time.time() - prev_time:.2f} seconds\")\n",
    "\n",
    "        return LLaMA(model, tokenizer, model_args)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: list[str],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "    ):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.args.max_seq_len - 1\n",
    "\n",
    "        # convert each prompt into tokens\n",
    "        prompt_tokens = [\n",
    "            self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "        # Make sure the batch size is not too large\n",
    "        batch_size = len(prompt_tokens)\n",
    "        assert (\n",
    "            batch_size <= self.args.max_batch_size\n",
    "        ), f\"Batch size {batch_size} must be less than or equal to the Max batch size: {self.args.max_batch_size}\"\n",
    "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "        # Make sure the prompt length is not larger than the max sequence length\n",
    "        assert (\n",
    "            max_prompt_len <= self.args.max_seq_len\n",
    "        ), f\"Prompt length {max_prompt_len} must be less than or equal to the Max sequence length: {self.args.max_seq_len}\"\n",
    "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        # create the list that will contain the generated tokens, along with the initial prompt tokens\n",
    "        pad_id = self.tokenizer.pad_id()\n",
    "        tokens = torch.full(\n",
    "            (batch_size, total_len), pad_id, dtype=torch.long, device=device\n",
    "        )\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            # populate the initial tokens with the prompt tokens\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "\n",
    "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "        # True if the token is a prompt token, False otherwise\n",
    "        prompt_tokens_mask = tokens != pad_id\n",
    "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens...\")\n",
    "        for cur_pos in cur_iterator:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model.forward(tokens[:, cur_pos - 1 : cur_pos], cur_pos)\n",
    "            if temperature > 0:\n",
    "                # The temperature is applied before the softmax\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = self._sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                # greedily select the token with the max probability\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if it is a padding token\n",
    "            next_token = torch.where(\n",
    "                prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            # EOS is reached only if we found an EOS token for a padding position\n",
    "            eos_reached != (\n",
    "                ~prompt_tokens_mask[:, cur_pos] & (next_token == self.tokenizer.eos_id)\n",
    "            )\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        out_tokens = []\n",
    "        out_text = []\n",
    "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
    "            # cut to the EOS token, if present\n",
    "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
    "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
    "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "            out_tokens.append(current_prompt_tokens)\n",
    "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
    "        return (out_tokens, out_text)\n",
    "\n",
    "    def _sample_top_p(self, probs: torch.Tensor, p: float):\n",
    "        probs_sort, probs_idx = torch.sort(\n",
    "            probs, dim=-1, descending=True\n",
    "        )  # (B, vocab_size)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1)  # (B, vocab_size)\n",
    "        # (substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "        mask = probs_sum - probs_sort > p  # (B, vocab_size)\n",
    "        # zero out all the probabilities of tokens that are not selected by the top_p\n",
    "        probs_sort[mask] = 0.0\n",
    "        # redistribute the probabilities so that they sum up to 1.\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        # sample a token (its index) from the top p distribution\n",
    "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "        # get the token position in the vocabulary corresponding to the sampled index\n",
    "        next_token = torch.gather(probs_idx, -1, next_token)\n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "allow_cuda = False\n",
    "device = \"cuda\" if torch.cuda.is_available() and allow_cuda else \"cpu\"\n",
    "\n",
    "prompts = [\n",
    "    \"Simply put, the theory of relativity states that \",\n",
    "    \"If Google was an Italian company founded in Milan, it would\",\n",
    "    # Few shot promt\n",
    "    \"\"\"Translate English to French:\n",
    "    sea otter => loutre de mer\n",
    "    peppermint => menthe poivrée\n",
    "    plush girafe => girafe peluche\n",
    "    cheese =>\"\"\",\n",
    "    # Zero shot prompt\n",
    "    \"\"\"Tell me if the following person is actually a Jedi night disguised as human:\n",
    "    Name: Mukesh Mithrakumar\n",
    "    Decision: \n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "model = LLaMA.build(\n",
    "    checkpoints_dir=\"models/llama-2-7b\",\n",
    "    tokenizer_path=\"models/tokenizer.model\",\n",
    "    load_model=True,\n",
    "    max_seq_len=1024,\n",
    "    max_batch_size=len(prompts),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "out_tokens, out_texts = model.text_completion(prompts, max_gen_len=64)\n",
    "assert len(out_texts) == len(prompts)\n",
    "for i in range(len(out_texts)):\n",
    "    print(f\"{out_texts[i]}\\n'-'*50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
