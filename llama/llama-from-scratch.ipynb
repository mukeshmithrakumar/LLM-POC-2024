{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA From Scratch\n",
    "\n",
    "**References**\n",
    "- *Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm: [Youtube Video](https://youtu.be/oM4VmoabDAI?si=JtlNl00nZeIOkWxx), [Code](https://github.com/hkproj/pytorch-llama)*\n",
    "- *LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU: [Youtube Video](https://youtu.be/Mn_9W1nCFLo?si=4xJy4OzpPX5YxGqx)*\n",
    "- *RoFormer: Enhanced Transformer with Rotary Position Embedding: [Paper](https://arxiv.org/abs/2104.09864)*\n",
    "- *Root Mean Square Layer Normalization: [Paper](https://arxiv.org/abs/1910.07467)*\n",
    "- *GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints: [Paper](https://arxiv.org/abs/2305.13245)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Any\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td><strong>Training Data</strong></td>\n",
    "        <td><strong>Params</strong></td>\n",
    "        <td><strong>Context length</strong></td>\n",
    "        <td><strong>GQA</strong></td>\n",
    "        <td><strong>Token count</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"4\">Llama 1</td>\n",
    "        <td rowspan=\"4\">See Touvron et al. (2023)</td>\n",
    "        <td>7B</td>\n",
    "        <td>2k</td>\n",
    "        <td>✗</td>\n",
    "        <td>1T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>13B</td>\n",
    "        <td>2k</td>\n",
    "        <td>✗</td>\n",
    "        <td>1T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>33B</td>\n",
    "        <td>2k</td>\n",
    "        <td>✗</td>\n",
    "        <td>1.4T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>65B</td>\n",
    "        <td>2k</td>\n",
    "        <td>✗</td>\n",
    "        <td>1.4T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"4\">Llama 2</td>\n",
    "        <td rowspan=\"4\">A new mix of publicly available online data.</td>\n",
    "        <td>7B</td>\n",
    "        <td>4k</td>\n",
    "        <td>✗</td>\n",
    "        <td rowspan=\"4\">2T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>13B</td>\n",
    "        <td>4k</td>\n",
    "        <td>✗</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>34B</td>\n",
    "        <td>4k</td>\n",
    "        <td>✔️</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>70B</td>\n",
    "        <td>4k</td>\n",
    "        <td>✔️</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Llama 3</td>\n",
    "        <td rowspan=\"2\">A new mix of publicly available online data.</td>\n",
    "        <td>8B</td>\n",
    "        <td>8k</td>\n",
    "        <td>✔️</td>\n",
    "        <td rowspan=\"2\">15T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>70B</td>\n",
    "        <td>8k</td>\n",
    "        <td>✔️</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    # * Unlike the og transformer, we don't need to have the same q, k, v values in LLaMA\n",
    "    n_heads: int = 32  # number of heads for the queries\n",
    "    n_kv_heads: Optional[int] = None  # Number of heads for the keys and values\n",
    "    vocab_size: int = -1  # will be set when we load the tokenizer\n",
    "    # * since grouped query attention heads are reduced,\n",
    "    # * the number of params in the FFN is increased to keep the total number of parameters the same\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5  # epsilon for layer norm\n",
    "\n",
    "    # needed for KV cache\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "    device: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotary Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute Theta Posistional Frequencies\n",
    "\n",
    "Below are the steps involved in precomputing theta positional frequencies:\n",
    "\n",
    "![Precompute Theta Posistional Frequencies Steps](images/theta-pos-freq-steps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_pos_frequencies(\n",
    "    head_dim: int, seq_len: int, device: str, theta: float = 10000.0\n",
    "):\n",
    "    # theta 10000.0 is the default value in the paper\n",
    "    # As written in the paragraph 3.2.2 of the paper\n",
    "    # >> In order to generalize our results in 2D to any xi ∈ Rd where **d is even**, [...]\n",
    "    assert (\n",
    "        head_dim % 2 == 0\n",
    "    ), \"Dimension must be even since rotary embedding can't be applied to odd.\"\n",
    "\n",
    "    # Build the theta parameter\n",
    "    # According to the formula theta_i = 10000^(-2(i-1)/dim) for i = [1, 2, ..., dim/2]\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()  # (head_dim / 2)\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)  # (dim / 2)\n",
    "    # construct the positions (the \"m\" parameter)\n",
    "    m = torch.arange(seq_len, device=device)  # (seq_len)\n",
    "    # Multiply each theta by each position using the outer product.\n",
    "    # (seq_len), outer_product*(head_dim/2) -> (seq_len,head_dim/2)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    # we can compute complex numbers in the polar form c = R*exp(m*theta), where R=1 as follow:\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Embeddings\n",
    "\n",
    "The Steps in calculating the Rotary Embedding:\n",
    "![The Steps in calculating the Rotary Embedding](images/rotary-embedding-steps.png)\n",
    "\n",
    "Figure 1: Implementation of Rotary Position Embedding(RoPE):\n",
    "![Implementation of Rotary Position Embedding](images/implementation-of-rope.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    # * OP 1 & 2 >>\n",
    "    # seperate the last dimension pairs of 2 values, representing the real & imaginary parts of the complex number\n",
    "    # two consecutive values will become a single complex number\n",
    "    # (B,seq_len,H,head_dim) -> (B,seq_len,H,head_dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    # reshape the freqs_complex tensor to match the shape of the x_complex tensor.\n",
    "    # So we need to add the batch dimension and the head dimension.\n",
    "    # (seq_len,head_dim/2) -> (1,seq_len,1,head_dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    # * OP 3 >>\n",
    "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
    "    # which results in the rotation of the complex number as shown in the Figure 1 of the paper.\n",
    "    # (B,seq_len,H,head_dim/2)*(1,seq_len,1,head_dim/2) -> (B,seq_len,H,head_dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    # * OP 4 >> convert the complex number back to the real number\n",
    "    # (B,seq_len,H,head_dim/2) -> (B,seq_len,H,head_dim/2,2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    # * OP 5 >> Flattening to the shape of the original tensor\n",
    "    # (B,seq_len,H,head_dim/2,2) -> (B,seq_len,H,head_dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Normalization\n",
    "\n",
    "LayerNorm works because of its re-centering and re-scaling invariance property. Re-centering enables the model to be insensitive to shift noises on both inputs and weights, and re-scaling keeps the output representations intact when both inputs and weights are randomly scaled. \n",
    "RMS Normalizaiton paper hypothesize that the re-scaling invariance is the reason for success of LayerNorm, rather than re-centering invariance and they propose RMSNorm which only focuses on re-scaling invariance and regularizes the summed inputs simply according to the root mean square (RMS) statistic:\n",
    "\n",
    "$$\n",
    "\\bar{a}_i= \\frac{a_i}{RMS(a)} g_i \\\\\n",
    "\\text{where} \\ RMS(a) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n a_{i}^2}\n",
    "$$\n",
    "\n",
    "Intuitively, RMSNorm simplifies LayerNorm by totally removing the mean statistic at the cost of sacrificing the invariance that mean normalization affords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))  # The gamma parameter\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # rsqrt: 1 / sqrt(x)\n",
    "        # (B,seq_len,dim)*(B,seq_len,1) -> (B,seq_len,dim)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (dim)*(B,seq_len,dim) -> (B,seq_len,dim)\n",
    "        return self.weight * self._norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "### SwiGLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = args.multiple_of * (\n",
    "            (hidden_dim + args.multiple_of - 1) // args.multiple_of\n",
    "        )\n",
    "\n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        swish = F.silu(self.w1(x))  # (B, seq_len, dim) -> (B, seq_len, hidden_dim)\n",
    "        x_v = self.w3(x)  # (B, seq_len, dim) -> (B, seq_len, hidden_dim)\n",
    "        # (B, seq_len, hidden_dim) * (B, seq_len, hidden_dim) -> (B, seq_len, hidden_dim)\n",
    "        x = swish * x_v\n",
    "        x = self.w2(x)  # (B, seq_len, hidden_dim) -> (B, seq_len, dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat KV Cache\n",
    "\n",
    "Self-Attention during Next Token Prediction Task at Inference T=1:\n",
    "\n",
    "![Self-Attention during Next Token Prediction Task at T1](images/Self-Attention-during-NTP-Task-T1.png)\n",
    "\n",
    "Self-Attention during Next Token Prediction Task at Inference T=4:\n",
    "\n",
    "![Self-Attention during Next Token Prediction Task at T4](images/Self-Attention-during-NTP-Task-T4.png)\n",
    "\n",
    "Where KV Cache is useful:\n",
    "\n",
    "![Where KV Cache is useful](images/where-kv-cache-come-in.png)\n",
    "\n",
    "Self-Attention with KV-Cache at Inference T=1:\n",
    "\n",
    "![Self-Attention with KV-Cache at T1](images/Self-Attention-with-KV-Cache-T1.png)\n",
    "\n",
    "Self-Attention with KV-Cache at Inference T=4:\n",
    "\n",
    "![Self-Attention with KV-Cache at T4](images/Self-Attention-with-KV-Cache-T4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, None, :]  # (B, seq_len, n_kv_heads, 1, head_dim)\n",
    "        .expand(\n",
    "            batch_size, seq_len, n_kv_heads, n_rep, head_dim\n",
    "        )  # (B, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(\n",
    "            batch_size, seq_len, n_kv_heads * n_rep, head_dim\n",
    "        )  # (B, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different attention algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla batched Multi-Head Attention\n",
    "\n",
    "- Multihead Attention as presented in the original paper \"Attention is all you need\".\n",
    "- By setting 𝑚 = 𝑛 (sequence length of query = seq. length of keys and values)\n",
    "- The number of arithmetic operations performed is $O(bnd^2)$\n",
    "- The total memory involved in the operations, given by the sum of all the tensors involved in the calculations (including the derived ones!) is $O(bnd + bhn^2 + d^2)$ \n",
    "- The ratio between the total memory and the number of arithmetic operations is $O (\\frac{1}{k} + \\frac{1}{bn})$\n",
    "- In this case, the ratio is much smaller than 1, which means that the number of memory access we are performing is much less than the number of arithmetic operations, so the memory access is not the bottleneck here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiHeadAttentionBatched():\n",
    "    d, m, n, b, h, k, v = 512, 10, 10, 32, 8, (512 // 8), (512 // 8)\n",
    "\n",
    "    X = torch.rand(b, n, d)  # Query\n",
    "    M = torch.rand(b, m, d)  # Key and Value\n",
    "    mask = torch.rand(b, h, n, m)\n",
    "    P_q = torch.rand(h, d, k)  # W_q\n",
    "    P_k = torch.rand(h, d, k)  # W_k\n",
    "    P_v = torch.rand(h, d, v)  # W_v\n",
    "    P_o = torch.rand(h, d, v)  # W_o\n",
    "\n",
    "    Q = torch.einsum(\"bnd,hdk->bhnk\", X, P_q)\n",
    "    K = torch.einsum(\"bmd,hdk->bhmk\", M, P_k)\n",
    "    V = torch.einsum(\"bmd,hdv->bhmv\", M, P_v)\n",
    "\n",
    "    logits = torch.einsum(\"bhnk,bhmk->bhnm\", Q, K)\n",
    "    weights = torch.softmax(logits + mask, dim=-1)\n",
    "\n",
    "    O = torch.einsum(\"bhnm,bhmv->bhnv\", weights, V)\n",
    "    Y = torch.einsum(\"bhnv,hdv->bnd\", O, P_o)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Einsum\n",
    "\n",
    "`torch.einsum(equation, *operands) -> Tensor`\n",
    "\n",
    "equation (str): The equation string specifies the subscripts for each dimension of the input operands in the same order as the dimensions. It uses letters in [a-zA-Z] to represent subscripts. The subscripts for each operand are separated by a comma (','). For example, 'ij,jk->ik' specifies subscripts for two 2D operands. The subscripts that appear exactly once in the equation will be part of the output, sorted in increasing alphabetical order. Optionally, you can define the output subscripts by adding an arrow ('->') at the end of the equation followed by the subscripts for the output.\n",
    "\n",
    "Note that for example for Q, if you want bhkn instead of bhnk, you can do that since the operation is always the sum, its just the output dimension is different so essentially something like a reshape.\n",
    "\n",
    "The torch.einsum() function in PyTorch is a powerful tool that allows you to perform various multi-dimensional, linear algebraic operations in a concise way. Here are some common use cases:\n",
    "\n",
    "1. **Element-wise multiplication**: You can use `einsum` to perform element-wise multiplication of two tensors, similar to the `*` operator.\n",
    "    ```python\n",
    "    A = torch.rand(3, 3)\n",
    "    B = torch.rand(3, 3)\n",
    "    result = torch.einsum('ij,ij->ij', A, B)\n",
    "    ```\n",
    "\n",
    "2. **Matrix multiplication**: einsum can be used to perform matrix multiplication, similar to `torch.matmul()` or the `@` operator.\n",
    "    ```python\n",
    "    A = torch.rand(3, 4)\n",
    "    B = torch.rand(4, 5)\n",
    "    result = torch.einsum('ij,jk->ik', A, B)\n",
    "    ```\n",
    "\n",
    "3. **Batch matrix multiplication**: If you have a batch of matrices and you want to multiply them, you can use `einsum` to do this in a single operation.\n",
    "    ```python\n",
    "    A = torch.rand(10, 3, 4)\n",
    "    B = torch.rand(10, 4, 5)\n",
    "    result = torch.einsum('bij,bjk->bik', A, B)\n",
    "    ```\n",
    "\n",
    "4. **Dot product**: `einsum` can be used to compute the dot product of two vectors.\n",
    "    ```python\n",
    "    a = torch.rand(5)\n",
    "    b = torch.rand(5)\n",
    "    result = torch.einsum('i,i->', a, b)\n",
    "    ```\n",
    "\n",
    "5. **Sum along a dimension**: You can use `einsum` to sum the elements of a tensor along a specific dimension, similar to `torch.sum()`.\n",
    "    ```python\n",
    "    A = torch.rand(3, 4, 5)\n",
    "    sum_along_dim1 = torch.einsum('ijk->ik', A)\n",
    "    ```\n",
    "\n",
    "6. **Complex operations**: `einsum` really shines when you want to perform more complex operations that involve multiple steps and intermediate tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batched Multi-Head Attention with KV cache\n",
    "\n",
    "- Uses the KV cache to reduce the number of operations performed.\n",
    "- By setting 𝑚 = 𝑛 (sequence length of query = seq. length of keys and values)\n",
    "- The number of arithmetic operations performed is $O(bnd^2)$\n",
    "- The total memory involved in the operations, given by the sum of all the tensors involved in the calculations (including the derived ones!) is $O(bn^2d + nd^2)$\n",
    "- The ratio between the total memory and the number of arithmetic operations is $O (\\frac{n}{d} + \\frac{1}{b})$\n",
    "- When 𝑛 ≈ 𝑑 (the sequence length is close to the size of the embedding vector) or 𝑏 ≈ 1 (the batch size is 1), the ratio becomes 1 and the memory access now becomes the bottleneck of the algorithm. For the batch size is not a problem, since it is generally much higher than 1, while for the 𝑛/𝑑 term, we need to reduce the sequence length. But there’s a better way...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiHeadSelfAttentionIncremental():\n",
    "    d, b, h, k, v = 512, 32, 8, (512 // 8), (512 // 8)\n",
    "\n",
    "    m = 5  # Suppose we have already cached \"m\" tokens\n",
    "    prev_K = torch.rand(b, h, m, k)\n",
    "    prev_V = torch.rand(b, h, m, v)\n",
    "\n",
    "    X = torch.rand(b, d)  # Query\n",
    "    M = torch.rand(b, d)  # Key and Value\n",
    "    P_q = torch.rand(h, d, k)  # W_q\n",
    "    P_k = torch.rand(h, d, k)  # W_k\n",
    "    P_v = torch.rand(h, d, v)  # W_v\n",
    "    P_o = torch.rand(h, d, v)  # W_o\n",
    "\n",
    "    q = torch.einsum(\"bd,hdk->bhk\", X, P_q)\n",
    "    K = torch.concat([prev_K, torch.einsum(\"bd,hdk->bhk\", M, P_k).unsqueeze(2)], axis=2)\n",
    "    V = torch.concat([prev_V, torch.einsum(\"bd,hdv->bhv\", M, P_v).unsqueeze(2)], axis=2)\n",
    "\n",
    "    logits = torch.einsum(\"bhk,bhmk->bhnm\", q, K)\n",
    "    weights = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    O = torch.einsum(\"bhm,bhmv->bhv\", weights, V)\n",
    "    Y = torch.einsum(\"bhv,hdv->bd\", O, P_o)\n",
    "    return Y, K, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Query Attention with KV cache\n",
    "\n",
    "- We remove the ℎ dimension from the 𝐾 and the 𝑉, while keeping it for the 𝑄. This means that all the different query heads will share the same keys and values.\n",
    "- The number of arithmetic operations performed is $O(bnd^2)$\n",
    "- The total memory involved in the operations, given by the sum of all the tensors involved in the calculations (including the derived ones!) is $O(bnd + bn^2k + nd^2)$\n",
    "- The ratio between the total memory and the number of arithmetic operations is $O (\\frac{1}{d} + \\frac{n}{dh} + \\frac{1}{b})$\n",
    "- Comparing with the previous approach, we have reduced the expensive term 𝑛/𝑑 by a factor of h.\n",
    "- The performance gains are important, while the model's quality degrades only a little bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiquerySelfAttentionIncremental():\n",
    "    d, b, h, k, v = 512, 32, 8, (512 // 8), (512 // 8)\n",
    "\n",
    "    m = 5  # Suppose we have already cached \"m\" tokens\n",
    "    prev_K = torch.rand(b, m, k)\n",
    "    prev_V = torch.rand(b, m, v)\n",
    "\n",
    "    X = torch.rand(b, d)  # Query\n",
    "    M = torch.rand(b, d)  # Key and Value\n",
    "    P_q = torch.rand(h, d, k)  # W_q\n",
    "    P_k = torch.rand(d, k)  # W_k\n",
    "    P_v = torch.rand(d, v)  # W_v\n",
    "    P_o = torch.rand(d, v)  # W_o\n",
    "\n",
    "    q = torch.einsum(\"bd,hdk->bhk\", X, P_q)\n",
    "    K = torch.concat([prev_K, torch.einsum(\"bd,dk->bk\", M, P_k).unsqueeze(1)], axis=1)\n",
    "    V = torch.concat([prev_V, torch.einsum(\"bd,dv->bv\", M, P_v).unsqueeze(1)], axis=1)\n",
    "\n",
    "    logits = torch.einsum(\"bhk,bmk->bhm\", q, K)\n",
    "    weights = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    O = torch.einsum(\"bhm,bmv->bhv\", weights, V)\n",
    "    Y = torch.einsum(\"bhv,hdv->bd\", O, P_o)\n",
    "    return Y, K, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped Multi-Query Attention\n",
    "\n",
    "Grouped Multi-Query Attention is a compromise between Multi-Head Attention and Multi-Query Attention:\n",
    "\n",
    "![Grouped Multi-Query Attention compared](images/Grouped-Multi-Query-Attention-compared.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # indicates the number of heads for the keys and values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # indicates the number of heads for the queries\n",
    "        self.n_heads_q = args.n_heads\n",
    "        # indicates how many times the keys and values should be repeated\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        # indicates the dimension of each head, i.e the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        batch_size, seq_len, _ = x.shape  # (B, 1, dim)\n",
    "        xq = self.wq(x)  # (B, 1, dim) -> (B, 1, H_Q * head_dim)\n",
    "        xk = self.wk(x)  # (B, 1, dim) -> (B, 1, H_KV * head_dim)\n",
    "        xv = self.wv(x)  # (B, 1, dim) -> (B, 1, H_KV * head_dim)\n",
    "\n",
    "        # (B, 1, H_Q * Head_Dim) -> (B, 1, H_Q, Head_Dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "        # Size is the same for xk & xv: (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # Size doesn't change for xq & zk: (B, 1, H_Q, head_dim) -> (B, 1, H_Q, head_dim)\n",
    "        xq = apply_rotary_embeddings(xq, freqs_complex, x.device)\n",
    "        xk = apply_rotary_embeddings(xk, freqs_complex, x.device)\n",
    "\n",
    "        # replace the entry in the cache for this token\n",
    "        self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
    "\n",
    "        # retrieve all the cached keys and values so far\n",
    "        # Size is the same for keys & values: (B, seq_len_kv, H_KV, head_dim)\n",
    "        keys = self.cache_k[:batch_size, : start_pos + seq_len]\n",
    "        values = self.cache_v[:batch_size, : start_pos + seq_len]\n",
    "\n",
    "        # since every group of Q shares the same K & V heads,\n",
    "        # just repeat the K & V heads for every Q in the same group.\n",
    "        # Doesn't look like grouped query attention is being done here since only 70B LLaMA has this feature.\n",
    "        # So this is just multi-head attention.\n",
    "        # Size is the same for keys & values:\n",
    "        # (B, seq_len_kv, H_KV, head_dim) -> (B, seq_len_kv, H_Q, head_dim)\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (B, 1, H_Q, head_dim) -> (B, H_Q, 1, head_dim)\n",
    "        # Size is the same for keys & values:\n",
    "        # (B, seq_len_kv, H_Q, head_dim) -> (B, H_Q, seq_len_kv, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (B, H_Q, 1, head_dim) @ (B, H_Q, head_dim, seq_len_kv) -> (B, H_Q, 1, seq_len_kv)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # (B, H_Q, 1, seq_len_kv) -> (B, H_Q, 1, seq_len_kv)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "\n",
    "        # (B, H_Q, 1, seq_len) @ (B, H_Q, seq_len_kv, head_dim) -> (B, H_Q, 1, head_dim)\n",
    "        output = torch.matmul(scores, values)\n",
    "        # (B, H_Q, 1, head_dim) -> (B, 1, H_Q, head_dim) -> (B, 1, dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.wo(output)  # (B, 1, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = self.dim // self.n_head\n",
    "\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "\n",
    "        # Normalization before the attention block\n",
    "        self.attention_norm = RMSNorm(self.dim, eps=args.norm_eps)\n",
    "        # Normalization before the feed-forward block\n",
    "        self.ffn_norm = RMSNorm(self.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # (B,seq_len,dim) + (B,seq_len,dim) -> (B,seq_len,dim)\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_complex)\n",
    "        # (B,seq_len,dim) + (B,seq_len,dim) -> (B,seq_len,dim)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer vs LLaMA](images/Transformer-vs-LLaMA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        assert args.vocab_size != -1, \"vocab_size must be set\"\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers  # represents Nx in the figure above: 32 layers\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(args.n_layers):\n",
    "            self.layers.append(EncoderBlock(args))\n",
    "\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(\n",
    "            self.args.dim // self.args.n_heads,\n",
    "            self.args.max_seq_len * 2,\n",
    "            device=self.args.device,\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        batch_size, seq_len = tokens.shape  # (B, seq_len)\n",
    "        assert seq_len == 1, \"Only one token at a time can be processed.\"\n",
    "\n",
    "        h = self.tok_embeddings(tokens)  # (B, seq_len) -> (B, seq_len, dim)\n",
    "        # retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
    "        freqs_complex = self.freqs_complex[start_pos : start_pos + seq_len]\n",
    "\n",
    "        # consequently apply all the encoder layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_complex)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing strategies\n",
    "\n",
    "### Logits\n",
    "\n",
    "- The output of the last linear layer in the Transformer model is called logits. The logits represent the unscaled “probabilities”, but they’re not really probabilities because they do not sum up to 1.\n",
    "- The softmax scales all the logits in such a way that they sum up to 1.\n",
    "- The output of the softmax is thus a probability distribution over all the words in the vocabulary, that is, each words in the vocabulary will have a probability associated to it.\n",
    "- But how do we choose the next token, given this distribution? There are many strategies..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy\n",
    "\n",
    "- At every step, we choose the token with the maximum probability, which is appended to the input to generate the next token and so on...\n",
    "- If the initial tokens happens to be the wrong ones, it’s very likely that the next ones will be wrong as well.\n",
    "- It is easy to implement but performs poorly in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search with K=2\n",
    "\n",
    "- Beam search has a parameter K which when set to 2 will choose the top 2 probabilities.\n",
    "\n",
    "Beam Search with K=2 at T=1\n",
    "\n",
    "![Beam Search with K=2 at T=1](images/Beam-Search-with-K2-at-T1.png)\n",
    "\n",
    "Beam Search with K=2 at T=2\n",
    "\n",
    "![Beam Search with K=2 at T=2](images/Beam-Search-with-K2-at-T2.png)\n",
    "\n",
    "Beam Search with K=2 at T=3\n",
    "\n",
    "![Beam Search with K=2 at T=3](images/Beam-Search-with-K2-at-T3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "\n",
    "- At every step, we keep alive the top K paths, all the others are killed.\n",
    "- Increases inferencing time, since at every step must explore K possible options.\n",
    "- Generally, performs better than the greedy strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "- The idea is to scale the logits before applying the softmax\n",
    "- A **low temperature** makes the model **more confident** (the gap between low and high probabilities increases).\n",
    "- A **high temperature** makes the model **less confident** (the gap between low and high probabilities reduces).\n",
    "    ```python\n",
    "    logits = torch.Tensor([-2.5, -3, -0.6])\n",
    "    torch.softmax(logits, dim=0)  # No temperature\n",
    "    >>> tensor([0.1206, 0.0731, 0.8063])\n",
    "\n",
    "    torch.softmax(logits / 0.4, dim=0)  # Low temperature = 0.4 -> High confident\n",
    "    >>> tensor([0.0086, 0.0025, 0.9890])\n",
    "\n",
    "    torch.softmax(logits / 5, dim=0)  # High temperature = 5 -> Less confident\n",
    "    >>> tensor([0.2970, 0.2687, 0.4343])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling\n",
    "\n",
    "- We sample from the random distribution that is output from the softmax.\n",
    "    ```python\n",
    "    logits = torch.Tensor([-2.5, -3, -0.6])\n",
    "    distribution = torch.softmax(logits, dim=0)\n",
    "    distribution\n",
    "    >>> tensor([0.1206, 0.0731, 0.8063])\n",
    "    ```\n",
    "- The first token will be chosen with a probability of 12.06%, the second with a probability of 7.31% and the last one with a probability of 80.63%\n",
    "- The higher the probability, the more likely the probability of it being chosen.\n",
    "- **Problem**: with very little probability it may happen that we choose tokens that are total nonsense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K\n",
    "\n",
    "- In the random sampling strategy, it may happen that we choose words that have very little probability, which usually indicates that the token is unrelated to the previous ones.\n",
    "- With Top K, we keep only the top k highest probabilities, so that tokens with very low probabilities will never be chosen.\n",
    "    ```python\n",
    "    # sort the logits in decreasing order\n",
    "    # we can sort the logits directly because the softmax is a monotonous function\n",
    "    logits, _ = torch.sort(torch.Tensor([-2.5, -3, -2.8, -0.5, -0.6]), descending=True)\n",
    "    k = 2\n",
    "    top_k_logits = logits[:k]\n",
    "    distribution = torch.softmax(top_k_logits, dim=0)\n",
    "    distribution\n",
    "    >>> tensor([0.5250, 0.4750])\n",
    "    ```\n",
    "- **Problem**: given the following distributions, low-probability tokens still make their way into the top k tokens (k = 2)\n",
    "- Distribution 1: **0.5, 0.4**, 0.05, 0.025, 0.025\n",
    "- Distribution 2: **0.9, 0.05**, 0.025, 0.020, 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top P\n",
    "\n",
    "- With Top P, we keep only the tokens with highest probability, such that their cumulative probability is greater than or equal to the parameter p. This way, we get more tokens for distributions that are more “flat” and less tokens for distributions with a very prominent mode.\n",
    "    ```python\n",
    "    # sort the logits in descending order\n",
    "    p = 0.5\n",
    "    logits, _ = torch.sort(torch.Tensor([-2.5, -3, -2.8, -0.5, -0.6]), descending=True)\n",
    "    probs = torch.softmax(logits, dim=0)\n",
    "    print(f\"Probabilities: {probs}\")\n",
    "\n",
    "    probs_cumulative = torch.cumsum(probs, dim=0)\n",
    "    print(f\"Cumulative Probabilities: {probs_cumulative}\")\n",
    "\n",
    "    mask = probs_cumulative - probs > p  # Mask for non-top-p positions\n",
    "    print(f\"Cumulative Probabilities (shifted): {probs_cumulative - probs}\")\n",
    "    print(f\"mask: {mask}\")\n",
    "\n",
    "    probs[mask] = 0.0  # zero out all non-top-k tokens\n",
    "    probs.div_(probs.sum(dim=-1, keepdim=True))  # redistribute probabilities among surviving tokens\n",
    "    print(f\"Top P: {probs}\")\n",
    "\n",
    "    >>> Probabilities: tensor([0.4499, 0.4071, 0.0609, 0.0451, 0.0369])\n",
    "    >>> Cumulative Probabilities: tensor([0.4499, 0.8571, 0.9180, 0.9631, 1.0000])\n",
    "    >>> Cumulative Probabilities (shifted): tensor([0.0000, 0.4499, 0.8571, 0.9180, 0.9631])\n",
    "    >>> mask: tensor([False, False,  True,  True,  True])\n",
    "    >>> Top P: tensor([0.5250, 0.4750, 0.0000, 0.0000, 0.0000])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Transformer,\n",
    "        tokenizer: SentencePieceProcessor,\n",
    "        model_args: ModelArgs,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = model_args\n",
    "\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        checkpoints_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        load_model: bool,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        device: str,\n",
    "    ):\n",
    "        prev_time = time.time()\n",
    "        if load_model:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
    "            assert len(checkpoints) > 0, f\"No checkpoints found in {checkpoints_dir}\"\n",
    "            ckpt_path = checkpoints[0]\n",
    "            print(f\"Loading model from checkpoint: {ckpt_path}\")\n",
    "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            print(f\"Loaded checkpoint in {time.time() - prev_time:.2f} seconds\")\n",
    "            prev_time = time.time()\n",
    "\n",
    "        with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            device=device,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
    "\n",
    "        model = Transformer(model_args).to(device)\n",
    "\n",
    "        if load_model:\n",
    "            # The only unmatched key in the checkpoint is rotary positional encoding freqs\n",
    "            # because we will create it during inference, so we can remove it\n",
    "            del checkpoint[\"rope.freqs\"]\n",
    "            model.load_state_dict(checkpoint, strict=True)\n",
    "            print(f\"Loaded state dict in {time.time() - prev_time:.2f} seconds\")\n",
    "\n",
    "        return LLaMA(model, tokenizer, model_args)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        device: str,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "    ):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.args.max_seq_len - 1\n",
    "\n",
    "        # convert each prompt into tokens\n",
    "        prompt_tokens = [\n",
    "            self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "\n",
    "        # Make sure the batch size is not too large\n",
    "        batch_size = len(prompt_tokens)\n",
    "        assert (\n",
    "            batch_size <= self.args.max_batch_size\n",
    "        ), f\"Batch size {batch_size} must be less than or equal to the Max batch size: {self.args.max_batch_size}\"\n",
    "\n",
    "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "        # Make sure the prompt length is not larger than the max sequence length\n",
    "        assert (\n",
    "            max_prompt_len <= self.args.max_seq_len\n",
    "        ), f\"Prompt length {max_prompt_len} must be less than or equal to the Max sequence length: {self.args.max_seq_len}\"\n",
    "\n",
    "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        # create the list that will contain the generated tokens, along with the initial prompt tokens\n",
    "        pad_id = self.tokenizer.pad_id()\n",
    "        tokens = torch.full(\n",
    "            (batch_size, total_len), pad_id, dtype=torch.long, device=device\n",
    "        )\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            # populate the initial tokens with the prompt tokens\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "\n",
    "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "        # True if the token is a prompt token, False otherwise\n",
    "        prompt_tokens_mask = tokens != pad_id\n",
    "\n",
    "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens...\")\n",
    "        for cur_pos in cur_iterator:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model.forward(tokens[:, cur_pos - 1 : cur_pos], cur_pos)\n",
    "            if temperature > 0:\n",
    "                # The temperature is applied before the softmax\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = self._sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                # greedily select the token with the max probability\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if it is a padding token\n",
    "            next_token = torch.where(\n",
    "                prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "\n",
    "            # EOS is reached only if we found an EOS token for a padding position\n",
    "            eos_reached != (\n",
    "                ~prompt_tokens_mask[:, cur_pos] & (next_token == self.tokenizer.eos_id)\n",
    "            )\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        out_tokens = []\n",
    "        out_text = []\n",
    "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
    "            # cut to the EOS token, if present\n",
    "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
    "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
    "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "            out_tokens.append(current_prompt_tokens)\n",
    "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
    "\n",
    "        return (out_tokens, out_text)\n",
    "\n",
    "    def _sample_top_p(self, probs: torch.Tensor, p: float):\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1)  # (B, vocab_size)\n",
    "        # (substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "        mask = probs_sum - probs_sort > p  # (B, vocab_size)\n",
    "        # zero out all the probabilities of tokens that are not selected by the top_p\n",
    "        probs_sort[mask] = 0.0\n",
    "        # redistribute the probabilities so that they sum up to 1.\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        # sample a token (its index) from the top p distribution\n",
    "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "        # get the token position in the vocabulary corresponding to the sampled index\n",
    "        next_token = torch.gather(probs_idx, -1, next_token)\n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint: models\\llama-2-7b\\consolidated.00.pth\n",
      "Loaded checkpoint in 5.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mukes\\anaconda3\\envs\\torchdl\\lib\\site-packages\\torch\\__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:433.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state dict in 9.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens...: 100%|██████████| 112/112 [00:08<00:00, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that 1) the speed of light is constant in all inertial reference frames, and 2) the laws of physics are the same for all inertial reference frames.Ћ The theory of relativity has two versions: the special theory of relativity and the general theory of relativity. The special theory of relativity applies to all physical phenomena in inertial reference frames. The general theory of relativity applies to the phenomena in non-inertial reference frames. The theory\n",
      "--------------------------------------------------\n",
      "If Google was an Italian company founded in Milan, it would be the largest company in the world, with a market capitalization of $ 713 billion. The company was founded in 1998 by Larry Page and Sergey Brin, two PhD students at Stanford University.\n",
      "The company's name is a reference to the number of letters in the word \"googol\", which is a mathematical term for the number 1 followed by 100 zeros.\n",
      "The company has its headquarters in Mountain View, California\n",
      "--------------------------------------------------\n",
      "Translate English to French:\n",
      "    sea otter => loutre de mer\n",
      "    peppermint => menthe poivrée\n",
      "    plush girafe => girafe peluche\n",
      "    cheese => fromage\n",
      "    \n",
      "    \"\"\"\n",
      "    def translate(self, text):\n",
      "        if text is None:\n",
      "            return None\n",
      "        text = text.strip()\n",
      "        if len(text) == 0:\n",
      "            return None\n",
      "        words = text.split()\n",
      "        for i in\n",
      "--------------------------------------------------\n",
      "Tell me if the following person is actually a Jedi knight disguised as human:\n",
      "    Name: Mukesh Mithrakumar\n",
      "    Decision: \n",
      "    1. I have a good feeling about this.\n",
      "    2. I have a bad feeling about this.\n",
      "\n",
      "    [1]\n",
      "    [2]\n",
      "\n",
      "    [1]\n",
      "    [2]\n",
      "\n",
      "    [1]\n",
      "    [2]\n",
      "\n",
      "    [1]\n",
      "    [2]\n",
      "\n",
      "    [1]\n",
      "   \n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: If you want to rerun this with limited GPU: I am using 16175MiB, RTX 3080 Ti Laptop GPU\n",
    "# You will need to restart the kernal cause jupyter notebook doesn't release the GPU memory\n",
    "# I tried: `torch.cuda.empty_cache()` to release all the GPU memory that can be freed\n",
    "# and `%reset -f` to clear jupyter notebook variables, but neither worked\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_version = \"llama-2-7b\"\n",
    "allow_cuda = True\n",
    "device = \"cuda\" if torch.cuda.is_available() and allow_cuda else \"cpu\"\n",
    "\n",
    "prompts = [\n",
    "    \"Simply put, the theory of relativity states that \",\n",
    "    \"If Google was an Italian company founded in Milan, it would\",\n",
    "    # Few shot promt\n",
    "    \"\"\"Translate English to French:\n",
    "    sea otter => loutre de mer\n",
    "    peppermint => menthe poivrée\n",
    "    plush girafe => girafe peluche\n",
    "    cheese =>\"\"\",\n",
    "    # Zero shot prompt\n",
    "    \"\"\"Tell me if the following person is actually a Jedi knight disguised as human:\n",
    "    Name: Mukesh Mithrakumar\n",
    "    Decision: \n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "model = LLaMA.build(\n",
    "    checkpoints_dir=f\"models/{model_version}\",\n",
    "    tokenizer_path=f\"models/{model_version}/tokenizer.model\",\n",
    "    load_model=True,\n",
    "    max_seq_len=1024,\n",
    "    max_batch_size=len(prompts),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "out_tokens, out_texts = model.text_completion(prompts, device, max_gen_len=64)\n",
    "assert len(out_texts) == len(prompts)\n",
    "for i in range(len(out_texts)):\n",
    "    print(f\"{out_texts[i]}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
