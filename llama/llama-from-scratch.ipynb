{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA From Scratch\n",
    "\n",
    "**References**\n",
    "- *Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm: [Youtube Video](https://youtu.be/oM4VmoabDAI?si=JtlNl00nZeIOkWxx), [Code](https://github.com/hkproj/pytorch-llama)*\n",
    "- *LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU: [Youtube Video](https://youtu.be/Mn_9W1nCFLo?si=4xJy4OzpPX5YxGqx)*\n",
    "- *RoFormer: Enhanced Transformer with Rotary Position Embedding: [Paper](https://arxiv.org/abs/2104.09864)*\n",
    "- *Root Mean Square Layer Normalization: [Paper](https://arxiv.org/abs/1910.07467)*\n",
    "- *Rotary Embeddings: A Relative Revolution: [Blog](https://blog.eleuther.ai/rotary-embeddings/)*\n",
    "- *Transformers Optimization: Part 1 - KV Cache: [Blog](https://r4j4n.github.io/blogs/posts/kv/)*\n",
    "- *The Secret Sauce of LLaMAü¶ô : A Deep Dive!: [Blog](https://r4j4n.github.io/blogs/posts/llama/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td><strong>Training Data</strong></td>\n",
    "        <td><strong>Params</strong></td>\n",
    "        <td><strong>Context length</strong></td>\n",
    "        <td><strong>GQA</strong></td>\n",
    "        <td><strong>Token count</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"4\">Llama 1</td>\n",
    "        <td rowspan=\"4\">See Touvron et al. (2023)</td>\n",
    "        <td>7B</td>\n",
    "        <td>2k</td>\n",
    "        <td>‚ùå</td>\n",
    "        <td>1T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>13B</td>\n",
    "        <td>2k</td>\n",
    "        <td>‚ùå</td>\n",
    "        <td>1T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>33B</td>\n",
    "        <td>2k</td>\n",
    "        <td>‚ùå</td>\n",
    "        <td>1.4T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>65B</td>\n",
    "        <td>2k</td>\n",
    "        <td>‚ùå</td>\n",
    "        <td>1.4T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"4\">Llama 2</td>\n",
    "        <td rowspan=\"4\">A new mix of publicly available online data.</td>\n",
    "        <td>7B</td>\n",
    "        <td>4k</td>\n",
    "        <td>‚ùå</td>\n",
    "        <td rowspan=\"4\">2T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>13B</td>\n",
    "        <td>4k</td>\n",
    "        <td>‚ùå</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>34B</td>\n",
    "        <td>4k</td>\n",
    "        <td>‚úîÔ∏è</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>70B</td>\n",
    "        <td>4k</td>\n",
    "        <td>‚úîÔ∏è</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Llama 3</td>\n",
    "        <td rowspan=\"2\">A new mix of publicly available online data.</td>\n",
    "        <td>8B</td>\n",
    "        <td>8k</td>\n",
    "        <td>‚úîÔ∏è</td>\n",
    "        <td rowspan=\"2\">15T+</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>70B</td>\n",
    "        <td>8k</td>\n",
    "        <td>‚úîÔ∏è</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    # * Unlike the og transformer, we don't need to have the same q, k, v values in LLaMA\n",
    "    n_heads: int = 32  # number of heads for the queries\n",
    "    n_kv_heads: Optional[int] = None  # Number of heads for the keys and values\n",
    "    vocab_size: int = -1  # will be set when we load the tokenizer\n",
    "    # * since grouped query attention heads are reduced,\n",
    "    # * the number of params in the FFN is increased to keep the total number of parameters the same\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5  # epsilon for layer norm\n",
    "\n",
    "    # needed for KV cache\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "    device: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotary Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Absolute Positional Encodings vs Relative Positional Encodings?**\n",
    "- Absolute positional encodings are fixed vectors that are added to the embedding of a token to represent its absolute position in the sentence. So, it deals with one token at a time. You can think of it as the pair (latitude, longitude) on a map: each point on earth will have a unique pair.\n",
    "- Relative positional encodings, on the other hand, deals with two tokens at a time and it is involved when we calculate the attention: since the attention mechanism captures the \"intensity\" of how much two words are related to each other, relative positional encodings tells the attention mechanism the distance between the two words involved in it. So, given two tokens, we create a vector that represent their distance.\n",
    "\n",
    "**The drawbacks of absolute or relative position information**\n",
    "- The vanilla positional encoding is designed for a fixed maximum sequence length. If you have a more extended sequence than the maximum length used during training, handling it becomes problematic. You might need to truncate, split, or find another way to fit it within the maximum length. A model trained with a particular maximum sequence length may not generalize well to sequences of very different lengths, even if they‚Äôre within the allowed range. The positional encoding for these lengths might be outside the distribution seen during training.\n",
    "- The sinusoidal nature of the positional encoding might not always be optimal for capturing very long-term dependencies in long sequences. While self-attention theoretically allows for such connections, in practice, the model might still struggle due to the fixed nature of the encoding.\n",
    "\n",
    "**Rotary Positional Embedding (RoPE)**\n",
    "\n",
    "Rotary Positional Embedding (RoPE) is a new type of position encoding that unifies absolute and relative approaches.\n",
    "\n",
    "For dot-production attention the rotary encoding gives relative attention. so,\n",
    "\n",
    "$$\\bold{q}_m^\\top k_n = (\\bold{R}_{\\Theta, m}^d \\bold{W}_q x_m)^\\top (\\bold{R}_{\\Theta, n}^d \\bold{W}_k x_n) = x^{\\top} \\bold{W}_q R_{\\Theta, n-m}^d \\bold{W}_k x_n$$\n",
    "\n",
    "\n",
    "![Rotary position embedding Overview](images/rotary-position-embedding-overview.png)\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "We would like to find a positional encoding function $f(\\bold{x}, l)$ for an item $\\bold{x}$ and its position $l$ such that, for two items $\\bold{q}$ and $\\bold{k}$ at positions $m$ and $n$, the inner product between $f(\\bold{q}, l)$ and $f(\\bold{k}, n)$ is sensitive only to the values of $\\bold{q}$, $\\bold{k}$, and their relative position $m-n$. This is related in spirit to the kernel trick: we are searching for a feature map such that its kernel has certain properties. A key piece of information is the geometric definition of the dot product between Euclidean vectors: $\\bold{q} \\cdot \\bold{k} = \\| \\bold{q} \\| \\| \\bold{k} \\| cos(\\theta_{qk})$\n",
    "\n",
    "In plain english, the intuition behind RoPE is that we can represent the token embeddings as complex numbers and their positions as pure rotations that we apply to them. If we shift both the query and key by the same amount, changing absolute position but not relative position, this will lead both representations to be additionally rotated in the same manner, thus the angle between them will remain unchanged and thus the dot product will also remain unchanged. By exploiting the nature of rotations, the dot product used in self-attention will have the property we are looking for, preserving relative positional information while discarding absolute position.\n",
    "\n",
    "**The RoPE Solution further explained in simpler terms**\n",
    "\n",
    "1. Imagine Words as Positions on a Circle: Think of each word's embedding (its numerical representation) as a point on a circle.\n",
    "2. Positions as Rotations: Instead of using separate values for each word's position, RoPE uses rotations. Words closer together experience smaller rotations, while further words have larger rotations.\n",
    "3. Shifting Together Keeps Things Relative: Since both the \"query\" (looking for information) and \"key\" (holding information) embeddings are rotated by the same amount when considering relative position, the angle between them stays the same. This, in turn, keeps the \"dot product\" (a measure of similarity) between them unchanged.\n",
    "\n",
    "**How is this different from the sinusoidal embeddings used in \"Attention is All You Need\"?**\n",
    "\n",
    "There are two ways that rotary embeddings are different from sinusoidal embeddings:\n",
    "- Sinusoidal embeddings apply to each coordinate individually, while rotary embeddings mix pairs of coordinates\n",
    "- Sinusoidal embeddings add a $cos (m \\theta)$ or $sin (m \\theta)$ term, while rotary embeddings use a multiplicative factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute Theta Positional Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the steps involved in precomputing theta positional frequencies:\n",
    "\n",
    "![Precompute Theta Positional Frequencies Steps](images/theta-pos-freq-steps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_pos_frequencies(\n",
    "    head_dim: int, seq_len: int, device: str, theta: float = 10000.0\n",
    "):\n",
    "    # theta 10000.0 is the default value in the paper\n",
    "    # As written in the paragraph 3.2.2 of the paper\n",
    "    # >> In order to generalize our results in 2D to any xi ‚àà Rd where **d is even**, [...]\n",
    "    assert (\n",
    "        head_dim % 2 == 0\n",
    "    ), \"Dimension must be even since rotary embedding can't be applied to odd.\"\n",
    "\n",
    "    # Build the theta parameter\n",
    "    # According to the formula theta_i = 10000^(-2(i-1)/dim) for i = [1, 2, ..., dim/2]\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()  # (head_dim / 2)\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)  # (dim / 2)\n",
    "    # construct the positions (the \"m\" parameter)\n",
    "    m = torch.arange(seq_len, device=device)  # (seq_len)\n",
    "    # Multiply each theta by each position using the outer product.\n",
    "    # (seq_len), outer_product*(head_dim/2) -> (seq_len,head_dim/2)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    # we can compute complex numbers in the polar form c = R*exp(m*theta), where R=1 as follow:\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Steps in calculating the Rotary Embedding:\n",
    "![The Steps in calculating the Rotary Embedding](images/rotary-embedding-steps.png)\n",
    "\n",
    "Figure 1: Implementation of Rotary Position Embedding(RoPE):\n",
    "![Implementation of Rotary Position Embedding](images/implementation-of-rope.png)\n",
    "\n",
    "**Practical Considerations**\n",
    "- The rotary position embeddings are only applied to the query and the keys, but not the values.\n",
    "- The rotary position embeddings are applied after the vector q and k have been multiplied by the W matrix in the attention mechanism, while in the vanilla transformer they're applied before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    # * OP 1 & 2 >>\n",
    "    # seperate the last dimension pairs of 2 values, representing the real & imaginary parts of the complex number\n",
    "    # two consecutive values will become a single complex number\n",
    "    # (B,seq_len,H,head_dim) -> (B,seq_len,H,head_dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    # reshape the freqs_complex tensor to match the shape of the x_complex tensor.\n",
    "    # So we need to add the batch dimension and the head dimension.\n",
    "    # (seq_len,head_dim/2) -> (1,seq_len,1,head_dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    # * OP 3 >>\n",
    "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
    "    # which results in the rotation of the complex number as shown in the Figure 1 of the paper.\n",
    "    # (B,seq_len,H,head_dim/2)*(1,seq_len,1,head_dim/2) -> (B,seq_len,H,head_dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    # * OP 4 >> convert the complex number back to the real number\n",
    "    # (B,seq_len,H,head_dim/2) -> (B,seq_len,H,head_dim/2,2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    # * OP 5 >> Flattening to the shape of the original tensor\n",
    "    # (B,seq_len,H,head_dim/2,2) -> (B,seq_len,H,head_dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Internal covariate shift**\n",
    "\n",
    "Internal covariate shift refers to the *gradual change in the distribution of data* as it flows through the network's layers. As training progresses, the weights in the earlier layers are updated based on the input data. These weight changes alter the way the data is transformed between layers. Consequently, the distribution of the data (activation values) at each layer changes compared to the initial distribution. This shift in distribution across layers can disrupt the learning process in deeper networks. Neurons in later layers have to constantly adapt to the changing inputs they receive, making it harder for the network to converge on a stable set of weights. Normalization techniques, like batch normalization and layer normalization, address internal covariate shift by essentially standardizing the data at each layer.\n",
    "- Batch Normalization: This technique normalizes the activations of each mini-batch of data presented to a layer. With batch normalization we *normalize by columns (features)*.\n",
    "- Layer Normalization: This technique normalizes the activations of each neuron within a layer, independent of the mini-batch. With layer normalization we *normalize by rows (data items)*.\n",
    "\n",
    "**Root Mean Square Normalization**\n",
    "\n",
    "LayerNorm works because of its re-centering and re-scaling invariance property. Re-centering enables the model to be insensitive to shift noises on both inputs and weights, and re-scaling keeps the output representations intact when both inputs and weights are randomly scaled. \n",
    "\n",
    "The RMS Normalizaiton paper hypothesize that the re-scaling invariance is the reason for success of LayerNorm, rather than re-centering invariance and they propose RMSNorm which only focuses on re-scaling invariance and regularizes the summed inputs simply according to the root mean square (RMS) statistic:\n",
    "\n",
    "$$\n",
    "\\bar{a}_i= \\frac{a_i}{RMS(a)} g_i \\\\\n",
    "\\text{where} \\ RMS(a) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n a_{i}^2}\n",
    "$$\n",
    "\n",
    "Intuitively, RMSNorm simplifies LayerNorm by totally removing the mean statistic at the cost of sacrificing the invariance that mean normalization affords. This helps in reducing the computation cost compared to Layer Normalization and also works well in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))  # The gamma parameter\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # rsqrt: 1 / sqrt(x)\n",
    "        # (B,seq_len,dim)*(B,seq_len,1) -> (B,seq_len,dim)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (dim)*(B,seq_len,dim) -> (B,seq_len,dim)\n",
    "        return self.weight * self._norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "### SwiGLU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SwiGLU**\n",
    "\n",
    "SwiGLU is a combination of the Swish activation function and the Gated Linear Unit (GLU) concept. It was introduced in the paper ‚ÄúGLU Variants Improve Transformer‚Äù (Sho Takase, Naoaki Okazaki, 2020). The authors propose several variations of the standard GLU that can improve performance on machine translation tasks when used in a Transformer model.\n",
    "\n",
    "The SwiGLU variant is defined as:\n",
    "$$\\mathit{SwiGLU}(x, x') = x \\odot \\mathit{Swish}(x')$$\n",
    "\n",
    "where $\\odot$ is the element-wise multiplication operation, and x' is the transformed input (generally, a linear transformation of the input x).\n",
    "\n",
    "**Swish**\n",
    "\n",
    "Swish is a smooth version of ReLU with a non-zero gradient for negative values. It is a smooth, non-monotonic function that consistently matches or outperforms ReLU.\n",
    "\n",
    "Simply put, Swish is an extension of the SILU activation function. SILU's formula $f(x) = x * \\mathit{sigmoid}(x)$. The slight modification made in the Swish formulation is the addition of a trainable $\\beta$ parameter, making it $f(x) = x \\mathit{sigmoid}(\\beta x)$.\n",
    "\n",
    "In contrast to ReLU, which is a piecewise linear function, swish is a smooth, continuous function that allows small number of negative weights to pass through unlike ReLU which sets all negative weights to zero. This non-monotonic property is particularly beneficial in deep neural networks. A non-monotonic function is a type of function that does not consistently increase or decrease in value.\n",
    "\n",
    "The trainable parameter $\\beta$ enables the activation function to be fine-tuned more effectively to optimize information propagation and push for smoother gradients.\n",
    "\n",
    "**Gated Linear Unit (GLU)**\n",
    "\n",
    "GLU (Gated Linear Units) is a layer within a neural network, rather than a strict activation function. It involves a linear transformation followed by a gating process. This gating process is controlled by a sigmoid function that manages the information flow from the linear transformation.\n",
    "\n",
    "$$h_l (\\mathbf{X}) = (\\mathbf{X} \\ast \\mathbf{W} + b) \\otimes \\sigma (\\mathbf{X} \\mathbf{V} + \\mathbf{c})$$\n",
    "\n",
    "$\\sigma$ means the sigmoid function. So we have two sets of weights W and V, and two biases, b, and c. The idea is simple. I want to allow the network to decide how much information should flow through a given path, like a logical gate, hence the name. How?\n",
    "- If we multiply X by 0, nothing passes.\n",
    "- If we multiply X by 1, everything passes.\n",
    "- If we multiply X by 0.5, half of it passes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = args.multiple_of * (\n",
    "            (hidden_dim + args.multiple_of - 1) // args.multiple_of\n",
    "        )\n",
    "\n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        swish = F.silu(self.w1(x))  # (B, seq_len, dim) -> (B, seq_len, hidden_dim)\n",
    "        x_v = self.w3(x)  # (B, seq_len, dim) -> (B, seq_len, hidden_dim)\n",
    "        # (B, seq_len, hidden_dim) * (B, seq_len, hidden_dim) -> (B, seq_len, hidden_dim)\n",
    "        x = swish * x_v\n",
    "        x = self.w2(x)  # (B, seq_len, hidden_dim) -> (B, seq_len, dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KV Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is KV Cache?**\n",
    "\n",
    "A common technique for improving the performance of large model inferences is by using the KV cache of the last inference. Using the KV cache of the last inference improves inference performance and reduces end-to-end latency without affecting any accuracy.\n",
    "\n",
    "**Why KV Cache?**\n",
    "\n",
    "While generating text (tokens) in autoregressive language models like GPT, all the previously generated tokens are fed into the network when generating a new token. Here, the hidden representation of the previously generated tokens needs to be recalculated each time a new token is generated. This causes a lot of computational waste.\n",
    "\n",
    "As the input tokens for each inference process become longer, it increases inference FLOPs (floating point operations). KV cache solves this problem by storing hidden representations of previously computed key-value pairs while generating a new token.\n",
    "\n",
    "Consider a transformer architecture with 12 attention heads and KV Cache. The following figure represents the transformer state while generating 9th token of the input sequence.\n",
    "\n",
    "![Multi-headed Attention with KV Cache](images/Multi-headed-Attention-with-KV-Cache.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention during Next Token Prediction Task at Inference T=1:\n",
    "\n",
    "![Self-Attention during Next Token Prediction Task at T1](images/Self-Attention-during-NTP-Task-T1.png)\n",
    "\n",
    "Self-Attention during Next Token Prediction Task at Inference T=4:\n",
    "\n",
    "![Self-Attention during Next Token Prediction Task at T4](images/Self-Attention-during-NTP-Task-T4.png)\n",
    "\n",
    "Where KV Cache is useful:\n",
    "\n",
    "![Where KV Cache is useful](images/where-kv-cache-come-in.png)\n",
    "\n",
    "Self-Attention with KV-Cache at Inference T=1:\n",
    "\n",
    "![Self-Attention with KV-Cache at T1](images/Self-Attention-with-KV-Cache-T1.png)\n",
    "\n",
    "Self-Attention with KV-Cache at Inference T=4:\n",
    "\n",
    "![Self-Attention with KV-Cache at T4](images/Self-Attention-with-KV-Cache-T4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, None, :]  # (B, seq_len, n_kv_heads, 1, head_dim)\n",
    "        .expand(\n",
    "            batch_size, seq_len, n_kv_heads, n_rep, head_dim\n",
    "        )  # (B, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(\n",
    "            batch_size, seq_len, n_kv_heads * n_rep, head_dim\n",
    "        )  # (B, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla batched Multi-Head Attention\n",
    "\n",
    "- Multihead Attention as presented in the original paper \"Attention is all you need\".\n",
    "- By setting ùëö = ùëõ (sequence length of query = seq. length of keys and values)\n",
    "- The number of arithmetic operations performed is $O(bnd^2)$\n",
    "- The total memory involved in the operations, given by the sum of all the tensors involved in the calculations (including the derived ones!) is $O(bnd + bhn^2 + d^2)$ \n",
    "- The ratio between the total memory and the number of arithmetic operations is $O (\\frac{1}{k} + \\frac{1}{bn})$\n",
    "- In this case, the ratio is much smaller than 1, which means that the number of memory access we are performing is much less than the number of arithmetic operations, so the memory access is not the bottleneck here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiHeadAttentionBatched():\n",
    "    d, m, n, b, h, k, v = 512, 10, 10, 32, 8, (512 // 8), (512 // 8)\n",
    "\n",
    "    X = torch.rand(b, n, d)  # Query\n",
    "    M = torch.rand(b, m, d)  # Key and Value\n",
    "    mask = torch.rand(b, h, n, m)\n",
    "    P_q = torch.rand(h, d, k)  # W_q\n",
    "    P_k = torch.rand(h, d, k)  # W_k\n",
    "    P_v = torch.rand(h, d, v)  # W_v\n",
    "    P_o = torch.rand(h, d, v)  # W_o\n",
    "\n",
    "    Q = torch.einsum(\"bnd,hdk->bhnk\", X, P_q)\n",
    "    K = torch.einsum(\"bmd,hdk->bhmk\", M, P_k)\n",
    "    V = torch.einsum(\"bmd,hdv->bhmv\", M, P_v)\n",
    "\n",
    "    logits = torch.einsum(\"bhnk,bhmk->bhnm\", Q, K)\n",
    "    weights = torch.softmax(logits + mask, dim=-1)\n",
    "\n",
    "    O = torch.einsum(\"bhnm,bhmv->bhnv\", weights, V)\n",
    "    Y = torch.einsum(\"bhnv,hdv->bnd\", O, P_o)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batched Multi-Head Attention with KV cache\n",
    "\n",
    "- Uses the KV cache to reduce the number of operations performed.\n",
    "- By setting ùëö = ùëõ (sequence length of query = seq. length of keys and values)\n",
    "- The number of arithmetic operations performed is $O(bnd^2)$\n",
    "- The total memory involved in the operations, given by the sum of all the tensors involved in the calculations (including the derived ones!) is $O(bn^2d + nd^2)$\n",
    "- The ratio between the total memory and the number of arithmetic operations is $O (\\frac{n}{d} + \\frac{1}{b})$\n",
    "- When ùëõ ‚âà ùëë (the sequence length is close to the size of the embedding vector) or ùëè ‚âà 1 (the batch size is 1), the ratio becomes 1 and the memory access now becomes the bottleneck of the algorithm. For the batch size is not a problem, since it is generally much higher than 1, while for the ùëõ/ùëë term, we need to reduce the sequence length. But there‚Äôs a better way...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiHeadSelfAttentionIncremental():\n",
    "    d, b, h, k, v = 512, 32, 8, (512 // 8), (512 // 8)\n",
    "\n",
    "    m = 5  # Suppose we have already cached \"m\" tokens\n",
    "    prev_K = torch.rand(b, h, m, k)\n",
    "    prev_V = torch.rand(b, h, m, v)\n",
    "\n",
    "    X = torch.rand(b, d)  # Query\n",
    "    M = torch.rand(b, d)  # Key and Value\n",
    "    P_q = torch.rand(h, d, k)  # W_q\n",
    "    P_k = torch.rand(h, d, k)  # W_k\n",
    "    P_v = torch.rand(h, d, v)  # W_v\n",
    "    P_o = torch.rand(h, d, v)  # W_o\n",
    "\n",
    "    q = torch.einsum(\"bd,hdk->bhk\", X, P_q)\n",
    "    K = torch.concat([prev_K, torch.einsum(\"bd,hdk->bhk\", M, P_k).unsqueeze(2)], axis=2)\n",
    "    V = torch.concat([prev_V, torch.einsum(\"bd,hdv->bhv\", M, P_v).unsqueeze(2)], axis=2)\n",
    "\n",
    "    logits = torch.einsum(\"bhk,bhmk->bhnm\", q, K)\n",
    "    weights = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    O = torch.einsum(\"bhm,bhmv->bhv\", weights, V)\n",
    "    Y = torch.einsum(\"bhv,hdv->bd\", O, P_o)\n",
    "    return Y, K, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Query Attention with KV cache\n",
    "\n",
    "- We remove the ‚Ñé dimension from the ùêæ and the ùëâ, while keeping it for the ùëÑ. This means that all the different query heads will share the same keys and values.\n",
    "- The number of arithmetic operations performed is $O(bnd^2)$\n",
    "- The total memory involved in the operations, given by the sum of all the tensors involved in the calculations (including the derived ones!) is $O(bnd + bn^2k + nd^2)$\n",
    "- The ratio between the total memory and the number of arithmetic operations is $O (\\frac{1}{d} + \\frac{n}{dh} + \\frac{1}{b})$\n",
    "- Comparing with the previous approach, we have reduced the expensive term ùëõ/ùëë by a factor of h.\n",
    "- The performance gains are important, while the model's quality degrades only a little bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiquerySelfAttentionIncremental():\n",
    "    d, b, h, k, v = 512, 32, 8, (512 // 8), (512 // 8)\n",
    "\n",
    "    m = 5  # Suppose we have already cached \"m\" tokens\n",
    "    prev_K = torch.rand(b, m, k)\n",
    "    prev_V = torch.rand(b, m, v)\n",
    "\n",
    "    X = torch.rand(b, d)  # Query\n",
    "    M = torch.rand(b, d)  # Key and Value\n",
    "    P_q = torch.rand(h, d, k)  # W_q\n",
    "    P_k = torch.rand(d, k)  # W_k\n",
    "    P_v = torch.rand(d, v)  # W_v\n",
    "    P_o = torch.rand(d, v)  # W_o\n",
    "\n",
    "    q = torch.einsum(\"bd,hdk->bhk\", X, P_q)\n",
    "    K = torch.concat([prev_K, torch.einsum(\"bd,dk->bk\", M, P_k).unsqueeze(1)], axis=1)\n",
    "    V = torch.concat([prev_V, torch.einsum(\"bd,dv->bv\", M, P_v).unsqueeze(1)], axis=1)\n",
    "\n",
    "    logits = torch.einsum(\"bhk,bmk->bhm\", q, K)\n",
    "    weights = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    O = torch.einsum(\"bhm,bmv->bhv\", weights, V)\n",
    "    Y = torch.einsum(\"bhv,hdv->bd\", O, P_o)\n",
    "    return Y, K, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped Multi-Query Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped Multi-Query Attention is a compromise between Multi-Head Attention and Multi-Query Attention:\n",
    "\n",
    "![Grouped Multi-Query Attention compared](images/Grouped-Multi-Query-Attention-compared.png)\n",
    "\n",
    "GQA can be thought of as a way to optimize the attention mechanism in transformer-based models. Instead of computing attention for each query independently, GQA groups queries together and computes their attention jointly. This reduces the number of attention computations, leading to faster inference times.\n",
    "\n",
    "However, while MQA drastically speeds up decoder inference, it can lead to quality degradation. To address this, GQA was introduced as a generalization of MQA, using an intermediate number of key-value heads, which is more than one but less than the number of query heads.\n",
    "\n",
    "In GQA, query heads are divided into groups, each of which shares a single key head and value head. This approach allows GQA to interpolate between multi-head and multi-query attention, achieving a balance between quality and speed. For instance, GQA with a single group (and therefore a single key and value head) is equivalent to MQA, while GQA with groups equal to the number of heads is equivalent to MHA.\n",
    "\n",
    "**What are some common methods for implementing Grouped Query Attention?**\n",
    "\n",
    "Common methods for implementing Grouped Query Attention (GQA) include:\n",
    "- Grouping queries based on similarity ‚Äî One popular method for implementing GQA is to group queries based on their similarity. This involves computing a similarity metric between queries and then assigning them to groups accordingly.\n",
    "- Dividing query heads into groups ‚Äî In GQA, query heads are divided into groups, each of which shares a single key head and value head. This approach allows GQA to interpolate between multi-head and multi-query attention, achieving a balance between quality and speed.\n",
    "- Using an intermediate number of key-value heads ‚Äî GQA strikes a balance between multi-query attention (MQA) and multi-head attention (MHA) by using an intermediate number of key-value heads, which is more than one but less than the number of query heads.\n",
    "- Repeating key-value pairs for computational efficiency ‚Äî In GQA, key-value pairs are repeated to optimize performance while maintaining quality. This is achieved by repeating key-value pairs n_rep times, where n_rep corresponds to the number of query heads that share the same key-value pair.\n",
    "\n",
    "These methods can be combined and adapted to suit the specific requirements of a given task or model architecture.\n",
    "\n",
    "**What are some challenges associated with Grouped Query Attention?**\n",
    "\n",
    "There are several challenges associated with GQA:\n",
    "- Quality Degradation and Training Instability ‚Äî GQA is an evolution of Multi-Query Attention (MQA), which uses multiple query heads but a single key and value head. While MQA speeds up decoder inference, it can lead to quality degradation and training instability. GQA attempts to mitigate this by using an intermediate number of key-value heads (more than one but fewer than the query heads), but the balance between speed and quality is a challenge.\n",
    "- Memory Bandwidth Overhead ‚Äî Autoregressive decoder inference is a severe bottleneck for Transformer models due to the memory bandwidth overhead from loading decoder weights and all attention keys and values at every decoding step. GQA attempts to address this by dividing query heads into groups, each of which shares a single key head and value head. However, managing this memory bandwidth overhead is a significant challenge.\n",
    "- Complexity of Implementation ‚Äî Implementing GQA within the context of an autoregressive decoder using a Transformer model can be complex. It involves repeating key-value pairs for computational efficiency, managing cached key-value pairs, and performing scaled dot-product attention computation.\n",
    "- Group Division ‚Äî The input nodes are divided into several groups and attention is calculated only within that local block. If the total number of nodes cannot be divided by the group length, zero-padded nodes are added to match the length. This division and management of groups add to the complexity of the GQA implementation.\n",
    "- Hyperparameter Tuning ‚Äî Achieving optimal performance with GQA requires careful tuning of hyperparameters. For instance, the number of groups into which the query heads are divided can significantly impact the model's performance and efficiency.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # indicates the number of heads for the keys and values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # indicates the number of heads for the queries\n",
    "        self.n_heads_q = args.n_heads\n",
    "        # indicates how many times the keys and values should be repeated\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        # indicates the dimension of each head, i.e the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        batch_size, seq_len, _ = x.shape  # (B, 1, dim)\n",
    "        xq = self.wq(x)  # (B, 1, dim) -> (B, 1, H_Q * head_dim)\n",
    "        xk = self.wk(x)  # (B, 1, dim) -> (B, 1, H_KV * head_dim)\n",
    "        xv = self.wv(x)  # (B, 1, dim) -> (B, 1, H_KV * head_dim)\n",
    "\n",
    "        # (B, 1, H_Q * Head_Dim) -> (B, 1, H_Q, Head_Dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "        # Size is the same for xk & xv: (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # Size doesn't change for xq & zk: (B, 1, H_Q, head_dim) -> (B, 1, H_Q, head_dim)\n",
    "        xq = apply_rotary_embeddings(xq, freqs_complex, x.device)\n",
    "        xk = apply_rotary_embeddings(xk, freqs_complex, x.device)\n",
    "\n",
    "        # replace the entry in the cache for this token\n",
    "        self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
    "\n",
    "        # retrieve all the cached keys and values so far\n",
    "        # Size is the same for keys & values: (B, seq_len_kv, H_KV, head_dim)\n",
    "        keys = self.cache_k[:batch_size, : start_pos + seq_len]\n",
    "        values = self.cache_v[:batch_size, : start_pos + seq_len]\n",
    "\n",
    "        # since every group of Q shares the same K & V heads,\n",
    "        # just repeat the K & V heads for every Q in the same group.\n",
    "        # Doesn't look like grouped query attention is being done here since only 70B LLaMA has this feature.\n",
    "        # So this is just multi-head attention.\n",
    "        # Size is the same for keys & values:\n",
    "        # (B, seq_len_kv, H_KV, head_dim) -> (B, seq_len_kv, H_Q, head_dim)\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (B, 1, H_Q, head_dim) -> (B, H_Q, 1, head_dim)\n",
    "        # Size is the same for keys & values:\n",
    "        # (B, seq_len_kv, H_Q, head_dim) -> (B, H_Q, seq_len_kv, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (B, H_Q, 1, head_dim) @ (B, H_Q, head_dim, seq_len_kv) -> (B, H_Q, 1, seq_len_kv)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # (B, H_Q, 1, seq_len_kv) -> (B, H_Q, 1, seq_len_kv)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "\n",
    "        # (B, H_Q, 1, seq_len) @ (B, H_Q, seq_len_kv, head_dim) -> (B, H_Q, 1, head_dim)\n",
    "        output = torch.matmul(scores, values)\n",
    "        # (B, H_Q, 1, head_dim) -> (B, 1, H_Q, head_dim) -> (B, 1, dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.wo(output)  # (B, 1, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = self.dim // self.n_head\n",
    "\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "\n",
    "        # Normalization before the attention block\n",
    "        self.attention_norm = RMSNorm(self.dim, eps=args.norm_eps)\n",
    "        # Normalization before the feed-forward block\n",
    "        self.ffn_norm = RMSNorm(self.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # (B,seq_len,dim) + (B,seq_len,dim) -> (B,seq_len,dim)\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_complex)\n",
    "        # (B,seq_len,dim) + (B,seq_len,dim) -> (B,seq_len,dim)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Model: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer vs LLaMA](images/Transformer-vs-LLaMA.png)\n",
    "\n",
    "- Llama is an encoder only model.\n",
    "- RMS Norm is before the attention, unlike in Vanilla Transformer\n",
    "- The block inside Nx is repeated N times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        assert args.vocab_size != -1, \"vocab_size must be set\"\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers  # represents Nx in the figure above: 32 layers\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(args.n_layers):\n",
    "            self.layers.append(EncoderBlock(args))\n",
    "\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(\n",
    "            self.args.dim // self.args.n_heads,\n",
    "            self.args.max_seq_len * 2,\n",
    "            device=self.args.device,\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        batch_size, seq_len = tokens.shape  # (B, seq_len)\n",
    "        assert seq_len == 1, \"Only one token at a time can be processed.\"\n",
    "\n",
    "        h = self.tok_embeddings(tokens)  # (B, seq_len) -> (B, seq_len, dim)\n",
    "        # retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
    "        freqs_complex = self.freqs_complex[start_pos : start_pos + seq_len]\n",
    "\n",
    "        # consequently apply all the encoder layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_complex)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits\n",
    "\n",
    "- The output of the last linear layer in the Transformer model is called logits. The logits represent the unscaled ‚Äúprobabilities‚Äù, but they‚Äôre not really probabilities because they do not sum up to 1.\n",
    "- The softmax scales all the logits in such a way that they sum up to 1.\n",
    "- The output of the softmax is thus a probability distribution over all the words in the vocabulary, that is, each words in the vocabulary will have a probability associated to it.\n",
    "- But how do we choose the next token, given this distribution? There are many strategies..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy\n",
    "\n",
    "- At every step, we choose the token with the maximum probability, which is appended to the input to generate the next token and so on...\n",
    "- If the initial tokens happens to be the wrong ones, it‚Äôs very likely that the next ones will be wrong as well.\n",
    "- It is easy to implement but performs poorly in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search with K=2\n",
    "\n",
    "- Beam search has a parameter K which when set to 2 will choose the top 2 probabilities.\n",
    "\n",
    "Beam Search with K=2 at T=1\n",
    "\n",
    "![Beam Search with K=2 at T=1](images/Beam-Search-with-K2-at-T1.png)\n",
    "\n",
    "Beam Search with K=2 at T=2\n",
    "\n",
    "![Beam Search with K=2 at T=2](images/Beam-Search-with-K2-at-T2.png)\n",
    "\n",
    "Beam Search with K=2 at T=3\n",
    "\n",
    "![Beam Search with K=2 at T=3](images/Beam-Search-with-K2-at-T3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "\n",
    "- At every step, we keep alive the top K paths, all the others are killed.\n",
    "- Increases inferencing time, since at every step must explore K possible options.\n",
    "- Generally, performs better than the greedy strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "- The idea is to scale the logits before applying the softmax\n",
    "- A **low temperature** makes the model **more confident** (the gap between low and high probabilities increases).\n",
    "- A **high temperature** makes the model **less confident** (the gap between low and high probabilities reduces).\n",
    "    ```python\n",
    "    logits = torch.Tensor([-2.5, -3, -0.6])\n",
    "    torch.softmax(logits, dim=0)  # No temperature\n",
    "    >>> tensor([0.1206, 0.0731, 0.8063])\n",
    "\n",
    "    torch.softmax(logits / 0.4, dim=0)  # Low temperature = 0.4 -> High confident\n",
    "    >>> tensor([0.0086, 0.0025, 0.9890])\n",
    "\n",
    "    torch.softmax(logits / 5, dim=0)  # High temperature = 5 -> Less confident\n",
    "    >>> tensor([0.2970, 0.2687, 0.4343])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling\n",
    "\n",
    "- We sample from the random distribution that is output from the softmax.\n",
    "    ```python\n",
    "    logits = torch.Tensor([-2.5, -3, -0.6])\n",
    "    distribution = torch.softmax(logits, dim=0)\n",
    "    distribution\n",
    "    >>> tensor([0.1206, 0.0731, 0.8063])\n",
    "    ```\n",
    "- The first token will be chosen with a probability of 12.06%, the second with a probability of 7.31% and the last one with a probability of 80.63%\n",
    "- The higher the probability, the more likely the probability of it being chosen.\n",
    "- **Problem**: with very little probability it may happen that we choose tokens that are total nonsense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K\n",
    "\n",
    "- In the random sampling strategy, it may happen that we choose words that have very little probability, which usually indicates that the token is unrelated to the previous ones.\n",
    "- With Top K, we keep only the top k highest probabilities, so that tokens with very low probabilities will never be chosen.\n",
    "    ```python\n",
    "    # sort the logits in decreasing order\n",
    "    # we can sort the logits directly because the softmax is a monotonous function\n",
    "    logits, _ = torch.sort(torch.Tensor([-2.5, -3, -2.8, -0.5, -0.6]), descending=True)\n",
    "    k = 2\n",
    "    top_k_logits = logits[:k]\n",
    "    distribution = torch.softmax(top_k_logits, dim=0)\n",
    "    distribution\n",
    "    >>> tensor([0.5250, 0.4750])\n",
    "    ```\n",
    "- **Problem**: given the following distributions, low-probability tokens still make their way into the top k tokens (k = 2)\n",
    "- Distribution 1: **0.5, 0.4**, 0.05, 0.025, 0.025\n",
    "- Distribution 2: **0.9, 0.05**, 0.025, 0.020, 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top P\n",
    "\n",
    "- With Top P, we keep only the tokens with highest probability, such that their cumulative probability is greater than or equal to the parameter p. This way, we get more tokens for distributions that are more ‚Äúflat‚Äù and less tokens for distributions with a very prominent mode.\n",
    "    ```python\n",
    "    # sort the logits in descending order\n",
    "    p = 0.5\n",
    "    logits, _ = torch.sort(torch.Tensor([-2.5, -3, -2.8, -0.5, -0.6]), descending=True)\n",
    "    probs = torch.softmax(logits, dim=0)\n",
    "    print(f\"Probabilities: {probs}\")\n",
    "\n",
    "    probs_cumulative = torch.cumsum(probs, dim=0)\n",
    "    print(f\"Cumulative Probabilities: {probs_cumulative}\")\n",
    "\n",
    "    mask = probs_cumulative - probs > p  # Mask for non-top-p positions\n",
    "    print(f\"Cumulative Probabilities (shifted): {probs_cumulative - probs}\")\n",
    "    print(f\"mask: {mask}\")\n",
    "\n",
    "    probs[mask] = 0.0  # zero out all non-top-k tokens\n",
    "    probs.div_(probs.sum(dim=-1, keepdim=True))  # redistribute probabilities among surviving tokens\n",
    "    print(f\"Top P: {probs}\")\n",
    "\n",
    "    >>> Probabilities: tensor([0.4499, 0.4071, 0.0609, 0.0451, 0.0369])\n",
    "    >>> Cumulative Probabilities: tensor([0.4499, 0.8571, 0.9180, 0.9631, 1.0000])\n",
    "    >>> Cumulative Probabilities (shifted): tensor([0.0000, 0.4499, 0.8571, 0.9180, 0.9631])\n",
    "    >>> mask: tensor([False, False,  True,  True,  True])\n",
    "    >>> Top P: tensor([0.5250, 0.4750, 0.0000, 0.0000, 0.0000])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Model: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Transformer,\n",
    "        tokenizer: SentencePieceProcessor,\n",
    "        model_args: ModelArgs,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = model_args\n",
    "\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        checkpoints_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        load_model: bool,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        device: str,\n",
    "    ):\n",
    "        prev_time = time.time()\n",
    "        if load_model:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
    "            assert len(checkpoints) > 0, f\"No checkpoints found in {checkpoints_dir}\"\n",
    "            ckpt_path = checkpoints[0]\n",
    "            print(f\"Loading model from checkpoint: {ckpt_path}\")\n",
    "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            print(f\"Loaded checkpoint in {time.time() - prev_time:.2f} seconds\")\n",
    "            prev_time = time.time()\n",
    "\n",
    "        with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            device=device,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
    "\n",
    "        model = Transformer(model_args).to(device)\n",
    "\n",
    "        if load_model:\n",
    "            # The only unmatched key in the checkpoint is rotary positional encoding freqs\n",
    "            # because we will create it during inference, so we can remove it\n",
    "            del checkpoint[\"rope.freqs\"]\n",
    "            model.load_state_dict(checkpoint, strict=True)\n",
    "            print(f\"Loaded state dict in {time.time() - prev_time:.2f} seconds\")\n",
    "\n",
    "        return LLaMA(model, tokenizer, model_args)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        device: str,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "    ):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.args.max_seq_len - 1\n",
    "\n",
    "        # convert each prompt into tokens\n",
    "        prompt_tokens = [\n",
    "            self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "\n",
    "        # Make sure the batch size is not too large\n",
    "        batch_size = len(prompt_tokens)\n",
    "        assert (\n",
    "            batch_size <= self.args.max_batch_size\n",
    "        ), f\"Batch size {batch_size} must be less than or equal to the Max batch size: {self.args.max_batch_size}\"\n",
    "\n",
    "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "        # Make sure the prompt length is not larger than the max sequence length\n",
    "        assert (\n",
    "            max_prompt_len <= self.args.max_seq_len\n",
    "        ), f\"Prompt length {max_prompt_len} must be less than or equal to the Max sequence length: {self.args.max_seq_len}\"\n",
    "\n",
    "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        # create the list that will contain the generated tokens, along with the initial prompt tokens\n",
    "        pad_id = self.tokenizer.pad_id()\n",
    "        tokens = torch.full(\n",
    "            (batch_size, total_len), pad_id, dtype=torch.long, device=device\n",
    "        )\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            # populate the initial tokens with the prompt tokens\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "\n",
    "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "        # True if the token is a prompt token, False otherwise\n",
    "        prompt_tokens_mask = tokens != pad_id\n",
    "\n",
    "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens...\")\n",
    "        for cur_pos in cur_iterator:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model.forward(tokens[:, cur_pos - 1 : cur_pos], cur_pos)\n",
    "            if temperature > 0:\n",
    "                # The temperature is applied before the softmax\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = self._sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                # greedily select the token with the max probability\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if it is a padding token\n",
    "            next_token = torch.where(\n",
    "                prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "\n",
    "            # EOS is reached only if we found an EOS token for a padding position\n",
    "            eos_reached != (\n",
    "                ~prompt_tokens_mask[:, cur_pos] & (next_token == self.tokenizer.eos_id)\n",
    "            )\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        out_tokens = []\n",
    "        out_text = []\n",
    "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
    "            # cut to the EOS token, if present\n",
    "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
    "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
    "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "            out_tokens.append(current_prompt_tokens)\n",
    "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
    "\n",
    "        return (out_tokens, out_text)\n",
    "\n",
    "    def _sample_top_p(self, probs: torch.Tensor, p: float):\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1)  # (B, vocab_size)\n",
    "        # (substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "        mask = probs_sum - probs_sort > p  # (B, vocab_size)\n",
    "        # zero out all the probabilities of tokens that are not selected by the top_p\n",
    "        probs_sort[mask] = 0.0\n",
    "        # redistribute the probabilities so that they sum up to 1.\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        # sample a token (its index) from the top p distribution\n",
    "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "        # get the token position in the vocabulary corresponding to the sampled index\n",
    "        next_token = torch.gather(probs_idx, -1, next_token)\n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint: models\\llama-2-7b\\consolidated.00.pth\n",
      "Loaded checkpoint in 5.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mukes\\anaconda3\\envs\\torchdl\\lib\\site-packages\\torch\\__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:433.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state dict in 9.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112/112 [00:08<00:00, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that 1) the speed of light is constant in all inertial reference frames, and 2) the laws of physics are the same for all inertial reference frames.–ã The theory of relativity has two versions: the special theory of relativity and the general theory of relativity. The special theory of relativity applies to all physical phenomena in inertial reference frames. The general theory of relativity applies to the phenomena in non-inertial reference frames. The theory\n",
      "--------------------------------------------------\n",
      "If Google was an Italian company founded in Milan, it would be the largest company in the world, with a market capitalization of $ 713 billion. The company was founded in 1998 by Larry Page and Sergey Brin, two PhD students at Stanford University.\n",
      "The company's name is a reference to the number of letters in the word \"googol\", which is a mathematical term for the number 1 followed by 100 zeros.\n",
      "The company has its headquarters in Mountain View, California\n",
      "--------------------------------------------------\n",
      "Translate English to French:\n",
      "    sea otter => loutre de mer\n",
      "    peppermint => menthe poivr√©e\n",
      "    plush girafe => girafe peluche\n",
      "    cheese => fromage\n",
      "    \n",
      "    \"\"\"\n",
      "    def translate(self, text):\n",
      "        if text is None:\n",
      "            return None\n",
      "        text = text.strip()\n",
      "        if len(text) == 0:\n",
      "            return None\n",
      "        words = text.split()\n",
      "        for i in\n",
      "--------------------------------------------------\n",
      "Tell me if the following person is actually a Jedi knight disguised as human:\n",
      "    Name: Mukesh Mithrakumar\n",
      "    Decision: \n",
      "    1. I have a good feeling about this.\n",
      "    2. I have a bad feeling about this.\n",
      "\n",
      "    [1]\n",
      "    [2]\n",
      "\n",
      "    [1]\n",
      "    [2]\n",
      "\n",
      "    [1]\n",
      "    [2]\n",
      "\n",
      "    [1]\n",
      "    [2]\n",
      "\n",
      "    [1]\n",
      "   \n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: If you want to rerun this with limited GPU: I am using 16175MiB, RTX 3080 Ti Laptop GPU\n",
    "# You will need to restart the kernal cause jupyter notebook doesn't release the GPU memory\n",
    "# I tried: `torch.cuda.empty_cache()` to release all the GPU memory that can be freed\n",
    "# and `%reset -f` to clear jupyter notebook variables, but neither worked\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_version = \"llama-2-7b\"\n",
    "allow_cuda = True\n",
    "device = \"cuda\" if torch.cuda.is_available() and allow_cuda else \"cpu\"\n",
    "\n",
    "prompts = [\n",
    "    \"Simply put, the theory of relativity states that \",\n",
    "    \"If Google was an Italian company founded in Milan, it would\",\n",
    "    # Few shot promt\n",
    "    \"\"\"Translate English to French:\n",
    "    sea otter => loutre de mer\n",
    "    peppermint => menthe poivr√©e\n",
    "    plush girafe => girafe peluche\n",
    "    cheese =>\"\"\",\n",
    "    # Zero shot prompt\n",
    "    \"\"\"Tell me if the following person is actually a Jedi knight disguised as human:\n",
    "    Name: Mukesh Mithrakumar\n",
    "    Decision: \n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "model = LLaMA.build(\n",
    "    checkpoints_dir=f\"models/{model_version}\",\n",
    "    tokenizer_path=f\"models/{model_version}/tokenizer.model\",\n",
    "    load_model=True,\n",
    "    max_seq_len=1024,\n",
    "    max_batch_size=len(prompts),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "out_tokens, out_texts = model.text_completion(prompts, device, max_gen_len=64)\n",
    "assert len(out_texts) == len(prompts)\n",
    "for i in range(len(out_texts)):\n",
    "    print(f\"{out_texts[i]}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
