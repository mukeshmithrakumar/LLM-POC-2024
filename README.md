<div align="center" id="top"> 
	<img src="llm_poc_2024_banner.jpeg" alt="llm_poc_2024_banner"  width="50%" height="25%"/>
</div>

<h1 align="center">Large Language Models POC 2024</h1>

<p align="center">
	<img src="https://img.shields.io/github/last-commit/mukeshmithrakumar/LLM-POC-2024"/>
	<img alt="Python 3.10.14" src="https://img.shields.io/badge/Python-3.10.14-black.svg?logo=python&labelColor=blue&logoColor=white">
	<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-2.3.0+cu121-black?logo=pytorch&labelColor=EE4C2C&logoColor=white">
	<img alt="PyTorch Lightning" src="https://img.shields.io/badge/PyTorch_Lightning-2.2.3-black?labelColor=792ee5">
	<img alt="Huggingface Transformers" src="https://img.shields.io/badge/Transformers-4.40.1-black?labelColor=eeba30">
	<img alt="Github Issues" src="https://img.shields.io/github/issues/mukeshmithrakumar/LLM-POC-2024" />
	<img alt="Github Stars" src="https://img.shields.io/github/stars/mukeshmithrakumar/LLM-POC-2024?style=flatlogo" />
</p> 


<!-- Status -->
<h4 align="center"> 
	üöß üöÄ WIP...  üöß
</h4> 

<hr>

<p align="center">
	<a href="#dart-about">About</a> &#xa0; | &#xa0; 
	<a href="#sparkles-models">Models</a> &#xa0; | &#xa0;
	<a href="#fire-helpful-notebooks">Helpful Notebooks</a> &#xa0; | &#xa0;
	<a href="#test_tube-requirements">Requirements</a> &#xa0; | &#xa0;
	<a href="#memo-license">License</a> &#xa0; | &#xa0;
	<a href="https://github.com/mukeshmithrakumar" target="_blank">Author</a>
</p>

<br>

## :dart: About

My goal is to work my way through certain LLM models starting from Transformers to help me understand how each model works and builds from the previous models. Once I have a set of models I am interested in, the next focus will be on fine-tuning and optimizing the models to run on the cheapest hardware possible.

## :sparkles: Models

<p>
  <span>‚úÖ Transformer</span><br>
  <span>‚úÖ GPT</span><br>
  <span>‚úÖ LLaMA</span><br>
  <span>‚óªÔ∏è LLM Inference Optimization</span><br>
  &emsp;<span>‚óªÔ∏è In-flight Batching</span><br>
  &emsp;<span>‚óªÔ∏è Speculative inference</span><br>
  &emsp;<span>‚óªÔ∏è Key-Value Caching</span><br>
  &emsp;<span>‚óªÔ∏è PagedAttention</span><br>
  &emsp;<span>‚óªÔ∏è Pipeline Parallelism</span><br>
  &emsp;<span>‚óªÔ∏è Tensor Parallelism</span><br>
  &emsp;<span>‚óªÔ∏è Sequence Parallelism</span><br>
  &emsp;<span>‚óªÔ∏è Flash Attention</span><br>
  &emsp;<span>‚óªÔ∏è Quantization</span><br>
  &emsp;<span>‚óªÔ∏è Sparsity</span><br>
  &emsp;<span>‚óªÔ∏è Distillation</span><br>
  &emsp;<span>‚óªÔ∏è </span><br>
  &emsp;<span>‚óªÔ∏è </span><br>
</p>


## :fire: Helpful Notebooks

<p>
  <span>‚úÖ <a href="./transformer/transformer-arithmetic.ipynb">Transformer Arithmetic</a></span><br>
  <span>‚úÖ <a href="./transformer/transformer-scaling.ipynb">[WIP] Transformer Scaling</a></span><br>
  <span>‚óªÔ∏è </span><br>
</p>


## :test_tube: Requirements

Requirements for all the models are stored under a single `requirements.txt` file.


## :memo: License

This project is under license from MIT. For more details, see the [LICENSE](LICENSE) file.


---

Made with :heart: by <a href="https://github.com/mukeshmithrakumar" target="_blank">Mukesh Mithrakumar</a>

&#xa0;

<a href="#top">Back to top</a>
