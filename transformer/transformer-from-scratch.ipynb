{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model From Scratch\n",
    "\n",
    "**References**\n",
    "- *Coding a Transformer from scratch on PyTorch, with full explanation, training and inference: [Youtube Video](https://youtu.be/ISNdQcPhsts?si=M80AnG5chc6sKzk5), [Code](https://github.com/hkproj/pytorch-transformer)*\n",
    "- *Attention in transformers, visually explained | Chapter 6, Deep Learning: [Youtube Video](https://youtu.be/eMlx5fFNoYc?si=_GGTHKDsoqi9KuVE)*\n",
    "- *Illustrated Guide to Transformers Neural Network: A step by step explanation: [Youtube Video](https://youtu.be/4Bdc55j80l8?si=SbXl6rr0Sh4_3IuS)*\n",
    "- *An Introduction to Transformers: [Paper](https://arxiv.org/abs/2304.10557)*\n",
    "- *The Transformer Family Version 2.0: [Blog](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)*\n",
    "- *A survey of transformers: [Paper](https://www.sciencedirect.com/science/article/pii/S2666651022000146)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# General\n",
    "import math\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# Huggingface\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# lightning ai\n",
    "import torchmetrics\n",
    "\n",
    "# My python files\n",
    "from config import Config\n",
    "from helpers import latest_weights_file_path, get_weights_file_path, load_pkl_files, save_pkl_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embeddings\n",
    "\n",
    "- Input Matrix (sequence, d_model) - each word is made up of say 512 numbers then the shape of input matrix will be (x, 512) where x is the number of words.\n",
    "\n",
    "![Input Embeddings](images/InputEmbeddings.png)\n",
    "\n",
    "- Embedding isn't fixed, it is learned by the model.\n",
    "- \n",
    "\n",
    "*Why are we multiplying the embedding output by the square root of the dimension?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        \"\"\"Input Embedding Module.\n",
    "        Initializes the Embedder module. This module is used to embed the input tokens into a vector space.\n",
    "        \n",
    "        Args:\n",
    "            d_model (int): Dimension of Input Embedding\n",
    "            vocab_size (int): Vocabulary Size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds the input tensor into a vector space. \n",
    "        This is done by looking up the embedding for each token in the input tensor.\n",
    "        - Scales output by sqrt(dimensions) as per Vaswani et al. (2017).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor in R^d (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "- Because self-attention operation is permutation invariant, it is important to use proper positional encoding to provide order information to the model.\n",
    "- Positional Encoding conveys information about the position of the word in a sentence.\n",
    "- We want the model to treat words that appear close to each other as \"close\" and words that are distant as \"distant\"\n",
    "\n",
    "![Positional Embedding](images/PositionalEmbedding.png)\n",
    "\n",
    "- Unlike embeddings, positional embeddings aren't learned. We just compute this once. *Is this why we need a large dataset? Because we aren't learning positional encoding to tell the model appropriately what word can be closer to the other?*\n",
    "\n",
    "**Sinusoidal Positional Encoding**\n",
    "\n",
    "Sinusoidal positional encoding is defined as follows, given the token position $i = 1, ..., L$ and the dimension $\\delta = 1, ..., d$:\n",
    "\n",
    "$$\n",
    "PE (i, \\delta) = \\begin{cases}\n",
    "   sin \\frac{i}{10000^{d \\delta ' / d}} \\text{if} \\ \\delta = 2 \\delta ' \\\\\n",
    "   cos \\frac{i}{10000^{d \\delta ' / d}} \\text{if} \\ \\delta = 2 \\delta ' + 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this way each dimension of the positional encoding corresponds to a sinusoid of different wavelengths in different dimensions, from $2 \\pi$ to $10000 \\cdot 2 \\pi$\n",
    "\n",
    "![Positional Embedding Vector](images/PositionalEmbeddingVector.png)\n",
    "\n",
    "- *Why is there an odd and even position equation?*\n",
    "- *Why do we want the positional encoding to represent a pattern that can be learned by the model? Is that why we use sine and cosine?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: Config.dropout_constant) -> None:\n",
    "        \"\"\"Initializes the PositionalEncoder module. \n",
    "        This module is used to encode the position of the input tokens into the input embeddings.\n",
    "        Create a matrix of shape (max_sequence_length, dimensions) to store the positional encodings.\n",
    "        For each position in the sequence, compute the positional encoding for each dimension.\n",
    "        Store the positional encodings in the matrix.\n",
    "        Implement the positional encodings as per Vaswani et al. (2017).\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The number of dimensions to embed the input tokens into.\n",
    "            seq_len (int): The maximum length of the sentence since we need to create one vector for each position.\n",
    "            dropout (float): The dropout rate to apply to the PEs.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create a matrix of shape(seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len, 1)\n",
    "        # torch.arange - returns a 1D tensor of size (end-start/step) with values from the interval [start, end) \n",
    "        # taken with common difference step beginning from start.\n",
    "        # ? Why are we unsqueezing here?\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n",
    "        # ? Formula - I am not sure of the arange part here and how it translates to the equation above\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # (d_model / 2)\n",
    "        # ? It seems that you are doing this log this for numerical stability, need to look into why?\n",
    "        # Apply the sin to even position & cosine to odd positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # sin(position * (10000 ** (2i / d_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # cos(position * (10000 ** (2i / d_model))\n",
    "        # Adding a new dimension to account for the batch of sentences, we use unsqueeze to do this.\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "        # Register the positional encoding as a buffer\n",
    "        # Why: to keep inside the module, not as a learned param, but to be saved along with the state of the model\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Adds the positional encodings to the input tensor. Applies dropout to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to add positional encodings to.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # Since we are not learning this tensor, we add requires grad as False\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)  # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "Attention is a mechanism in neural network that a model can learn to make predictions by selectively attending to a given set of data. The amount of attention is quantified by learned weights and thus the output is usually formed as a weighted average.\n",
    "\n",
    "The output of the first stage of the transformer block is another $D \\times N$ array, $Y^{(m)}$. The output is produced by aggregating information across the sequence independently for each feature using an operation called attention.\n",
    "\n",
    "Specifically, the output vector at location n, denoted $y_n^{(m)}$, is produced by a simple weighted average of the input features at location $n' = 1 ... N$, denoted $x_{n'}^{(m-1)}$, that is\n",
    "\n",
    "$$y_n^{(m)} = \\sum_{n'=1}^N x_{n'}^{(m-1)} A_{n', n}^{(m)}$$\n",
    "\n",
    "**Relationship to Convolutional Neural Networks**\n",
    "- The attention mechanism can recover convolutional filtering as a special case e.g if $x_n^{(0)}$ is a 1D regularly sampled time-series and $A_{n', n}^{(m)} = A_{n' - n}^{(m)}$ then the attention mechanism above becomes a convolution. Unlike normal CNNs, these filters have full temporal support. Later we will see that the filters themselves dynamically depend on the input, another difference from standard CNNs. We will also see a similarity: transformers will use multiple attention maps in each layer in the same way that CNNs use multiple filters (though typically transformers have fewer attention maps than CNNs have channels).\n",
    "\n",
    "Here the weighting is given by a so-called attention matrix $A_{n', n}^{(m)}$ which is of size $N \\times N$ and normalises over its columns $\\sum_{n'=1}^{N} A_{n', n}^{(m)} = 1$. Intuitively speaking $A_{n', n}^{(m)}$ will take a high value for locations in the sequence n' which are of high relevance for location n.  For irrelevant locations, it will take the value 0. For example, all patches of a visual scene coming from a single object might have high corresponding attention values. We can compactly write the relationship as a matrix multiplication,\n",
    "\n",
    "$$Y^{(m)} = X^{(m-1)} A^{(m)}$$\n",
    "\n",
    "and we illustrate it below:\n",
    "\n",
    "![Element of the Attention Mechanism](images/ElementoftheAttentionMechanism.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "\n",
    "**Where does the attention matrix come from?**\n",
    "\n",
    "A simple way of generating the attention matrix from the input would be to measure the similarity between two locations by the dot product between the features at those two locations and then use a softmax function to handle the normalisation. However, this naïve approach entangles information about the similarity between locations in the sequence with the content of the sequence itself. An alternative is to perform the same operation on a linear transformation of the sequence, $Ux_n$, so that\n",
    "\n",
    "$$A_{n,n'} = \\frac{exp(x_n^{\\top} U^{\\top} Ux_{n'})}{ \\sum_{n''=1}^{N} exp(x_{n''}^{\\top} U^{\\top} Ux_{n'})}$$\n",
    "\n",
    "Typically, U will project to a lower dimensional space i.e. U is $K \\times D$ dimensional with $K < D$. In this way only some of the features in the input sequence need be used to compute the similarity, the others being projected out, thereby decoupling the attention computation from the content. However, the numerator in this construction is symmetric. This could be a disadvantage. For example, we might want the word ‘caulking iron’ to be strongly associated with the word ‘tool’ (as it is a type of tool), but have the word ‘tool’ more weakly associated with the word ‘caulking iron’ (because most of us rarely encounter it).\n",
    "\n",
    "Fortunately, it is simple to generalise the attention mechanism above to be asymmetric by applying two different linear transformations to the original sequence\n",
    "\n",
    "$$A_{n,n'} = \\frac{exp(x_n^{\\top} U_k^{\\top} U_q x_{n'})}{ \\sum_{n''=1}^{N} exp(x_{n''}^{\\top} U_k^{\\top} U_q x_{n'})}$$\n",
    "\n",
    "The two quantities that are dot-producted together here $q_n = U_q x_n$ and $k_n = U_k x_n$ are typically known as the *queries* and the *keys*, respectively.\n",
    "\n",
    "**Relationship to Recurrent Neural Networks**\n",
    "- It is illuminating to compare the temporal processing in the transformer to that of RNNs which recursively update a hidden state feature representation $(x_n^{(1)})$ based on the current observation $(x_n^{(0)})$ and the previous hidden state $(x_n^{(1)}) = f(x_{n-1}^{(1)} ; x_{n}^{(0)}) = f(f(x_{n-2}^{(1)} ; x_{n-1}^{(0)}); x_n^{(0)})$. Here we’ve unrolled the RNN one step to show that observations which are nearby to the hidden state (e.g. $x_n^{(0)}$) are treated differently from observations that are further away (e.g. $x_{n-1}^{(0)}$), as information is propagated by recurrent application of the function $f(\\cdot)$. In contrast, in the transformer, self-attention treats all observations at all time-points in an identical manner, no matter how far away they are. This is one reason why they find it simpler to learn long-range relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compute Self Attention\n",
    "\n",
    "- Self-Attention is permutation invariant.\n",
    "\n",
    "![Self Attention Part 1](images/SelfAttentionPart1.png)\n",
    "\n",
    "![Self Attention Part 2](images/SelfAttentionPart2.png)\n",
    "\n",
    "- Self-Attention requires no parameters. Up to now the interaction between words has been driven by their embedding and the positional encodings. This will change later.\n",
    "- We expect values along the diagonal to be the highest.\n",
    "- If we don’t want some positions to interact, we can always set their values to –∞ before applying the softmax in this matrix and the model will not learn those interactions. We will use this in the decoder.\n",
    "\n",
    "**What is query, keys and values?**\n",
    "- Query (Q): This represents the current word or part of the sequence the model is focusing on.\n",
    "- Key (K): This represents all the words or parts in the input sequence.\n",
    "- Value (V): This also represents all the words or parts in the input sequence, but it contains the actual information associated with each key.\n",
    "\n",
    "**Where are Q and K coming from?**\n",
    "- The transformer encoder training builds the weight parameter matrices W_Q and W_k. The calculation goes like below where x is a sequence of position-encoded word embedding vectors that represents an input sentence.\n",
    "    1. Picks up <span style=\"color:yellow\">a word vector (position encoded)</span> from the input sentence sequence, and transfer it to a vector space Q. This becomes the query. $Q = X \\cdot W_Q^T$\n",
    "    2. Pick <span style=\"color:yellow\">all the words in the sentence</span> and transfer them to the vector space K. They become keys and each of them is used as key. $K = X \\cdot W_K^T$\n",
    "    3. For each (q, k) pair, their relation strength is calculated using dot product. $\\text{qk similarity scores} = matmul(Q, K^T)$\n",
    "    4. Weight matrices $W_Q$ and $W_K$ are trained via the back propagations during the Transformer training.\n",
    "\n",
    "**Steps Involved:**\n",
    "- Encoding Inputs: Each word in the input sequence is converted into a vector representation.\n",
    "- Calculating Compatibility Scores: The model calculates a score for each key-query pair. This score represents how relevant a particular word (key) is to the current word being processed (query). There are different ways to calculate this score, but they typically involve measuring the dot product or cosine similarity between the query and key vectors.\n",
    "- Softmax Attention: A softmax function is applied to the compatibility scores, converting them into attention weights. These weights represent the importance of each word in the sequence relative to the current word.\n",
    "- Weighted Sum: The attention weights are multiplied element-wise with the value vectors. This essentially amplifies the information from the relevant parts of the sequence (based on the weights) and weakens the contribution of less relevant parts.\n",
    "- Attention Output: The weighted sum of the value vectors is the attention output. This output incorporates information from the entire sequence, but with a focus on the parts most relevant to the current word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "In the self-attention mechanisms there is one attention matrix which describes the similarity of two locations within the sequence. This can act as a bottleneck in the architecture – it would be useful for pairs of points to be similar in some ‘dimensions’ and different in others. If attention matrices are viewed as a datadriven version of filters in a CNN, then the need for more filters/channels is clear. Typical choices for the number of heads H is 8 or 16, lower than typical numbers of channels in a CNN.\n",
    "\n",
    "In order to increase capacity of the first self-attention stage, the transformer block applies H sets of self-attention in parallel and then linearly projects the results down to the $D \\times N$ array required for further processing. This slight generalisation is called multi-head self-attention. \n",
    "\n",
    "The computational cost of multi-head self-attention is usually dominated by the matrix multiplication involving the attention matrix and is therefore $O(HDN^2)$.\n",
    "\n",
    "![Multi-Head Attention Equation](images/MultiHeadAttentionEquation.png)\n",
    "\n",
    "Here $H$ matrices $V_h^{(m)}$ which are $D \\times D$ project the $H$ self-attention stages down to the required output dimensionality $D$. \n",
    "\n",
    "The product of the matrices $V_h^{(m)} X^{(m-1)}$ is related to the so-called values which are normally introduced in descriptions of self-attention along side queries and keys. In the usual presentation, there is a redundancy between the linear transform used to compute the values and the linear projection at the end of the multi-head selfattention, so we have not explicitly introduced them here. The standard presentation can be recovered by setting $V_h$ to be a low-rank matrix $V_h = U_h U_{v, h}$ where $U_h$ is $D \\times K$ and $U_{v, h}$ is $K \\times D$. Typically K is set to K = D/H so that changing the number of heads leads to models with similar numbers of parameters and computational demands.\n",
    "\n",
    "The addition of the matrices $V_h^{(m)}$, and the fact that retaining just the diagonal elements of the attention matrix $A^{(m)}$ will interact the signal instantaneously with itself, does mean there is some cross-feature processing in multi-head self-attention, as opposed to it containing purely cross-sequence processing. However, the stage has limited capacity for this type of processing and it is the job of the second stage to address this.\n",
    "\n",
    "![Multi-Head Attention](images/MultiHeadAttention.png)\n",
    "\n",
    "**Here's how it works:**\n",
    "- Multiple Attention Heads: Instead of having a single set of query (Q), key (K), and value (V) vectors, the model creates multiple sets (often 4, 8, or 16). These are called attention heads.\n",
    "- Linear Projections: Each input word embedding is projected independently for each attention head using different weight matrices. This creates different query, key, and value vectors for each head, allowing them to focus on different aspects of the relationships between words.\n",
    "- Independent Attention: Each attention head then performs the standard attention mechanism steps (compatibility score calculation, softmax, weighted sum) independently. This results in multiple attention outputs, each capturing a different aspect of the context.\n",
    "- Concatenation: Finally, the outputs from all the attention heads are concatenated to form a single final output. This combined output incorporates information from all the heads, providing a richer representation of the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        \"\"\"This module is used to calculate the attention scores for the input tensor.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Embedding vector size.\n",
    "            h (int): Number of heads.\n",
    "            dropout (float): The dropout rate to apply to the attention scores.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        # for integer divions: // for floating point division: /\n",
    "        self.d_k = d_model // h  # Dimension of vector seen by each head\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # staticmethod: because we can call this without an instance of the above class\n",
    "    @staticmethod\n",
    "    def attention(\n",
    "        query: torch.Tensor, \n",
    "        key: torch.Tensor, \n",
    "        value: torch.Tensor, \n",
    "        mask: torch.Tensor, \n",
    "        dropout: nn.Dropout\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Calculates the attention scores for the input tensor.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): The query tensor to calculate the attention scores for.\n",
    "            key (torch.Tensor): The key tensor to calculate the attention scores for.\n",
    "            value (torch.Tensor): The value tensor to calculate the attention scores for.\n",
    "            mask (torch.Tensor): The mask tensor to apply to the attention scores.\n",
    "            dropout (nn.Dropout): The dropout rate to apply to the attention scores\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        # @ matrix multiplication in pytorch\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # masking is just multiplying by a very small value which then becomes zero after applying softmax\n",
    "        if mask is not None:\n",
    "            # replace all the values for mask==0 with -1e9\n",
    "            attention_scores.masked_fill(mask==0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch, h, seq_len, seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        \n",
    "        # The second value in the tuple is used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"Projects the input tensor into the query, key, and value tensors.\n",
    "        Splits the query, key, and value tensors into multiple heads.\n",
    "        Calculates the attention scores for the input tensor.\n",
    "        Concatenates the attention scores for the multiple heads.\n",
    "        Projects the attention scores back into the original dimension.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): The query tensor to calculate the attention scores for.\n",
    "            k (torch.Tensor): The key tensor to calculate the attention scores for.\n",
    "            v (torch.Tensor): The value tensor to calculate the attention scores for.\n",
    "            mask (torch.Tensor, optional): The mask tensor to apply to the attention scores. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # for q, k, v: (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "    \n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        # Using view: keep batch dim since we don't want to split the sentence, we want to split the embedding\n",
    "        # torch.tensor.view: returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "        # Why transpose: prefer h as the second dim, this way each head will see all the sentences\n",
    "        # view() is used cause it's typically faster than reshape(), cause it doesn't need to copy the underlying data\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Getting attention scores\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "**Batch Normalization (BN):**\n",
    "- Normalization Scope: BN normalizes activations across a mini-batch of training data. It calculates the mean and standard deviation of the activations for each feature (channel) within the mini-batch and then uses these statistics to normalize the individual activations.\n",
    "- Impact: This helps to address the problem of internal covariate shift, where the distribution of activations can change significantly between mini-batches during training. By normalizing each mini-batch, BN ensures that the gradients used for updating the model's weights are more stable and less prone to exploding or vanishing gradients.\n",
    "\n",
    "*Limitations*:\n",
    "- Reliance on Batch Size: BN's effectiveness depends on the mini-batch size. Smaller batch sizes can lead to less accurate estimates of the mean and standard deviation, potentially reducing its effectiveness.\n",
    "- Increased Memory Consumption: BN requires storing the mean and standard deviation for each feature across the mini-batch, which can increase memory usage.\n",
    "\n",
    "![Layer Normalization](images/LayerNormalization.png)\n",
    "\n",
    "**Layer Normalization (LN):**\n",
    "- Normalization Scope: LN normalizes activations within each layer, independently for each sample in the mini-batch. It calculates the mean and standard deviation of the activations for each feature (channel) across all elements in a single sample (e.g., across the entire image width and height for a convolutional layer).\n",
    "- Impact: LN also addresses internal covariate shift, but at a different level. It ensures that the activations within a layer have a consistent distribution for each sample, regardless of the mini-batch or other samples in the training data. This can improve the stability of gradients and learning.\n",
    "\n",
    "*Benefits*:\n",
    "- Less Sensitive to Batch Size: LN is less sensitive to the mini-batch size compared to BN. This makes it potentially more robust for various training scenarios, including settings with small batch sizes.\n",
    "- Lower Memory Footprint: LN doesn't require storing statistics across the entire mini-batch, reducing memory consumption.\n",
    "\n",
    "*Potential Drawbacks*:\n",
    "- Limited Context: LN might not capture long-range dependencies between features as effectively as BN, which considers activations across the entire mini-batch.\n",
    "- Less Flexibility: LN applies the same normalization across all features within a layer, while BN allows for different normalizations for each feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 10**-6) -> None:\n",
    "        \"\"\"Initializes the LayerNormalization module. This module is used to normalize the input tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            eps (float, optional): Needed to prevent div by 0 error. Defaults to 10**-6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))  # Multiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(features))  # Added\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Normalizes the input tensor using the gamma and beta parameters.\n",
    "        - Assumes input has batch, the mean and standard deviation is calculated over the last dimension \n",
    "        leaving the batch dimension.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to normalize.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "        # Keep the dimension for broadcasting\n",
    "        self.mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        self.std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        return self.alpha * (x - self.mean) / (self.std + self.eps) + self.bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Network\n",
    "\n",
    "The transformer also includes this point-wise feed-forward network in both the encoder and decoder:\n",
    "\n",
    "![Feed Forward Network](images/FeedForwardNetwork.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        \"\"\"This module is used to project the input tensor into a higher dimension \n",
    "        and then back into the original dimension.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The number of dimensions of the input tensor.\n",
    "            d_ff (int): The number of dimensions to project the input tensor into.\n",
    "            dropout (float): The dropout rate to apply to the output tensor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)  # W1 and B1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)  # W2 and B2\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Projects the input tensor into a higher dimension using a linear layer and a ReLU activation function.\n",
    "        Projects the input tensor back into the original dimension using a linear layer.\n",
    "        Applies dropout to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to project and then project back.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection\n",
    "\n",
    "The use of residual connections is widespread across machine learning as they make initialisation simple, have a sensible inductive bias towards simple functions, and stabilise learning. Instead of directly specifying a function $x^{(m)} = f_{\\theta} (x^{(m-1)})$, the idea is to parameterise it in terms of an identity mapping and a residual term\n",
    "$$x^{(m)} = x^{(m-1)} + \\text{res}_{\\theta} (x^{(m-1)})$$\n",
    "\n",
    "Equivalently, this can be viewed as modelling the differences between the representation $x^{(m)} - x^{(m-1)} = \\text{res}_{\\theta} (x^{(m-1)})$ and will work well when the function that is being modelled is close to identity. This type of parameterisation is used for both the Multi-Head Self-Attention and MLP stages in the transformer, with the idea that each applies a mild non-linear transformation to the representation. Over many layers, these mild non-linear transformations compose to form large transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout: float = Config.dropout_constant) -> None:\n",
    "        \"\"\"This module is used to add the input tensor to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            dropout (float, optional): The dropout rate to apply to the output tensor. Defaults to Config.dropout_constant.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, sublayer: torch.nn.Module) -> torch.Tensor:\n",
    "        \"\"\"Adds the input tensor to the output tensor. Applies dropout to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to add to the output tensor.\n",
    "            sublayer (torch.nn.Module): The network layer to apply to the input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "\n",
    "![Encoder Block](images/EncoderBlock.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            features: int,\n",
    "            self_attention_block: MultiHeadAttentionBlock,\n",
    "            feed_forward_block: FeedForwardBlock,\n",
    "            dropout: float = Config.dropout_constant\n",
    "        ) -> None:\n",
    "        \"\"\"Initializes the EncoderBlock module. This module is used to encode the input tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            self_attention_block (MultiHeadAttentionBlock): The self attention layer to apply to the input tensor.\n",
    "            feed_forward_block (FeedForwardBlock): The feed forward layer to apply to the input tensor.\n",
    "            dropout (float, optional): The dropout rate to apply to the output tensor. Defaults to Config.dropout_constant.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Applies the self attention layer to the input tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "        Applies the feed forward layer to the output tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to encode.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "![Encoder](images/Encoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features:int, layers: nn.ModuleList) -> None:\n",
    "        \"\"\"Initializes the Encoder module. This module is used to encode the input tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            layers (nn.ModuleList): The layers to apply to the input tensor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Applies the layers to the input tensor. Applies the layer normalization to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to encode.\n",
    "            mask (torch.Tensor): The mask tensor to apply to the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "\n",
    "![Decoder Block](images/DecoderBlock.png)\n",
    "\n",
    "**Masked Multi-Head Attention**\n",
    "- Our goal is to make the model causal: it means the output at a certain position can only depend on the words on the previous positions. The model must not be able to see future words.\n",
    "\n",
    "![Masked Multi-Head Attention](images/MaskedMultiHeadAttention.png)\n",
    "\n",
    "- All the values above the diagonal are replace with -∞ before applying the softmax, which will replace them with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            features: int, \n",
    "            self_attention_block: MultiHeadAttentionBlock,\n",
    "            cross_attention_block: MultiHeadAttentionBlock,\n",
    "            feed_forward_block: FeedForwardBlock,\n",
    "            dropout: float = Config.dropout_constant\n",
    "        ) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            self_attention_block (MultiHeadAttentionBlock): The self attention layer to apply to the input tensor.\n",
    "            cross_attention_block (MultiHeadAttentionBlock): The source attention layer to apply to the input tensor.\n",
    "            feed_forward_block (FeedForwardBlock): The feed forward layer to apply to the input tensor.\n",
    "            dropout (float, optional): The dropout rate to apply to the output tensor. Defaults to Config.dropout_constant.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_bock = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor, \n",
    "        encoder_output: torch.Tensor, \n",
    "        src_mask: torch.Tensor, \n",
    "        tgt_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Applies the self attention layer to the input tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "        Applies the source attention layer to the output tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "        Applies the feed forward layer to the output tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to decode.\n",
    "            encoder_output (torch.Tensor): The output tensor from the encoder.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores for the source tensor.\n",
    "            tgt_mask (torch.Tensor): The mask tensor to apply to the attention scores for the target tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connection[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connection[2](x, self.feed_forward_bock)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "![Decoder](images/Decoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features:int, layers: nn.ModuleList) -> None:\n",
    "        \"\"\"Initializes the Decoder module. This module is used to decode the input tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            layers (nn.ModuleList): The layers to apply to the input tensor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        encoder_output: torch.Tensor, \n",
    "        src_mask: torch.Tensor, \n",
    "        tgt_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Applies the layers to the input tensor. Applies the layer normalization to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to decode.\n",
    "            encoder_output (torch.Tensor): The output tensor from the encoder.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores for the source tensor.\n",
    "            tgt_mask (torch.Tensor): The mask tensor to apply to the attention scores for the target tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        \"\"\"Initializes the ProjectionLayer module. \n",
    "        This module is used to project the input tensor into a higher dimension.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The number of dimensions of the input tensor.\n",
    "            vocab_size (int): The number of unique tokens in the input.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Projects the input tensor into a higher dimension using a linear layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to project.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, vocabulary_size).\n",
    "        \"\"\"\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "![Transformer Block](images/TransformerBlock.png)\n",
    "\n",
    "The representation of the input sequence will be produced by iteratively applying a transformer block \n",
    "$$X^{(m)} = \\text{transformer-block} (X^{(m-1)})$$\n",
    "\n",
    "The block itself comprises two stages: \n",
    "- one operating across the sequence and \n",
    "- one operating across the features.\n",
    "\n",
    "The first stage refines each feature independently according to relationships between tokens across the sequence e.g. how much a word in a sequence at position $n$ depends on previous words at position n′, or how much two different patches from an image are related to one another. This stage acts horizontally across rows of $X^{(m-1)}$.\n",
    "\n",
    "The second stage refines the features representing each token. This stage acts vertically across a column of $X^{(m-1)}$. By repeatedly applying the transformer block the representation at token n and feature d can be shaped by information at token n′ and feature d′.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            encoder: Encoder,\n",
    "            decoder: Decoder,\n",
    "            src_embed: InputEmbeddings,\n",
    "            tgt_embed: InputEmbeddings,\n",
    "            src_pos: PositionalEncoding,\n",
    "            tgt_pos: PositionalEncoding,\n",
    "            projection_layer: ProjectionLayer\n",
    "        ) -> None:\n",
    "        \"\"\"Initializes the Transformer module. This module is used to encode and decode the input tensor.\n",
    "\n",
    "        Args:\n",
    "            encoder (Encoder): The encoder to encode the input tensor.\n",
    "            decoder (Decoder): The decoder to decode the input tensor.\n",
    "            src_embed (InputEmbeddings): The input embedder to embed the input tensor.\n",
    "            tgt_embed (InputEmbeddings): The target embedder to embed the input tensor.\n",
    "            src_pos (PositionalEncoding): The positional encoder to encode the input tensor.\n",
    "            tgt_pos (PositionalEncoding): The positional encoder to encode the target tensor.\n",
    "            projection_layer (ProjectionLayer): _description_\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "    \n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds the input tensor. Encodes the input tensor.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): The input tensor (batch_size, sequence_length) to encode.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        encoder_output: torch.Tensor, \n",
    "        src_mask: torch.Tensor, \n",
    "        tgt: torch.Tensor, \n",
    "        tgt_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Decodes the input tensor.\n",
    "\n",
    "        Args:\n",
    "            encoder_output (torch.Tensor): The output tensor from the encoder.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores for the source tensor.\n",
    "            tgt (torch.Tensor): The target tensor.\n",
    "            tgt_mask (torch.Tensor): The mask tensor to apply to the attention scores for the target tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Projects the input tensor into a higher dimension.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to project.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, vocabulary_size).\n",
    "        \"\"\"\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(\n",
    "    src_vocab_size: int,\n",
    "    tgt_vocab_size: int,\n",
    "    src_seq_len: int,\n",
    "    tgt_seq_len: int,\n",
    "    d_model: int,\n",
    "    num_layers: int,\n",
    "    num_heads: int,\n",
    "    dropout: float,\n",
    "    d_ff: int\n",
    ") -> Transformer:\n",
    "    \"\"\"Builds the transformer model.\n",
    "\n",
    "    Args:\n",
    "        src_vocab_size (int): The number of unique tokens in the input.\n",
    "        tgt_vocab_size (int): The number of unique tokens in the target.\n",
    "        src_seq_len (int): The maximum length of the input sequence.\n",
    "        tgt_seq_len (int): The maximum length of the target sequence.\n",
    "        d_model (int): The number of dimensions to embed the input tokens into.\n",
    "        num_layers (int): The number of layers to apply to the input tensor.\n",
    "        num_heads (int): The number of heads to split the input tensor into.\n",
    "        dropout (float): The dropout rate to apply to the output tensor.\n",
    "        d_ff (int): The number of dimensions to project the input tensor into.\n",
    "\n",
    "    Returns:\n",
    "        Transformer: Returns a transformer model.\n",
    "    \"\"\"\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positioonal encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(num_layers):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(num_layers):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(\n",
    "            d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout\n",
    "        )\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply a transformer, data must be converted into a set or sequence of N tokens $x_n$ of dimension D. The tokens can be collected into a matrix $X^{(0)}$ which is $ D \\times N$. To give two concrete examples\n",
    "1. a passage of text can be broken up into a sequence of words or sub-words, with each word being represented by a single unique vector,\n",
    "2. an image can be broken up into a set of patches and each patch can be mapped into a vector.\n",
    "\n",
    "The embeddings can be fixed or they can be learned with the rest of the parameters of the model e.g. the vectors representing words can be optimised or a learned linear transform can be used to embed image patches.\n",
    "\n",
    "![Encoding an Image](images/EncodinganImage.png)\n",
    "\n",
    "A sequence of tokens is a generic representation to use as an input – many different types of data can be “tokenised” and transformers are then immediately applicable rather than requiring a bespoke architectures for each modality as was previously the case (CNNs for images, RNNs for sequences, deepsets for sets etc.). Moreover, this means that you don’t need bespoke handcrafted architectures for mixing data of different modalities — you can just throw them all into a big set of tokens.\n",
    "\n",
    "The transformer will ingest the input data $X^{(0)}$ and return a representation of the sequence in terms of another matrix $X^{(M)}$ which is also of size $ D \\times N$. The slice $x_n = X_{:,n}^{(M)}$ will be a vector of features representing the sequence at the location of token $n$. These representations can be used for auto-regressive prediction of the next (n+1)th token,  global classification of the entire sequence (by pooling across the whole representation), sequence-to-sequence or imageto-image prediction problems, etc. Here $M$ denotes the number of layers in the transformer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds: Dataset, lang: str):\n",
    "    \"\"\"A generator to get all sentences in a dataset.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): The dataset.\n",
    "        lang (str): The language.\n",
    "\n",
    "    Yields:\n",
    "        _type_: A generator.\n",
    "    \"\"\"\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config: Config, ds: Dataset, lang: str) -> Tokenizer:\n",
    "    \"\"\"Gets the tokenizer if it exists, otherwise builds it and saves it.\n",
    "\n",
    "    Args:\n",
    "        config (Config): The configuration.\n",
    "        ds (Dataset): The dataset.\n",
    "        lang (str): The language.\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: The tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer_path = Path(config.tokenizer_file.format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(size: int) -> torch.Tensor:\n",
    "    \"\"\"Create a causal mask for the decoder attention.\"\"\"\n",
    "    # torch.triu: returns the upper triangular part of a matrix (2-D tensor) or batch of matrices\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ds: Dataset, \n",
    "            tokenizer_src: Tokenizer, \n",
    "            tokenizer_tgt: Tokenizer, \n",
    "            src_lang: str, \n",
    "            tgt_lang: str, \n",
    "            seq_len: int\n",
    "        ) -> None:\n",
    "        \"\"\"Constructor for the BilingualDataset class.\n",
    "\n",
    "        Args:\n",
    "            ds (Dataset): The dataset to use.\n",
    "            tokenizer_src (Tokenizer): The tokenizer for the source language.\n",
    "            tokenizer_tgt (Tokenizer): The tokenizer for the target language.\n",
    "            src_lang (str): The source language.\n",
    "            tgt_lang (str): The target language.\n",
    "            seq_len (int): The sequence length to use.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # Start of Sentence, End of Sentence and Padding Tokens\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the length of the dataset.\"\"\"\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def truncate_sequence(self, sequence, max_len: int):\n",
    "        \"\"\"Truncates a sequence of tokens to the specified maximum length.\n",
    "\n",
    "        Args:\n",
    "            sequence: A list of integer tokens representing the sentence.\n",
    "            max_len: The maximum allowed length for the sequence.\n",
    "\n",
    "        Returns:\n",
    "            A list of truncated tokens and the original length of the sequence.\n",
    "        \"\"\"\n",
    "        if len(sequence) <= max_len:\n",
    "            return sequence, len(sequence)\n",
    "        else:\n",
    "            # Truncate from the beginning (optional: modify to truncate from the end)\n",
    "            truncated_sequence = sequence[:max_len]\n",
    "            return truncated_sequence, len(sequence)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict:\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens & Truncate sentences if necessary\n",
    "        # Note that for sentences > seq_len, by adding -2 and -1, I am choosing to not have any padding, \n",
    "        # cause enc_num_padding_tokens and dec_num_padding_tokens will turn out to be = 0\n",
    "        enc_input_tokens, orig_enc_len = self.truncate_sequence(self.tokenizer_src.encode(src_text).ids, self.seq_len - 2)\n",
    "        dec_input_tokens, orig_dec_len = self.truncate_sequence(self.tokenizer_tgt.encode(tgt_text).ids, self.seq_len - 1)\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # -2 cause we will add <s> and </s>\n",
    "        # We will only add <s> (SOS), and </s> (EOS) only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long.\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\n",
    "                f\"Original sentence length exceeded allowed limit of seq_len - {self.seq_len}. \"\n",
    "                f\"Original lengths: source - {orig_enc_len}, target - {orig_dec_len}\"\n",
    "            )\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        \n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "        \n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len, f\"encoder_input size: {encoder_input.size(0)} and seq_len: {self.seq_len}\"\n",
    "        assert decoder_input.size(0) == self.seq_len, f\"decoder_input size: {decoder_input.size(0)} and seq_len: {self.seq_len}\"\n",
    "        assert label.size(0) == self.seq_len, f\"label size: {label.size(0)} and seq_len: {self.seq_len}\"\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,   # (seq_len)\n",
    "            \"decoder_input\": decoder_input,   # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # (1, 1, seq_len)\n",
    "            # (1, seq_len) & (1, seq_len, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config: Config) -> Tuple[DataLoader, DataLoader, Tokenizer, Tokenizer]:\n",
    "    \"\"\"Loads the dataset, builds the tokenizers and returns the dataloaders.\n",
    "\n",
    "    Args:\n",
    "        config (Config): The configuration.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataLoader, DataLoader, Tokenizer, Tokenizer]: The training and validation dataloaders.\n",
    "    \"\"\"\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(f\"{config.datasource}\", f\"{config.lang_src}-{config.lang_tgt}\", split=\"train\")\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config.lang_src)\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config.lang_tgt)\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    # Need to create the tensors the model will use\n",
    "    train_ds = BilingualDataset(\n",
    "        train_ds_raw, tokenizer_src, tokenizer_tgt, config.lang_src, config.lang_tgt, config.seq_len\n",
    "    )\n",
    "    val_ds = BilingualDataset(\n",
    "        val_ds_raw, tokenizer_src, tokenizer_tgt, config.lang_src, config.lang_tgt, config.seq_len\n",
    "    )\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config.lang_src]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config.lang_tgt]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "    \n",
    "    print(f\"Item: {item}\")\n",
    "    print(f\"Encoder: {tokenizer_src.encode(item['translation'][config.lang_src])}\")\n",
    "    print(f\"Source Encoder ids: {tokenizer_src.encode(item['translation'][config.lang_src]).ids}\")\n",
    "    print(f\"Source Encoder tokens: {tokenizer_src.encode(item['translation'][config.lang_src]).tokens}\")\n",
    "    print(f\"Max length of source sentence: {max_len_src}\")\n",
    "    print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "![Training Model](images/TrainingModel.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(\n",
    "        model: Transformer, \n",
    "        source: torch.Tensor, \n",
    "        source_mask: torch.Tensor, \n",
    "        tokenizer_src: Tokenizer, \n",
    "        tokenizer_tgt: Tokenizer, \n",
    "        max_len: int, \n",
    "        device: torch.device\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Greedy Decode:\n",
    "    - Finds the most likely next token at each step and appends it to the decoder input.\n",
    "    - Just picks the token with the max probabilitiesability at each step as the next token.\n",
    "    - May not give the ideal output, but it's fast and simple. For better results, use beam search.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The transformer model\n",
    "    - source: The input sentence\n",
    "    - source_mask: The mask for the input sentence\n",
    "    - source_tokenizer: The tokenizer for the source language\n",
    "    - target_tokenizer: The tokenizer for the target language\n",
    "    - max_length: The maximum length of the output sentence\n",
    "    - device: The device to run the model on\n",
    "    \n",
    "    Shapes:\n",
    "    - source: (1, sequence_length, dimensions)\n",
    "    - source_mask: (1, 1, sequence_length, sequence_length)\n",
    "    \n",
    "    Returns: \n",
    "    - The output sentence\n",
    "    \"\"\"\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "    \n",
    "    return decoder_input.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config: Config, vocab_src_len: int, vocab_tgt_len: int) -> Transformer:\n",
    "    \"\"\"Builds the transformer model\n",
    "\n",
    "    Args:\n",
    "        config (Config): The configuration.\n",
    "        vocab_src_len (int): The length of the source vocabulary.\n",
    "        vocab_tgt_len (int): The length of the target vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        Transformer: The transformer model.\n",
    "    \"\"\"\n",
    "    model = build_transformer(\n",
    "        vocab_src_len,\n",
    "        vocab_tgt_len,\n",
    "        config.seq_len,\n",
    "        config.seq_len,\n",
    "        config.d_model,\n",
    "        config.num_layers,\n",
    "        config.num_heads,\n",
    "        config.dropout_constant,\n",
    "        config.d_ff,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(\n",
    "    model: Transformer,\n",
    "    validation_ds: DataLoader,\n",
    "    tokenizer_src: Tokenizer,\n",
    "    tokenizer_tgt: Tokenizer,\n",
    "    max_len: int,\n",
    "    device: torch.device,\n",
    "    print_msg,\n",
    "    global_step: int,\n",
    "    writer,\n",
    "    num_examples: int = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Validation:\n",
    "    - Runs the model on the validation dataset and prints the source, target and predicted sentences.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The transformer model\n",
    "    - validation_ds: The validation dataset\n",
    "    - source_tokenizer: The tokenizer for the source language\n",
    "    - target_tokenizer: The tokenizer for the target language\n",
    "    - max_length: The maximum length of the output sentence\n",
    "    - device: The device to run the model on\n",
    "    - print_message: A function to print messages\n",
    "    - global_step: The global step\n",
    "    - num_examples: The number of examples to print\n",
    "    \n",
    "    Shapes:\n",
    "    - source: (1, sequence_length, dimensions)\n",
    "    - source_mask: (1, 1, sequence_length, sequence_length)\n",
    "    \n",
    "    Returns:\n",
    "    - Inference\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except Exception:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)  # (b, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)  # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(0)==1, \"Batch size must be 1 for validation.\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-' * console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        # Compute the char error rate\n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config: Config):\n",
    "    \"\"\"Trains the transformer model.\n",
    "\n",
    "    Args:\n",
    "        config (Config): The configuration.\n",
    "    \n",
    "    Steps:\n",
    "        - Get the dataset\n",
    "        - Get the model\n",
    "        - Define the optimizer\n",
    "        - Define the loss function\n",
    "        - For each epoch:\n",
    "            - For each batch:\n",
    "                - Run the tensors through the encoder, decoder and the projection layer\n",
    "                - Compare the output with the label\n",
    "                - Compute the loss using a simple cross entropy\n",
    "                - Backpropagate the loss\n",
    "                - Update the weights\n",
    "            - Run validation at the end of every epoch\n",
    "            - Save the model at the end of every epoch\n",
    "    \"\"\"\n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if (device == \"cuda\"):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == \"mps\"):\n",
    "        print(\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "    \n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config.datasource}_{config.model_folder}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # check pre-existing saved data and either load or continue to getting the dataset\n",
    "    datafolder_path = Path(config.data_folder)\n",
    "    filenames = [\"train_dataloader.pkl\", \"val_dataloader.pkl\", \"tokenizer_src.pkl\", \"tokenizer_tgt.pkl\"]\n",
    "    file_existences = [os.path.exists(f\"{datafolder_path}/{filename}\") for filename in filenames]\n",
    "    if all(file_existences):\n",
    "        train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = load_pkl_files(datafolder_path, filenames)\n",
    "        print(\"Pickle files loaded.\")\n",
    "    else:\n",
    "        train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "        save_pkl_files(datafolder_path, filenames, [train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt])\n",
    "    \n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config.experiment_name)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config.preload\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f\"Preloading model {model_filename}\")\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print(\"No model to preload, starting from scratch.\")\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config.num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch: 02d}\")\n",
    "\n",
    "        for batch in batch_iterator:\n",
    "            encoder_input = batch['encoder_input'].to(device)  # (B, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device)  # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)  # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)  # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)  # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)  # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output)  # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device)  # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\" : f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "        \n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(\n",
    "            model,\n",
    "            val_dataloader,\n",
    "            tokenizer_src,\n",
    "            tokenizer_tgt,\n",
    "            config.seq_len,\n",
    "            device,\n",
    "            lambda msg: batch_iterator.write(msg),\n",
    "            global_step,\n",
    "            writer\n",
    "        )\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"global_step\": global_step\n",
    "        }, model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "datafolder_path = Path(config.data_folder)\n",
    "filenames = [\"train_dataloader.pkl\", \"val_dataloader.pkl\", \"tokenizer_src.pkl\", \"tokenizer_tgt.pkl\"]\n",
    "file_existences = [os.path.exists(f\"{datafolder_path}/{filename}\") for filename in filenames]\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = load_pkl_files(datafolder_path, filenames)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = latest_weights_file_path(config)\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_validation(\n",
    "    model, \n",
    "    val_dataloader, \n",
    "    tokenizer_src, \n",
    "    tokenizer_tgt, \n",
    "    config.seq_len, \n",
    "    device, \n",
    "    lambda msg: print(msg), \n",
    "    0, \n",
    "    None, \n",
    "    num_examples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "\n",
    "\n",
    "![Inference Model Time Step 1](images/InferenceModelTimeStep1.png)\n",
    "\n",
    "![Inference Model Time Step 4](images/InferenceModelTimeStep4.png)\n",
    "\n",
    "- We selected, at every step, the word with the maximum softmax value. This strategy is called *greedy* and usually does not perform very well.\n",
    "- A better strategy is to select at each step the top B words and evaluate all the possible next words for each of them and at each step, keeping the top B most probable sequences. This is the *Beam Search strategy* and generally performs better\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(config: Config, sentence: str):\n",
    "    # Define the device, tokenizers, and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer_src = Tokenizer.from_file(str(Path(config.tokenizer_file.format(config.lang_src))))\n",
    "    tokenizer_tgt = Tokenizer.from_file(str(Path(config.tokenizer_file.format(config.lang_tgt))))\n",
    "    model = build_transformer(\n",
    "        tokenizer_src.get_vocab_size(),\n",
    "        tokenizer_tgt.get_vocab_size(),\n",
    "        config.seq_len,\n",
    "        config.seq_len,\n",
    "        config.d_model\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    model_filename = latest_weights_file_path(config)\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "    # translate sentence\n",
    "    seq_len = config.seq_len\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Precompute the encoder output and reuse it for every generation step\n",
    "        source = tokenizer_src.encode(sentence)\n",
    "        source = torch.cat(\n",
    "            [\n",
    "                torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64),\n",
    "                torch.tensor(source.ids, dtype=torch.int64),\n",
    "                torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
    "                torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
    "            ], dim=0\n",
    "        ).to(device)\n",
    "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
    "        encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "        # Initialize the decoder input with the sos token\n",
    "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
    "\n",
    "        # Generate the translation word by word\n",
    "        while decoder_input.size(1) < seq_len:\n",
    "            # build mask for target and calculate output\n",
    "            decoder_mask = torch.triu(\n",
    "                torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1\n",
    "            ).type(torch.int).type_as(source_mask).to(device)\n",
    "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "            # project next token\n",
    "            prob = model.project(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            decoder_input = torch.cat(\n",
    "                [\n",
    "                    decoder_input,\n",
    "                    torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)\n",
    "                ], dim=1\n",
    "            )\n",
    "\n",
    "            # print the translated word\n",
    "            print(f\"{tokenizer_tgt.decode([next_word.item()])}\", end=' ')\n",
    "\n",
    "            # break if we predict the end of sentence token\n",
    "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
    "                break\n",
    "    \n",
    "    # convert ids to tokens\n",
    "    return tokenizer_tgt.decode(decoder_input[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(config, \"Good morning transformer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Alternatives\n",
    "\n",
    "Due to the success of Transformers, a variety of variants (a.k.a. X-formers) have been proposed over the past few years. These X-formers improve the vanilla Transformer from different perspectives.\n",
    "\n",
    "1. *Model Efficiency*. A key challenge of applying Transformer is its inefficiency at processing long sequences mainly due to the computation and memory complexity of the self-attention module. The improvement methods include lightweight attention (e.g. sparse attention variants) and Divide-and-conquer methods (e.g., recurrent and hierarchical mechanism).\n",
    "2. *Model Generalization*. Since the transformer is a flexible architecture and makes few assumptions on the structural bias of input data, it is hard to train on small-scale data. The improvement methods include introducing structural bias or regularization, pre-training on large-scale unlabeled data, etc.\n",
    "3. *Model Adaptation*. This line of work aims to adapt the Transformer to specific downstream tasks and applications.\n",
    "\n",
    "**Model usage**\n",
    "Generally, the Transformer architecture can be used in three different ways:\n",
    "- *Encoder–Decoder.* The full Transformer architecture as introduced in Section 2.1 is used. This is typically used in sequence-to-sequence modeling (e.g., neural machine translation).\n",
    "- *Encoder only*. Only the encoder is used and the outputs of the encoder are utilized as a representation for the input sequence. This is often used for Natural Language Understanding (NLU) tasks (e.g., text classification and sequence labeling).\n",
    "- *Decoder only*. Only the decoder is used, where the encoder-decoder cross-attention module is also removed. This is typically used for sequence generation (e.g., language modeling).\n",
    "\n",
    "**Taxonomy of Transformers**\n",
    "\n",
    "![Taxonomy of Transformers](images/TaxonomyofTransformers.jpg)\n",
    "\n",
    "**Attention**\n",
    "The improvements on attention mechanism can be divided into several directions:\n",
    "1. *Sparse Attention*. This line of work introduces sparsity bias into the attention mechanism, leading to reduced complexity.\n",
    "\n",
    "    ![Sparse Attention Variants](images/SparseAttentionVariants.jpg)\n",
    "\n",
    "2. *Linearized Attention*. This line of work disentangles the attention matrix with kernel feature maps. The attention is then computed in reversed order to achieve linear complexity.\n",
    "\n",
    "    ![Linearized Attention](images/LinearizedAttention.jpg)\n",
    "\n",
    "3. *Prototype and Memory Compression*. This class of methods reduces the number of queries or key–value memory pairs to reduce the size of the attention matrix.\n",
    "\n",
    "    ![Prototype and Memory Compression](images/PrototypeandMemoryCompression.jpg)\n",
    "\n",
    "4. *Low-rank Self-Attention*. This line of work capture the low-rank property of self-attention.\n",
    "\n",
    "5. *Attention with Prior*. The line of research explores supplementing or substituting standard attention with prior attention distributions.\n",
    "\n",
    "6. *Improved Multi-Head Mechanism*. The line of studies explores different alternative multi-head mechanisms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "### Learned Positional Encoding\n",
    "\n",
    "Learned positional encoding assigns each element with a learned column vector which encodes its absolute position  [(Gehring, et al. 2017)](https://arxiv.org/abs/1705.03122) and furthermroe this encoding can be learned differently per layer [(Al-Rfou et al. 2018)](https://arxiv.org/abs/1808.04444).\n",
    "\n",
    "### Relative Position Encoding\n",
    "\n",
    "[Shaw et al. (2018)](https://arxiv.org/abs/1803.02155) incorporated relative positional information into $W^k$ and $W^v$. Maximum relative position is clipped to a maximum absolute value of $k$ and this clipping operation enables the model to generalize to unseen sequence lengths. Therefore, $2k + 1$ unique edge labels are considered and let us denote $P^k, P^v \\in \\R^{2k + 1}$ as learnable relative position representations.\n",
    "\n",
    "**Transformer-XL** [(Dai et al., 2019)](https://arxiv.org/abs/1901.02860) proposed a type of relative positional encoding based on reparametrization of dot-product of keys and queries. To keep the positional information flow coherently across segments, Transformer-XL encodes the relative position instead, as it could be sufficient enough to know the position offset for making good predictions, i.e. $i - j$, between one key vector $k_{\\tau, j}$ and its query $q_{\\tau, i}$.\n",
    "\n",
    "### Rotary Position Embedding\n",
    "\n",
    "Rotary position embedding [(RoPE; Su et al. 2021)](https://arxiv.org/abs/2104.09864) encodes the absolution position with a rotation matrix and multiplies key and value matrices of every attention layer with it to inject relative positional information at every layer.\n",
    "\n",
    "When encoding relative positional information into the inner product of the i-th key and the j-th query, we would like to formulate the function in a way that the inner product is only about the relative position $i - j$. Rotary Position Embedding (RoPE) makes use of the rotation operation in Euclidean space and frames the relative position embedding as simply rotating feature matrix by an angle proportional to its position index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Longer Context\n",
    "\n",
    "The length of an input sequence for transformer models at inference time is upper-bounded by the context length used for training. Naively increasing context length leads to high consumption in both time ($O(L^2d)$) and memory ($O(L^2)$) and may not be supported due to hardware constraints.\n",
    "\n",
    "### Context Memory\n",
    "\n",
    "The vanilla Transformer has a fixed and limited attention span. The model can only attend to other elements in the same segments during each update step and no information can flow across separated fixed-length segments. This context segmentation causes several issues:\n",
    "- The model cannot capture very long term dependencies.\n",
    "- It is hard to predict the first few tokens in each segment given no or thin context.\n",
    "- The evaluation is expensive. Whenever the segment is shifted to the right by one, the new segment is re-processed from scratch, although there are a lot of overlapped tokens.\n",
    "\n",
    "**Transformer-XL** ([Dai et al., 2019](https://arxiv.org/abs/1901.02860); “XL” means “extra long”) modifies the architecture to reuse hidden states between segments with an additional memory. The recurrent connection between segments is introduced into the model by continuously using the hidden states from the previous segments.\n",
    "\n",
    "**Compressive Transformer** ([Rae et al. 2019](https://arxiv.org/abs/1911.05507)) extends Transformer-XL by compressing past memories to support longer sequences. It explicitly adds memory slots of size $m_m$ per layer for storing past activations of this layer to preserve long context. When some past activations become old enough, they are compressed and saved in an additional compressed memory of size $m_{cm}$ per layer.\n",
    "- Compressive transformer has two additional training losses:\n",
    "    - Auto-encoding loss (lossless compression objective) measures how well we can reconstruct the original memories from compressed memories\n",
    "    - Attention-reconstruction loss (lossy objective) reconstructs content-based attention over memory vs compressed memory and minimize the difference\n",
    "\n",
    "### Non-Differentiable External Memory\n",
    "\n",
    "**kNN-LM** ([Khandelwal et al. 2020](https://arxiv.org/abs/1911.00172)) enhances a pretrained LM with a separate kNN model by linearly interpolating the next token probabilities predicted by both models. The kNN model is built upon an external key-value store which can store any large pre-training dataset or OOD new dataset. This datastore is preprocessed to save a large number of pairs, (LM embedding representation of context, next token) and the nearest neighbor retrieval happens in the LM embedding space. Because the datastore can be gigantic, we need to rely on libraries for fast dense vector search such as [FAISS](https://github.com/facebookresearch/faiss) or [ScaNN](https://github.com/google-research/google-research/tree/master/scann). The indexing process only happens once and parallelism is easy to implement at inference time.\n",
    "\n",
    "**SPALM** (Adaptive semiparametric language models; [Yogatama et al. 2021](https://arxiv.org/abs/2102.02557)) incorporates both (1) Transformer-XL style memory for hidden states from external context as short-term memory and (2) kNN-LM style key-value store as long memory. During training, the key representations in the long-term memory stay constant, produced by a pretrained LM, but the value encoder, aka the word embedding matrix, gets updated.\n",
    "\n",
    "**Memorizing Transformer** [(Wu et al. 2022)](https://arxiv.org/abs/2203.08913) adds a kNN-augmented attention layer near the top stack of a decoder-only Transformer. This special layer maintains a Transformer-XL style FIFO cache of past key-value pairs. The same QKV values are used for both local attention and kNN mechanisms. The kNN lookup returns top-k (key, value) pairs for each query in the input sequence and then they are processed through the self-attention stack to compute a weighted average of retrieved values. Two types of attention are combined with a learnable per-head gating parameter. To prevent large distributional shifts in value magnitude, both keys and values in the cache are normalized.\n",
    "\n",
    "### Distance-Enhanced Attention Scores\n",
    "\n",
    "**Distance Aware Transformer** (DA-Transformer; [Wu, et al. 2021](https://arxiv.org/abs/2010.06925)) and **Attention with Linear Biases** (ALiBi; [Press et al. 2022](https://arxiv.org/abs/2108.12409)) are motivated by similar ideas — in order to encourage the model to extrapolate over longer context than what the model is trained on, we can explicitly attach the positional information to every pair of attention score based on the distance between key and query tokens.\n",
    "\n",
    "Note that the default positional encoding in vanilla Transformer only adds positional information to the input sequence, while later improved encoding mechanisms alter attention scores of every layer, such as rotary position embedding, and they take on form very similar to distance enhanced attention scores.\n",
    "\n",
    "DA-Transformer (Wu, et al. 2021) multiplies attention scores at each layer by a learnable bias that is formulated as a function of the distance between key and query. Different attention heads use different parameters to distinguish diverse preferences to short-term vs long-term context.\n",
    "\n",
    "### Make it Recurrent\n",
    "\n",
    "**Universal Transformer** [(Dehghani, et al. 2019)](https://arxiv.org/abs/1807.03819) combines self-attention in Transformer with the recurrent mechanism in RNN, aiming to benefit from both a long-term global receptive field of Transformer and learned inductive biases of RNN. Rather than going through a fixed number of layers, Universal Transformer dynamically adjusts the number of steps using adaptive computation time. If we fix the number of steps, an Universal Transformer is equivalent to a multi-layer Transformer with shared parameters across layers.\n",
    "\n",
    "On a high level, the universal transformer can be viewed as a recurrent function for learning the hidden state representation per token. The recurrent function evolves in parallel across token positions and the information between positions is shared through self-attention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adaptive Modeling\n",
    "\n",
    "Adaptive modeling refers to a mechanism that can adjust the amount of computation according to different inputs. For example, some tokens may only need local information and thus demand a shorter attention span; Or some tokens are relatively easier to predict and do not need to be processed through the entire attention stack.\n",
    "\n",
    "### Adaptive Attention Span\n",
    "\n",
    "If the attention span could adapt its length flexibly and only attend further back when needed, it would help reduce both computation and memory cost to support longer maximum context size in the model. This is the motivation for Adaptive Attention Span. [Sukhbaatar et al (2019)](https://arxiv.org/abs/1905.07799) proposed a self-attention mechanism that seeks an optimal attention span. They hypothesized that different attention heads might assign scores differently within the same context window and thus the optimal span would be trained separately per head.\n",
    "\n",
    "### Depth-Adaptive Transformer\n",
    "\n",
    "At inference time, it is natural to assume that some tokens are easier to predict and thus do not require as much computation as others. Therefore we may only process its prediction through a limited number of layers to achieve a good balance between speed and performance. Both **Depth-Adaptive Transformer** [(Elabyad et al. 2020)](https://arxiv.org/abs/1910.10073) and **Confident Adaptive Language Model** (CALM; [Schuster et al. 2022](https://arxiv.org/abs/2207.07061)) are motivated by this idea and learn to predict optimal numbers of layers needed for different input tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Attention\n",
    "\n",
    "The computation and memory cost of the vanilla Transformer grows quadratically with sequence length and hence it is hard to be applied on very long sequences. Many efficiency improvements for Transformer architecture have something to do with the self-attention module - making it cheaper, smaller or faster to run. See the survey paper on Efficient Transformers [(Tay et al. 2020)](https://arxiv.org/abs/2009.06732).\n",
    "\n",
    "### Sparse Attention Patterns\n",
    "\n",
    "#### Fixed Local Context\n",
    "\n",
    "A simple alternation to make self-attention less expensive is to restrict the attention span of each token to local context only, so that self-attention grows linearly with the sequence length.\n",
    "\n",
    "The idea was introduced by **Image Transformer** [(Parmer, et al 2018)](https://arxiv.org/abs/1802.05751), which formulates image generation as sequence modeling using an encoder-decoder transformer architecture:\n",
    "- The encoder generates a contextualized, per-pixel-channel representation of the source image;\n",
    "- Then the decoder autoregressively generates an output image, one channel per pixel at each time step.\n",
    "\n",
    "#### Strided Context\n",
    "\n",
    "**Sparse Transformer** [(Child et al., 2019)](https://arxiv.org/abs/1904.10509) introduced factorized self-attention, through sparse matrix factorization. It also proposed a set of changes so as to train the Transformer up to hundreds of layers, including gradient checkpointing, recomputing attention & FF layers during the backward pass, mixed precision training, efficient block-sparse implementation, etc. Please check the paper for more details or the blog post ([techniques for scaling up model training](https://lilianweng.github.io/posts/2021-09-25-train-large/)).\n",
    "\n",
    "**Blockwise Attention** [(Qiu et al. 2019)](https://arxiv.org/abs/1911.02972) introduces a sparse block matrix to only allow each token to attend to a small set of other tokens. \n",
    "\n",
    "#### Combination of Local and Global Context\n",
    "\n",
    "**ETC** (Extended Transformer Construction; [Ainslie et al. 2019](https://arxiv.org/abs/2004.08483)), **Longformer** [(Beltagy et al. 2020)](https://arxiv.org/abs/2004.05150) and Big Bird [(Zaheer et al. 2020)](https://arxiv.org/abs/2007.14062) models combine both local and global context when building an attention matrix. All these models can be initialized from existing pretrained models.\n",
    "\n",
    "One more update in ETC is to incorporate a CPC (contrastive predictive coding) task using NCE loss into the pretraining stage, besides the MLM task: The representation of one sentence should be similar to the representation of context around it when this sentence is masked.\n",
    "\n",
    "Attention pattern in Longformer contains three components:\n",
    "1. Local attention: Similar to ETC, local attention is controlled by a sliding window of fixed size w\n",
    "2. Global attention of preselected tokens: Longformer has a few pre-selected tokens (e.g. [CLS] token) assigned with global attention span, that is, attending to all other tokens in the input sequence.\n",
    "3. Dilated attention: Dilated sliding window of fixed size r and gaps of dilation size d, similar to Sparse Transformer\n",
    "\n",
    "Big Bird is quite similar to Longformer, equipped with both local attention and a few preselected tokens with global attention span, but Big Bird replaces dilated attention with a new mechanism where all tokens attend to a set of random tokens. The design is motivated by the fact that attention pattern can be viewed as a directed graph and a random graph has the property that information is able to rapidly flow between any pair of nodes.\n",
    "\n",
    "### Content-based Attention\n",
    "\n",
    "The improvements proposed by **Reformer** [(Kitaev, et al. 2020)](https://arxiv.org/abs/2001.04451) aim to solve the following pain points in vanilla Transformer:\n",
    "- Quadratic time and memory complexity within self-attention module.\n",
    "- Memory in a model with N layers is N-times larger than in a single-layer model because we need to store activations for back-propagation.\n",
    "- The intermediate FF layers are often quite large.\n",
    "\n",
    "Reformer proposed two main changes:\n",
    "1. Replace the dot-product attention with locality-sensitive hashing (LSH) attention, reducing the complexity from $O(L^2)$ to $O(L log L)$.\n",
    "2. Replace the standard residual blocks with reversible residual layers, which allows storing activations only once during training instead of N times (i.e. proportional to the number of layers).\n",
    "\n",
    "In order to find nearest neighbors quickly in high-dimensional space, Reformer incorporates Locality-Sensitive Hashing (LSH) into its attention mechanism.\n",
    "\n",
    "Another improvement by Reformer is to use reversible residual layers [(Gomez et al. 2017)](https://arxiv.org/abs/1707.04585). The motivation for reversible residual network is to design the architecture in a way that activations at any given layer can be recovered from the activations at the following layer, using only the model parameters. Hence, we can save memory by recomputing the activation during backprop rather than storing all the activations.\n",
    "\n",
    "**Routing Transformer** [(Roy et al. 2021)](https://arxiv.org/abs/2003.05997) is also built on content-based clustering of keys and queries. Instead of using a static hashing function like LSH, it utilizes online k-means clustering and combines it with local, temporal sparse attention to reduce the attention complexity from $O(L^2)$ to $O(L^{1.5})$.\n",
    "\n",
    "### Low-Rank Attention\n",
    "\n",
    "**Linformer** [(Wang et al. 2020)](https://arxiv.org/abs/2006.04768) approximates the full attention matrix with a low rank matrix, reducing the time & space complexity to be linear.\n",
    "\n",
    "Additional techniques can be applied to further improve efficiency of Linformer:\n",
    "- Parameter sharing between projection layers, such as head-wise, key-value and layer-wise (across all layers) sharing.\n",
    "- Use different k at different layers, as heads in higher layers tend to have a more skewed distribution (lower rank) and thus we can use smaller k at higher layers.\n",
    "- Use different types of projections; e.g. mean/max pooling, convolution layer with kernel and stride L/k.\n",
    "\n",
    "**Random Feature Attention** (RFA; [Peng et al. 2021](https://arxiv.org/abs/2103.02143)) relies on random feature methods (Rahimi & Recht, 2007) to approximate softmax operation in self-attention with low rank feature maps in order to achieve linear time and space complexity. **Performers** [(Choromanski et al. 2021)](https://arxiv.org/abs/2009.14794) also adopts random feature attention with improvements on the kernel construction to further reduce the kernel approximation error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers for Reinforcement Learning\n",
    "\n",
    "The self-attention mechanism avoids compressing the whole past into a fixed-size hidden state and does not suffer from vanishing or exploding gradients as much as RNNs. Reinforcement Learning tasks can for sure benefit from these traits. However, it is quite difficult to train Transformer even in supervised learning, let alone in the RL context. It could be quite challenging to stabilize and train a LSTM agent by itself, after all.\n",
    "\n",
    "The **Gated Transformer-XL** (GTrXL; [Parisotto, et al. 2019](https://arxiv.org/abs/1910.06764)) is one attempt to use Transformer for RL. GTrXL succeeded in stabilizing training with two changes on top of Transformer-XL:\n",
    "1. The layer normalization is only applied on the input stream in a residual module, but NOT on the shortcut stream. A key benefit to this reordering is to allow the original input to flow from the first to last layer.\n",
    "2. The residual connection is replaced with a GRU-style (Gated Recurrent Unit; Chung et al., 2014) gating mechanism.\n",
    "\n",
    "**Decision Transformer** (DT; [Chen et al 2021](https://arxiv.org/abs/2106.01345)) formulates Reinforcement Learning problems as a process of conditional sequence modeling, outputting the optimal actions conditioned on the desired return, past states and actions. It therefore becomes straightforward to use Transformer architecture. Decision Transformer is for off-policy RL, where the model only has access to a fixed collection of trajectories collected by other policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
