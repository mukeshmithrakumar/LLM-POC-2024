{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model From Scratch\n",
    "\n",
    "**References**\n",
    "- *[Coding a Transformer from scratch on PyTorch, with full explanation, training and inference](https://youtu.be/ISNdQcPhsts?si=M80AnG5chc6sKzk5) [Youtube Video]*\n",
    "- *[An Introduction to Transformers](https://arxiv.org/abs/2304.10557) [Paper]*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# General\n",
    "import math\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# Huggingface\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# lightning ai\n",
    "import torchmetrics\n",
    "\n",
    "# My python files\n",
    "from config import Config\n",
    "from helpers import latest_weights_file_path, get_weights_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embeddings\n",
    "\n",
    "- Input Matrix (sequence, d_model) - each word is made up of say 512 numbers then the shape of input matrix will be (x, 512) where x is the number of words.\n",
    "\n",
    "![Input Embeddings](images/InputEmbeddings.png)\n",
    "\n",
    "- Embedding isn't fixed, it is learned by the model.\n",
    "- \n",
    "\n",
    "*Why are we multiplying the embedding output by the square root of the dimension?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        \"\"\"Input Embedding Module.\n",
    "        Initializes the Embedder module. This module is used to embed the input tokens into a vector space.\n",
    "        \n",
    "        Args:\n",
    "            d_model (int): Dimension of Input Embedding\n",
    "            vocab_size (int): Vocabulary Size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds the input tensor into a vector space. \n",
    "        This is done by looking up the embedding for each token in the input tensor.\n",
    "        - Scales output by sqrt(dimensions) as per Vaswani et al. (2017).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor in R^d (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "- Conveys information about the position of the word in a sentence.\n",
    "- We want the model to treat words that appear close to each other as \"close\" and words that are distant as \"distant\"\n",
    "- \n",
    "\n",
    "![Positional Embedding](images/PositionalEmbedding.png)\n",
    "\n",
    "- Unlike embeddings, positional embeddings aren't learned. We just compute this once. *Is this why we need a large dataset? Because we aren't learning positional encoding to tell the model appropriately what word can be closer to the other?*\n",
    "- \n",
    "\n",
    "The formula used to create the positional encoding:\n",
    "![Positional Embedding Vector](images/PositionalEmbeddingVector.png)\n",
    "\n",
    "- *Why is there an odd and even position equation?*\n",
    "- *Why do we want the positional encoding to represent a pattern that can be learned by the model? Is that why we use sine and cosine?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: Config.dropout_constant) -> None:\n",
    "        \"\"\"Initializes the PositionalEncoder module. \n",
    "        This module is used to encode the position of the input tokens into the input embeddings.\n",
    "        Create a matrix of shape (max_sequence_length, dimensions) to store the positional encodings.\n",
    "        For each position in the sequence, compute the positional encoding for each dimension.\n",
    "        Store the positional encodings in the matrix.\n",
    "        Implement the positional encodings as per Vaswani et al. (2017).\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The number of dimensions to embed the input tokens into.\n",
    "            seq_len (int): The maximum length of the sentence since we need to create one vector for each position.\n",
    "            dropout (float): The dropout rate to apply to the PEs.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create a matrix of shape(seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len, 1)\n",
    "        # torch.arange - returns a 1D tensor of size (end-start/step) with values from the interval [start, end) \n",
    "        # taken with common difference step beginning from start.\n",
    "        # ? Why are we unsqueezing here?\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n",
    "        # ? Formula - I am not sure of the arange part here and how it translates to the equation above\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # (d_model / 2)\n",
    "        # ? It seems that you are doing this log this for numerical stability, need to look into why?\n",
    "        # Apply the sin to even position & cosine to odd positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # sin(position * (10000 ** (2i / d_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # cos(position * (10000 ** (2i / d_model))\n",
    "        # Adding a new dimension to account for the batch of sentences, we use unsqueeze to do this.\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "        # Register the positional encoding as a buffer\n",
    "        # Why: to keep inside the module, not as a learned param, but to be saved along with the state of the model\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Adds the positional encodings to the input tensor. Applies dropout to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to add positional encodings to.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # Since we are not learning this tensor, we add requires grad as False\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)  # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "\n",
    "- Why self-attention? Each word in a sentence relates to each other.\n",
    "\n",
    "### How to compute Self Attention\n",
    "\n",
    "- Self-Attention is permutation invariant.\n",
    "\n",
    "![Self Attention Part 1](images/SelfAttentionPart1.png)\n",
    "\n",
    "![Self Attention Part 2](images/SelfAttentionPart2.png)\n",
    "\n",
    "- Self-Attention requires no parameters. Up to now the interaction between words has been driven by their embedding and the positional encodings. This will change later.\n",
    "- We expect values along the diagonal to be the highest.\n",
    "- If we don’t want some positions to interact, we can always set their values to –∞ before applying the softmax in this matrix and the model will not learn those interactions. We will use this in the decoder.\n",
    "\n",
    "**What is query, keys and values?**\n",
    "- Query (Q): This represents the current word or part of the sequence the model is focusing on.\n",
    "- Key (K): This represents all the words or parts in the input sequence.\n",
    "- Value (V): This also represents all the words or parts in the input sequence, but it contains the actual information associated with each key.\n",
    "\n",
    "**Steps Involved:**\n",
    "- Encoding Inputs: Each word in the input sequence is converted into a vector representation.\n",
    "- Calculating Compatibility Scores: The model calculates a score for each key-query pair. This score represents how relevant a particular word (key) is to the current word being processed (query). There are different ways to calculate this score, but they typically involve measuring the dot product or cosine similarity between the query and key vectors.\n",
    "- Softmax Attention: A softmax function is applied to the compatibility scores, converting them into attention weights. These weights represent the importance of each word in the sequence relative to the current word.\n",
    "- Weighted Sum: The attention weights are multiplied element-wise with the value vectors. This essentially amplifies the information from the relevant parts of the sequence (based on the weights) and weakens the contribution of less relevant parts.\n",
    "- Attention Output: The weighted sum of the value vectors is the attention output. This output incorporates information from the entire sequence, but with a focus on the parts most relevant to the current word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "![Multi-Head Attention](images/MultiHeadAttention.png)\n",
    "\n",
    "\n",
    "**Here's how it works:**\n",
    "- Multiple Attention Heads: Instead of having a single set of query (Q), key (K), and value (V) vectors, the model creates multiple sets (often 4, 8, or 16). These are called attention heads.\n",
    "- Linear Projections: Each input word embedding is projected independently for each attention head using different weight matrices. This creates different query, key, and value vectors for each head, allowing them to focus on different aspects of the relationships between words.\n",
    "- Independent Attention: Each attention head then performs the standard attention mechanism steps (compatibility score calculation, softmax, weighted sum) independently. This results in multiple attention outputs, each capturing a different aspect of the context.\n",
    "- Concatenation: Finally, the outputs from all the attention heads are concatenated to form a single final output. This combined output incorporates information from all the heads, providing a richer representation of the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        \"\"\"This module is used to calculate the attention scores for the input tensor.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Embedding vector size.\n",
    "            h (int): Number of heads.\n",
    "            dropout (float): The dropout rate to apply to the attention scores.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        # for integer divions: // for floating point division: /\n",
    "        self.d_k = d_model // h  # Dimension of vector seen by each head\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # staticmethod: because we can call this without an instance of the above class\n",
    "    @staticmethod\n",
    "    def attention(\n",
    "        query: torch.Tensor, \n",
    "        key: torch.Tensor, \n",
    "        value: torch.Tensor, \n",
    "        mask: torch.Tensor = None, \n",
    "        dropout: int = Config.dropout_constant\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Calculates the attention scores for the input tensor.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): The query tensor to calculate the attention scores for.\n",
    "            key (torch.Tensor): The key tensor to calculate the attention scores for.\n",
    "            value (torch.Tensor): The value tensor to calculate the attention scores for.\n",
    "            mask (torch.Tensor, optional): The mask tensor to apply to the attention scores. Defaults to None.\n",
    "            dropout (int, optional): The dropout rate to apply to the attention scores. Defaults to Config.dropout.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        # @ matrix multiplication in pytorch\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # masking is just multiplying by a very small value which then becomes zero after applying softmax\n",
    "        if mask is not None:\n",
    "            # replace all the values for mask==0 with -1e9\n",
    "            attention_scores.masked_fill(mask==0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch, h, seq_len, seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = nn.Dropout(dropout)(attention_scores)\n",
    "        \n",
    "        # The second value in the tuple is used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"Projects the input tensor into the query, key, and value tensors.\n",
    "        Splits the query, key, and value tensors into multiple heads.\n",
    "        Calculates the attention scores for the input tensor.\n",
    "        Concatenates the attention scores for the multiple heads.\n",
    "        Projects the attention scores back into the original dimension.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): The query tensor to calculate the attention scores for.\n",
    "            k (torch.Tensor): The key tensor to calculate the attention scores for.\n",
    "            v (torch.Tensor): The value tensor to calculate the attention scores for.\n",
    "            mask (torch.Tensor, optional): The mask tensor to apply to the attention scores. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # for q, k, v: (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "    \n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        # Using view: keep batch dim since we don't want to split the sentence, we want to split the embedding\n",
    "        # torch.tensor.view: returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "        # Why transpose: prefer h as the second dim, this way each head will see all the sentences\n",
    "        # view() is used cause it's typically faster than reshape(), cause it doesn't need to copy the underlying data\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Getting attention scores\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "**Batch Normalization (BN):**\n",
    "- Normalization Scope: BN normalizes activations across a mini-batch of training data. It calculates the mean and standard deviation of the activations for each feature (channel) within the mini-batch and then uses these statistics to normalize the individual activations.\n",
    "- Impact: This helps to address the problem of internal covariate shift, where the distribution of activations can change significantly between mini-batches during training. By normalizing each mini-batch, BN ensures that the gradients used for updating the model's weights are more stable and less prone to exploding or vanishing gradients.\n",
    "\n",
    "*Limitations*:\n",
    "- Reliance on Batch Size: BN's effectiveness depends on the mini-batch size. Smaller batch sizes can lead to less accurate estimates of the mean and standard deviation, potentially reducing its effectiveness.\n",
    "- Increased Memory Consumption: BN requires storing the mean and standard deviation for each feature across the mini-batch, which can increase memory usage.\n",
    "\n",
    "![Layer Normalization](images/LayerNormalization.png)\n",
    "\n",
    "**Layer Normalization (LN):**\n",
    "- Normalization Scope: LN normalizes activations within each layer, independently for each sample in the mini-batch. It calculates the mean and standard deviation of the activations for each feature (channel) across all elements in a single sample (e.g., across the entire image width and height for a convolutional layer).\n",
    "- Impact: LN also addresses internal covariate shift, but at a different level. It ensures that the activations within a layer have a consistent distribution for each sample, regardless of the mini-batch or other samples in the training data. This can improve the stability of gradients and learning.\n",
    "\n",
    "*Benefits*:\n",
    "- Less Sensitive to Batch Size: LN is less sensitive to the mini-batch size compared to BN. This makes it potentially more robust for various training scenarios, including settings with small batch sizes.\n",
    "- Lower Memory Footprint: LN doesn't require storing statistics across the entire mini-batch, reducing memory consumption.\n",
    "\n",
    "*Potential Drawbacks*:\n",
    "- Limited Context: LN might not capture long-range dependencies between features as effectively as BN, which considers activations across the entire mini-batch.\n",
    "- Less Flexibility: LN applies the same normalization across all features within a layer, while BN allows for different normalizations for each feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 10**-6) -> None:\n",
    "        \"\"\"Initializes the LayerNormalization module. This module is used to normalize the input tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            eps (float, optional): Needed to prevent div by 0 error. Defaults to 10**-6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))  # Multiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(features))  # Added\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Normalizes the input tensor using the gamma and beta parameters.\n",
    "        - Assumes input has batch, the mean and standard deviation is calculated over the last dimension \n",
    "        leaving the batch dimension.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to normalize.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "        # Keep the dimension for broadcasting\n",
    "        self.mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        self.std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        return self.alpha * (x - self.mean) / (self.std + self.eps) + self.bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        \"\"\"This module is used to project the input tensor into a higher dimension \n",
    "        and then back into the original dimension.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The number of dimensions of the input tensor.\n",
    "            d_ff (int): The number of dimensions to project the input tensor into.\n",
    "            dropout (float): The dropout rate to apply to the output tensor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)  # W1 and B1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)  # W2 and B2\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Projects the input tensor into a higher dimension using a linear layer and a ReLU activation function.\n",
    "        Projects the input tensor back into the original dimension using a linear layer.\n",
    "        Applies dropout to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to project and then project back.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout: float = Config.dropout_constant) -> None:\n",
    "        \"\"\"This module is used to add the input tensor to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            dropout (float, optional): The dropout rate to apply to the output tensor. Defaults to Config.dropout_constant.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, sublayer: torch.nn.Module) -> torch.Tensor:\n",
    "        \"\"\"Adds the input tensor to the output tensor. Applies dropout to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to add to the output tensor.\n",
    "            sublayer (torch.nn.Module): The network layer to apply to the input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "\n",
    "![Encoder Block](images/EncoderBlock.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            features: int,\n",
    "            self_attention_block: MultiHeadAttentionBlock,\n",
    "            feed_forward_block: FeedForwardBlock,\n",
    "            dropout: float = Config.dropout_constant\n",
    "        ) -> None:\n",
    "        \"\"\"Initializes the EncoderBlock module. This module is used to encode the input tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            self_attention_block (MultiHeadAttentionBlock): The self attention layer to apply to the input tensor.\n",
    "            feed_forward_block (FeedForwardBlock): The feed forward layer to apply to the input tensor.\n",
    "            dropout (float, optional): The dropout rate to apply to the output tensor. Defaults to Config.dropout_constant.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Applies the self attention layer to the input tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "        Applies the feed forward layer to the output tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to encode.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features:int, layers: nn.ModuleList) -> None:\n",
    "        \"\"\"Initializes the Encoder module. This module is used to encode the input tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            layers (nn.ModuleList): The layers to apply to the input tensor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Applies the layers to the input tensor. Applies the layer normalization to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to encode.\n",
    "            mask (torch.Tensor): The mask tensor to apply to the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "\n",
    "![Decoder Block](images/DecoderBlock.png)\n",
    "\n",
    "\n",
    "**Masked Multi-Head Attention**\n",
    "- Our goal is to make the model causal: it means the output at a certain position can only depend on the words on the previous positions. The model must not be able to see future words.\n",
    "\n",
    "![Masked Multi-Head Attention](images/MaskedMultiHeadAttention.png)\n",
    "\n",
    "- All the values above the diagonal are replace with -∞ before applying the softmax, which will replace them with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            features: int, \n",
    "            self_attention_block: MultiHeadAttentionBlock,\n",
    "            cross_attention_block: MultiHeadAttentionBlock,\n",
    "            feed_forward_block: FeedForwardBlock,\n",
    "            dropout: float = Config.dropout_constant\n",
    "        ) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            self_attention_block (MultiHeadAttentionBlock): The self attention layer to apply to the input tensor.\n",
    "            cross_attention_block (MultiHeadAttentionBlock): The source attention layer to apply to the input tensor.\n",
    "            feed_forward_block (FeedForwardBlock): The feed forward layer to apply to the input tensor.\n",
    "            dropout (float, optional): The dropout rate to apply to the output tensor. Defaults to Config.dropout_constant.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_bock = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor, \n",
    "        encoder_output: torch.Tensor, \n",
    "        src_mask: torch.Tensor, \n",
    "        tgt_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Applies the self attention layer to the input tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "        Applies the source attention layer to the output tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "        Applies the feed forward layer to the output tensor.\n",
    "        Adds the input tensor to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to decode.\n",
    "            encoder_output (torch.Tensor): The output tensor from the encoder.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores for the source tensor.\n",
    "            tgt_mask (torch.Tensor): The mask tensor to apply to the attention scores for the target tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connection[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connection[2](x, self.feed_forward_bock)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features:int, layers: nn.ModuleList) -> None:\n",
    "        \"\"\"Initializes the Decoder module. This module is used to decode the input tensor.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            layers (nn.ModuleList): The layers to apply to the input tensor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        encoder_output: torch.Tensor, \n",
    "        src_mask: torch.Tensor, \n",
    "        tgt_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Applies the layers to the input tensor. Applies the layer normalization to the output tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to decode.\n",
    "            encoder_output (torch.Tensor): The output tensor from the encoder.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores for the source tensor.\n",
    "            tgt_mask (torch.Tensor): The mask tensor to apply to the attention scores for the target tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "    \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        \"\"\"Initializes the ProjectionLayer module. \n",
    "        This module is used to project the input tensor into a higher dimension.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The number of dimensions of the input tensor.\n",
    "            vocab_size (int): The number of unique tokens in the input.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Projects the input tensor into a higher dimension using a linear layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (batch_size, sequence_length, dimensions) to project.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, vocabulary_size).\n",
    "        \"\"\"\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "![Transformer Block](images/TransformerBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            encoder: Encoder,\n",
    "            decoder: Decoder,\n",
    "            src_embed: InputEmbeddings,\n",
    "            tgt_embed: InputEmbeddings,\n",
    "            src_pos: PositionalEncoding,\n",
    "            tgt_pos: PositionalEncoding,\n",
    "            projection_layer: ProjectionLayer\n",
    "        ) -> None:\n",
    "        \"\"\"Initializes the Transformer module. This module is used to encode and decode the input tensor.\n",
    "\n",
    "        Args:\n",
    "            encoder (Encoder): The encoder to encode the input tensor.\n",
    "            decoder (Decoder): The decoder to decode the input tensor.\n",
    "            src_embed (InputEmbeddings): The input embedder to embed the input tensor.\n",
    "            tgt_embed (InputEmbeddings): The target embedder to embed the input tensor.\n",
    "            src_pos (PositionalEncoding): The positional encoder to encode the input tensor.\n",
    "            tgt_pos (PositionalEncoding): The positional encoder to encode the target tensor.\n",
    "            projection_layer (ProjectionLayer): _description_\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "    \n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds the input tensor. Encodes the input tensor.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): The input tensor (batch_size, sequence_length) to encode.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        encoder_output: torch.Tensor, \n",
    "        src_mask: torch.Tensor, \n",
    "        tgt: torch.Tensor, \n",
    "        tgt_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Decodes the input tensor.\n",
    "\n",
    "        Args:\n",
    "            encoder_output (torch.Tensor): The output tensor from the encoder.\n",
    "            src_mask (torch.Tensor): The mask tensor to apply to the attention scores for the source tensor.\n",
    "            tgt (torch.Tensor): The target tensor.\n",
    "            tgt_mask (torch.Tensor): The mask tensor to apply to the attention scores for the target tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, dimensions).\n",
    "        \"\"\"\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Projects the input tensor into a higher dimension.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to project.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: An output tensor of shape (batch_size, sequence_length, vocabulary_size).\n",
    "        \"\"\"\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(\n",
    "    src_vocab_size: int,\n",
    "    tgt_vocab_size: int,\n",
    "    src_seq_len: int,\n",
    "    tgt_seq_len: int,\n",
    "    d_model: int,\n",
    "    num_layers: int,\n",
    "    num_heads: int,\n",
    "    dropout: float,\n",
    "    d_ff: int\n",
    ") -> Transformer:\n",
    "    \"\"\"Builds the transformer model.\n",
    "\n",
    "    Args:\n",
    "        src_vocab_size (int): The number of unique tokens in the input.\n",
    "        tgt_vocab_size (int): The number of unique tokens in the target.\n",
    "        src_seq_len (int): The maximum length of the input sequence.\n",
    "        tgt_seq_len (int): The maximum length of the target sequence.\n",
    "        d_model (int): The number of dimensions to embed the input tokens into.\n",
    "        num_layers (int): The number of layers to apply to the input tensor.\n",
    "        num_heads (int): The number of heads to split the input tensor into.\n",
    "        dropout (float): The dropout rate to apply to the output tensor.\n",
    "        d_ff (int): The number of dimensions to project the input tensor into.\n",
    "\n",
    "    Returns:\n",
    "        Transformer: Returns a transformer model.\n",
    "    \"\"\"\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positioonal encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(num_layers):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(num_layers):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(\n",
    "            d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout\n",
    "        )\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds: Dataset, lang: str):\n",
    "    \"\"\"A generator to get all sentences in a dataset.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): The dataset.\n",
    "        lang (str): The language.\n",
    "\n",
    "    Yields:\n",
    "        _type_: A generator.\n",
    "    \"\"\"\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config: Config, ds: Dataset, lang: str):\n",
    "    \"\"\"Gets the tokenizer if it exists, otherwise builds it and saves it.\n",
    "\n",
    "    Args:\n",
    "        config (Config): The configuration.\n",
    "        ds (Dataset): The dataset.\n",
    "        lang (str): The language.\n",
    "\n",
    "    Returns:\n",
    "        _type_: The tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer_path = Path(config.tokenizer_file.format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(size: int) -> torch.Tensor:\n",
    "    \"\"\"Create a causal mask for the decoder attention.\"\"\"\n",
    "    # torch.triu: returns the upper triangular part of a matrix (2-D tensor) or batch of matrices\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, ds: Dataset, tokenizer_src, tokenizer_tgt, src_lang: str, tgt_lang: str, seq_len: int) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the BilingualDataset class:\n",
    "        - dataset: the dataset to use\n",
    "        - tokenizer_source: the tokenizer for the source language\n",
    "        - tokenizer_target: the tokenizer for the target language\n",
    "        - source_language: the source language\n",
    "        - target_language: the target language\n",
    "        - sequence_length: the sequence length to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # Start of Sentence, End of Sentence and Padding Tokens\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the length of the dataset.\"\"\"\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def truncate_sequence(self, sequence, max_len: int):\n",
    "        \"\"\"Truncates a sequence of tokens to the specified maximum length.\n",
    "\n",
    "        Args:\n",
    "            sequence: A list of integer tokens representing the sentence.\n",
    "            max_len: The maximum allowed length for the sequence.\n",
    "\n",
    "        Returns:\n",
    "            A list of truncated tokens and the original length of the sequence.\n",
    "        \"\"\"\n",
    "        if len(sequence) <= max_len:\n",
    "            return sequence, len(sequence)\n",
    "        else:\n",
    "            # Truncate from the beginning (optional: modify to truncate from the end)\n",
    "            truncated_sequence = sequence[:max_len]\n",
    "            return truncated_sequence, len(sequence)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict:\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens & Truncate sentences if necessary\n",
    "        # Note that for sentences > seq_len, by adding -2 and -1, I am choosing to not have any padding, \n",
    "        # cause enc_num_padding_tokens and dec_num_padding_tokens will turn out to be = 0\n",
    "        enc_input_tokens, orig_enc_len = self.truncate_sequence(self.tokenizer_src.encode(src_text).ids, self.seq_len - 2)\n",
    "        dec_input_tokens, orig_dec_len = self.truncate_sequence(self.tokenizer_tgt.encode(tgt_text).ids, self.seq_len - 1)\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # -2 cause we will add <s> and </s>\n",
    "        # We will only add <s> (SOS), and </s> (EOS) only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long.\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\n",
    "                f\"Original sentence length exceeded allowed limit of seq_len - {self.seq_len}. \"\n",
    "                f\"Original lengths: source - {orig_enc_len}, target - {orig_dec_len}\"\n",
    "            )\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        \n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "        \n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len, f\"encoder_input size: {encoder_input.size(0)} and seq_len: {self.seq_len}\"\n",
    "        assert decoder_input.size(0) == self.seq_len, f\"decoder_input size: {decoder_input.size(0)} and seq_len: {self.seq_len}\"\n",
    "        assert label.size(0) == self.seq_len, f\"label size: {label.size(0)} and seq_len: {self.seq_len}\"\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,   # (seq_len)\n",
    "            \"decoder_input\": decoder_input,   # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # (1, 1, seq_len)\n",
    "            # (1, seq_len) & (1, seq_len, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config: Config) -> Tuple:\n",
    "    \"\"\"Loads the dataset, builds the tokenizers and returns the dataloaders.\n",
    "\n",
    "    Args:\n",
    "        config (Config): The configuration.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: The training and validation dataloaders.\n",
    "    \"\"\"\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(f\"{config.datasource}\", f\"{config.lang_src}-{config.lang_tgt}\", split=\"train\")\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config.lang_src)\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config.lang_tgt)\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    # Need to create the tensors the model will use\n",
    "    train_ds = BilingualDataset(\n",
    "        train_ds_raw, tokenizer_src, tokenizer_tgt, config.lang_src, config.lang_tgt, config.seq_len\n",
    "    )\n",
    "    val_ds = BilingualDataset(\n",
    "        val_ds_raw, tokenizer_src, tokenizer_tgt, config.lang_src, config.lang_tgt, config.seq_len\n",
    "    )\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config.lang_src]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config.lang_tgt]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "    \n",
    "    print(f\"Max length of source sentence: {max_len_src}\")\n",
    "    print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "![Training Model](images/TrainingModel.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len: int, device):\n",
    "    \"\"\"\n",
    "    Greedy Decode:\n",
    "    - Finds the most likely next token at each step and appends it to the decoder input.\n",
    "    - Just picks the token with the max probabilitiesability at each step as the next token.\n",
    "    - May not give the ideal output, but it's fast and simple. For better results, use beam search.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The transformer model\n",
    "    - source: The input sentence\n",
    "    - source_mask: The mask for the input sentence\n",
    "    - source_tokenizer: The tokenizer for the source language\n",
    "    - target_tokenizer: The tokenizer for the target language\n",
    "    - max_length: The maximum length of the output sentence\n",
    "    - device: The device to run the model on\n",
    "    \n",
    "    Shapes:\n",
    "    - source: (1, sequence_length, dimensions)\n",
    "    - source_mask: (1, 1, sequence_length, sequence_length)\n",
    "    \n",
    "    Returns: \n",
    "    - The output sentence\n",
    "    \"\"\"\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "    \n",
    "    return decoder_input.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config: Config, vocab_src_len: int, vocab_tgt_len: int) -> Transformer:\n",
    "    \"\"\"Builds the transformer model\n",
    "\n",
    "    Args:\n",
    "        config (Config): The configuration.\n",
    "        vocab_src_len (int): The length of the source vocabulary.\n",
    "        vocab_tgt_len (int): The length of the target vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        Transformer: The transformer model.\n",
    "    \"\"\"\n",
    "    model = build_transformer(\n",
    "        vocab_src_len,\n",
    "        vocab_tgt_len,\n",
    "        config.seq_len,\n",
    "        config.seq_len,\n",
    "        config.d_model,\n",
    "        config.num_layers,\n",
    "        config.num_heads,\n",
    "        config.dropout_constant,\n",
    "        config.d_ff,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(\n",
    "    model,\n",
    "    validation_ds,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len: int,\n",
    "    device,\n",
    "    print_msg,\n",
    "    global_step,\n",
    "    writer,\n",
    "    num_examples: int = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Validation:\n",
    "    - Runs the model on the validation dataset and prints the source, target and predicted sentences.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The transformer model\n",
    "    - validation_ds: The validation dataset\n",
    "    - source_tokenizer: The tokenizer for the source language\n",
    "    - target_tokenizer: The tokenizer for the target language\n",
    "    - max_length: The maximum length of the output sentence\n",
    "    - device: The device to run the model on\n",
    "    - print_message: A function to print messages\n",
    "    - global_step: The global step\n",
    "    - num_examples: The number of examples to print\n",
    "    \n",
    "    Shapes:\n",
    "    - source: (1, sequence_length, dimensions)\n",
    "    - source_mask: (1, 1, sequence_length, sequence_length)\n",
    "    \n",
    "    Returns:\n",
    "    - Inference\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except Exception:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)  # (b, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)  # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(0)==1, \"Batch size must be 1 for validation.\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-' * console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        # Compute the char error rate\n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config: Config):\n",
    "    \"\"\"Trains the transformer model.\n",
    "\n",
    "    Args:\n",
    "        config (Config): The configuration.\n",
    "    \n",
    "    Steps:\n",
    "        - Get the dataset\n",
    "        - Get the model\n",
    "        - Define the optimizer\n",
    "        - Define the loss function\n",
    "        - For each epoch:\n",
    "            - For each batch:\n",
    "                - Run the tensors through the encoder, decoder and the projection layer\n",
    "                - Compare the output with the label\n",
    "                - Compute the loss using a simple cross entropy\n",
    "                - Backpropagate the loss\n",
    "                - Update the weights\n",
    "            - Run validation at the end of every epoch\n",
    "            - Save the model at the end of every epoch\n",
    "    \"\"\"\n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if (device == \"cuda\"):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == \"mps\"):\n",
    "        print(\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "    \n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config.datasource}_{config.model_folder}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config.experiment_name)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config.preload\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f\"Preloading model {model_filename}\")\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print(\"No model to preload, starting from scratch.\")\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config.num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch: 02d}\")\n",
    "\n",
    "        for batch in batch_iterator:\n",
    "            encoder_input = batch['encoder_input'].to(device)  # (B, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device)  # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)  # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)  # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)  # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)  # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output)  # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device)  # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\" : f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "        \n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(\n",
    "            model,\n",
    "            val_dataloader,\n",
    "            tokenizer_src,\n",
    "            tokenizer_tgt,\n",
    "            config.seq_len,\n",
    "            device,\n",
    "            lambda msg: batch_iterator.write(msg),\n",
    "            global_step,\n",
    "            writer\n",
    "        )\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"global_step\": global_step\n",
    "        }, model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Saved Weights\n",
    "\n",
    "# Adjust Training Loop : epoch counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Device memory: 15.99951171875 GB\n",
      "Max length of source sentence: 471\n",
      "Max length of target sentence: 482\n",
      "No model to preload, starting from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch  0: 100%|██████████| 14297/14297 [1:50:54<00:00,  2.15it/s, loss=1.781] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Well, either that boot comes back before sundown or I'll see the manager and tell him that I go right straight out of this hotel.\"\n",
      "    TARGET: – Écoutez-moi : ou bien ce soulier me sera rendu avant ce soir, ou bien je me rends chez le directeur pour lui annoncer que je quitte immédiatement cet hôtel.\n",
      " PREDICTED: Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Perfectly,\" said the dragoon.\n",
      "    TARGET: -- Parfaitement, dit le dragon.\n",
      " PREDICTED: Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je Je\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch  1: 100%|██████████| 14297/14297 [3:43:50<00:00,  1.06it/s, loss=1.661]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: See how les petites cheries step out for the credit of their master.\n",
      "    TARGET: Voyez comme elles vont, les petites chéries, pour faire honneur à leur maître.\n",
      " PREDICTED: Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Mais une réflexion le décida tout à coup : « Si elle allait quitter cette loge pour faire une visite, je serais bien récompensé de l’avarice avec laquelle je m’économise ce plaisir. »\n",
      "    TARGET: But a sudden thought made up his mind once and for all. \"If she were to leave that box to pay someone else a visit, I should be well rewarded for the avarice with which I am hoarding up this pleasure.\"\n",
      " PREDICTED: , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , that , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch  2:   2%|▏         | 347/14297 [02:25<1:37:36,  2.38it/s, loss=1.609]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m Config()\n\u001b[0;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     68\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Update the weights\u001b[39;00m\n\u001b[0;32m     74\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\mukes\\anaconda3\\envs\\torchdl\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mukes\\anaconda3\\envs\\torchdl\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 471\n",
      "Max length of target sentence: 482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = latest_weights_file_path(config)\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was she who attacked me--I was in a narrow and shallow bay--the frigate barred my way--and I sank her!\"\n",
      "    TARGET: J'étais resserré dans une baie étroite et peu profonde!... il me fallait passer, et... j'ai passé!»\n",
      " PREDICTED: Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: . . . Two little black streaks showing over the top of the wall at the Fair Star, certainly the upturned shafts of a cart, have now disappeared.\n",
      "    TARGET: … Deux petits traits noirs, qui dépassaient le mur de La Belle-Étoile et qui devaient être les deux brancards dressés d’une voiture, ont disparu.\n",
      " PREDICTED: en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en , en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Mamma would like to go too of all things!\n",
      "    TARGET: Maman, aussi, ne demande qu’a y aller avec nous.\n",
      " PREDICTED: Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: His eyes shone and his skin glowed with exuberent health, and he was so confident of success that my own misgivings vanished as I watched his gallant bearing and listened to his quiet and cheerful words.\n",
      "    TARGET: Sa peau luisait de santé débordante. Il avait une telle confiance dans le succès que mes appréhensions s'évanouirent à la vue de sa vaillante attitude et quand j'entendis son langage empreint d'une joie tranquille.\n",
      " PREDICTED: Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We went out over the snow in absolute silence.\n",
      "    TARGET: Nous partîmes sur la neige, dans un silence absolu.\n",
      " PREDICTED: Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "run_validation(\n",
    "    model, \n",
    "    val_dataloader, \n",
    "    tokenizer_src, \n",
    "    tokenizer_tgt, \n",
    "    config.seq_len, \n",
    "    device, \n",
    "    lambda msg: print(msg), \n",
    "    0, \n",
    "    None, \n",
    "    num_examples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "![Inference Model Time Step 1](images/InferenceModelTimeStep1.png)\n",
    "\n",
    "![Inference Model Time Step 4](images/InferenceModelTimeStep4.png)\n",
    "\n",
    "- We selected, at every step, the word with the maximum softmax value. This strategy is called *greedy* and usually does not perform very well.\n",
    "- A better strategy is to select at each step the top B words and evaluate all the possible next words for each of them and at each step, keeping the top B most probable sequences. This is the *Beam Search strategy* and generally performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(config: Config, sentence: str):\n",
    "    # Define the device, tokenizers, and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer_src = Tokenizer.from_file(str(Path(config.tokenizer_file.format(config.lang_src))))\n",
    "    tokenizer_tgt = Tokenizer.from_file(str(Path(config.tokenizer_file.format(config.lang_tgt))))\n",
    "    model = build_transformer(\n",
    "        tokenizer_src.get_vocab_size(),\n",
    "        tokenizer_tgt.get_vocab_size(),\n",
    "        config.seq_len,\n",
    "        config.seq_len,\n",
    "        config.d_model\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    model_filename = latest_weights_file_path(config)\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "    # translate sentence\n",
    "    seq_len = config.seq_len\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Precompute the encoder output and reuse it for every generation step\n",
    "        source = tokenizer_src.encode(sentence)\n",
    "        source = torch.cat(\n",
    "            [\n",
    "                torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64),\n",
    "                torch.tensor(source.ids, dtype=torch.int64),\n",
    "                torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
    "                torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
    "            ], dim=0\n",
    "        ).to(device)\n",
    "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
    "        encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "        # Initialize the decoder input with the sos token\n",
    "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
    "\n",
    "        # Generate the translation word by word\n",
    "        while decoder_input.size(1) < seq_len:\n",
    "            # build mask for target and calculate output\n",
    "            decoder_mask = torch.triu(\n",
    "                torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1\n",
    "            ).type(torch.int).type_as(source_mask).to(device)\n",
    "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "            # project next token\n",
    "            prob = model.project(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            decoder_input = torch.cat(\n",
    "                [\n",
    "                    decoder_input,\n",
    "                    torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)\n",
    "                ], dim=1\n",
    "            )\n",
    "\n",
    "            # print the translated word\n",
    "            print(f\"{tokenizer_tgt.decode([next_word.item()])}\", end=' ')\n",
    "\n",
    "            # break if we predict the end of sentence token\n",
    "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
    "                break\n",
    "    \n",
    "    # convert ids to tokens\n",
    "    return tokenizer_tgt.decode(decoder_input[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi Vendredi'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(config, \"Good morning transformer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
